\documentclass[titlepage, fleqn, a4paper, 12pt, twoside]{article}
\usepackage{geometry}
\usepackage{exsheets} %question and solution environments
\usepackage{amsmath, amssymb, amsthm} %standard AMS packages
\usepackage[utf8]{inputenc}
\usepackage{esint} %integral signs
\usepackage{marginnote} %marginnotes
\usepackage{gensymb} %miscellaneous symbols
\usepackage{commath} %differential symbols
\usepackage{xcolor} %colours
\usepackage{cancel} %cancelling terms
\usepackage[free-standing-units,space-before-unit]{siunitx} %formatting units
	\sisetup
	{
		per-mode=fraction,
		fraction-function=\frac
	}
\usepackage{tikz, pgfplots} %diagrams
	\usetikzlibrary{calc, hobby, patterns, intersections, angles, quotes, spy}
	\usetikzlibrary{circuits.logic.US,circuits.logic.IEC}
\usepackage{graphicx} %inserting graphics
\usepackage{imakeidx}
\makeindex
\usepackage{hyperref} %hyperlinks
\usepackage{datetime} %date and time
\usepackage{enumerate, enumitem} %numbered lists
\usepackage{float} %inserting floats
\usepackage[american voltages]{circuitikz} %circuit diagrams
\usepackage{setspace} %double spacing
\usepackage{microtype} %micro-typography
\usepackage{listings} %formatting code
	\lstset{language=Matlab}
	\lstdefinestyle{standardMatlab}
	{
		belowcaptionskip=1\baselineskip,
		breaklines=true,
		frame=L,
		xleftmargin=\parindent,
		language=C,
		showstringspaces=false,
		basicstyle=\footnotesize\ttfamily,
		keywordstyle=\bfseries\color{green!40!black},
		commentstyle=\itshape\color{purple!40!black},
		identifierstyle=\color{blue},
		stringstyle=\color{orange},
	}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{todonotes}
\usepackage[noabbrev,capitalize]{cleveref}
\usepackage[section]{placeins}
\usepackage[style=numeric, backend=biber]{biblatex}
\usepackage{adjustbox}
\usepackage{algpseudocode} %algorithms
\usepackage{algorithm} %algorithms

% \bibliography{<mybibfile>}% ONLY selects .bib file; syntax for version <= 1.1b
\addbibresource{bibliography.bib}% Syntax for version >= 1.2

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} %adds numbers to specific equations in non-numbered list of equations

\DeclareMathAlphabet{\mathcal}{OT1}{pzc}{m}{it}

\theoremstyle{definition}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}

\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}
\newtheorem{law}{Law}

\tikzset{
block/.style = {draw, rectangle, minimum height=3em, minimum width=3em},
tmp/.style  = {coordinate},
sum/.style = {draw, circle, node distance=1cm},
input/.style = {coordinate},
output/.style = {coordinate},
pinstyle/.style = {pin edge={to-,thin,black}}
}

\makeatletter
\@addtoreset{section}{part} %resets section numbers in new part
\makeatother

\newcommand\blfootnote[1]{%
	\begingroup
	\renewcommand\thefootnote{}\footnote{#1}%
	\addtocounter{footnote}{-1}%
	\endgroup
}

\renewcommand{\marginfont}{\scriptsize \color{blue}}

\renewcommand{\tilde}{\widetilde}

\def\doubleunderline#1{\underline{\underline{#1}}}

\SetupExSheets{solution/print = true} %prints all solutions by default

\DeclareMathOperator{\FT}{\mathcal{FT}}
\DeclareMathOperator{\DTFT}{\mathcal{DTFT}}
\DeclareMathOperator{\IFT}{\mathcal{FT}^{-1}}
\DeclareMathOperator{\IDTFT}{\mathcal{DTFT}^{-1}}
\DeclareMathOperator{\DFT}{\mathcal{DFT}}
\DeclareMathOperator{\FFT}{\mathcal{FFT}}

\DeclareMathOperator{\Q}{\mathcal{Q}}

\DeclareMathOperator{\rect}{\mathrm{rect}}
\DeclareMathOperator{\sinc}{\mathrm{sinc}}

\DeclareMathOperator{\boxfunc}{\mathrm{box}}

\DeclareMathOperator{\vspan}{\mathrm{span}}

\DeclareMathOperator{\argmin}{\mathrm{argmin}}
\DeclareMathOperator{\argmax}{\mathrm{argmax}}

\DeclareMathOperator{\Db}{\mathrm{Db}}

\DeclareMathOperator{\normal}{\mathrm{N}}

\DeclareMathOperator{\expct}{\mathrm{E}}

\DeclareMathOperator{\sgn}{\mathrm{sgn}}

\DeclareMathOperator{\dist}{\mathrm{d}}

\DeclareMathOperator{\nullspace}{\mathrm{N}}
\DeclareMathOperator{\range}{\mathrm{R}}

\DeclareMathOperator{\l2}{\ell_2}

\DeclareMathOperator{\rank}{\mathrm{rank}}
\DeclareMathOperator{\diag}{\mathrm{diag}}

\DeclareMathOperator{\supp}{\mathrm{supp}}

\def\transpose#1{{#1}^{\mathsf{T}}}
\def\minustranspose#1{{#1}^{\mathsf{-T}}}

\def\downsample#1{\downarrow_{#1}}
\def\upsample#1{\uparrow_{#1}}

%opening
\title{Digital Processing of Single and Multidimensional Signals}
\author{Aakash Jog}
\date{2018-19}

\begin{document}

\pagenumbering{roman}
\begin{titlepage}
\newgeometry{margin=0cm}
\maketitle
\end{titlepage}
\restoregeometry
%\setlength{\mathindent}{0pt}

\blfootnote
{
	\begin{figure}[H]
		\includegraphics[height = 12pt]{cc.pdf}
		\includegraphics[height = 12pt]{by.pdf}
		\includegraphics[height = 12pt]{nc.pdf}
		\includegraphics[height = 12pt]{sa.pdf}
	\end{figure}
	This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy of this license, visit \url{http://creativecommons.org/licenses/by-nc-sa/4.0/}.
} %CC-BY-NC-SA license

\tableofcontents

\clearpage
\section{Lecturer Information}

\textbf{Dr. Raja Giryes}\\
~\\
E-mail: \href{mailto:raja@tauex.tau.ac.il}{raja@tauex.tau.ac.il}\\
~\\

\section{Grading}

\begin{enumerate}
	\item Homework (MATLAB or NumPy): 20\%
	\item Final exam: 80\%
\end{enumerate}

\clearpage
\pagenumbering{arabic}

\part{Basic Definition and Theorems}

\begin{definition}[Fourier transform]
	For an LTI system, the Fourier transform is defined as
	\begin{align*}
		\FT\left\{ x(t) \right\} &= \left\langle x,e^{2 \pi j f t} \right\rangle\\
		&= \int\limits_{t \in \mathrm{R}^d} x(t) \overline{e^{2 \pi j f t}} \dif t\\
		&= \int\limits_{t \in \mathrm{R}^d} x(t) e^{-2 \pi j \transpose{f} t} \dif t
	\end{align*}
	\label{def:Fourier_transform}
	\index{transform!Fourier!continuous time}
\end{definition}

\begin{definition}[Tensor product]
	\begin{align*}
		(x_1 \otimes \dots \otimes x_d)(t) &= x_1(t_1) \cdot \dots \cdot x_d(t_d)
	\end{align*}
	for $t \in R^d$.
	\index{product!tensor}
\end{definition}

\begin{definition}[Fourier transform of tensor product]
	The Fourier transform of a tensor product is defined as
	\begin{align*}
		\FT\left\{ x_1 \otimes \dots \otimes x_d \right\}(f) &= \int\limits_{R^d} x_1(t_1) \cdot \dots \cdot x_d(t_d) e^{-2 \pi j (t_1 f_1 + \dots + t_d f_d)} \dif t_1 \dots \dif t_d\\
		&= \left( \FT\left\{ x_1(t) \right\}(f_1) \right) \dots \left( \FT\left\{ x_d(t) \right\}(f_d) \right)
	\end{align*}
	\index{transform!Fourier!continuous time}
	\index{product!tensor}
\end{definition}

\begin{definition}[$\rect$]
	\begin{align*}
		\rect(t) &=
			\begin{cases}
				0 &;\quad |t| > \frac{1}{2}\\
				1 &;\quad |t| < \frac{1}{2}\\
			\end{cases}
	\end{align*}
	\index{standard functions!rect}
\end{definition}

\begin{definition}[$u_{[a,b]}$]
	\begin{align*}
		u_{[a,b]}(t) &=
			\begin{cases}
				1 &;\quad a \le t \le b\\
				0 &;\quad \text{otherwise}\\
			\end{cases}
	\end{align*}
	\index{standard functions!u@$u_{[a,b]}$}
\end{definition}

\begin{definition}[$\sinc$]
	\begin{align*}
		\sinc(t) &= \frac{\sin(\pi t)}{\pi t}
	\end{align*}
	\index{standard functions!sinc}
\end{definition}

\begin{theorem}
	\begin{align*}
		\FT\left\{ \rect(t) \right\} &= \sinc(f)
	\end{align*}
	\index{standard functions!rect}
	\index{standard functions!sinc}
\end{theorem}

\begin{definition}[$\boxfunc$]
	A box function is defined to be the product of rect functions in multiple dimensions.
	\begin{align*}
		\boxfunc(t_1,\dots,t_n) &= \rect(t_1) \cdot \dots \cdot \rect(t_n)\\
		&=
			\begin{cases}
				1 &;\quad |t_1| < \frac{1}{2}, \dots, |t_n| < \frac{1}{2}\\
				0 &;\quad \text{otherwise}\\
			\end{cases}
	\end{align*}
	\index{standard functions!box}
	\index{standard functions!rect}
\end{definition}

\begin{definition}[Signum function]
	The signum function is defined to be
	\begin{align*}
		\sgn(t) &=
			\begin{cases}
				1 &;\quad t > 0\\
				[-1,1] &;\quad t = 0\\
				-1 &;\quad t < 0\\
			\end{cases}
	\end{align*}
	\index{standard functions!sgn}
\end{definition}

\begin{theorem}[Shifting in time]
	\begin{align*}
		\FT\left\{ D_p\left( x(t) \right) \right\}(f) &= e^{-2 \pi j \transpose{f} p} \FT\left\{ x(t) \right\}(f)
	\end{align*}
	where
	\begin{align*}
		D_p\left( x(t) \right) &= x(t - p)
	\end{align*}
	\label{thm:shifting_in_time}
	\index{shift!in time}
\end{theorem}

\begin{theorem}
	\begin{align*}
		x(t) \ast h(t) &= \FT{x}(f) \FT{h}(f)
	\end{align*}
	\label{thm:convolution_in_time_multiplication_in_frequency}
	\index{convolution!in time}
	\index{multiplication!in frequency}
\end{theorem}

\begin{theorem}[Stretching in time]
	\begin{align*}
		\FT\left\{ S_A\left( x(t) \right) \right\}(f) &= \int\limits_{R^d} x(A t) e^{-j 2 \pi \transpose{f} t} \dif t\\
		&= \int\limits_{R^d} x(q) ^{-j 2 \pi \transpose{f} A^{-1} q} \frac{\dif q}{|\det A|}\\
		&= \frac{1}{|\det A|} \int\limits_{R^d}  x(q) e^{-j 2 \pi \left( \minustranspose{A} f \right)^T q} \dif q\\
		&= \frac{\FT\left\{ x(t) \right\}\left( \minustranspose{A} f \right)}{|\det A|}\\
		&= \frac{S_{\minustranspose{A}} X(f)}{|\det A|}
	\end{align*}
	where
	\begin{align*}
		S_A\left( x(t) \right) &= x(A t)
	\end{align*}
	where $A$ is a matrix which may represent stretching, rotating, etc.\\
	Hence, if $A$ is equal to a scalar $a$,
	\begin{align*}
		\FT\left\{ x(a t) \right\} &= \frac{\FT\left\{ x(t) \right\}\left( \frac{f}{a} \right)}{|a|}
	\end{align*}
	\label{thm:stretching_in_time}
	\index{stretch!in time}
\end{theorem}

\begin{theorem}[Fourier transform of Gaussian]
	Using \cref{thm:stretching_in_time}, for a single dimensional Gaussian signal,
	\begin{align*}
		\FT\left\{ e^{-t^2} \right\} &= e^{-\pi f^2}
	\end{align*}
	and for a orthogonal multidimensional Gaussian,
	\begin{align*}
		\FT\left\{ e^{-\transpose{t} t} \right\} &= e^{-\pi \transpose{f} f}
	\end{align*}
	Hence, using \cref{thm:stretching_in_time}, for a general multidimensional Gaussian,
	\begin{align*}
		\FT\left\{ e^{-\transpose{t} C^{-1} t} \right\} &= e^{-\pi \transpose{f} C f} |\det C|
	\end{align*}
	where $C$ is the covariance matrix.
	\label{thm:Fourier_transform_of_Gaussian}
	\index{transform!Fourier!continuous time}
	\index{standard functions!Gaussian}
\end{theorem}

\begin{definition}[Unitary matrix]
	A matrix $A$ is said to be unitary if
	\begin{align*}
		A^{-1} &= A^*
	\end{align*}
	where $A^*$ is the conjugate transpose of $A$.
	\index{types of matrices!unitary}
	\index{conjugate transpose}
\end{definition}

\begin{theorem}[Rotation]
	Stretching with by a unitary matrix $A$ is equivalent to rotation, and hence denoting $S_A$ by $R_R$,
	\begin{align*}
		\FT\left\{ R_R\left( x(t) \right) \right\} &= R_R \FT\left\{ x(t) \right\}
	\end{align*}
	\label{thm:rotation}
	\index{stretch!in time}
	\index{rotation!in time}
\end{theorem}

\begin{definition}
	A dimension-reducing projection $P$ from $R^d$ to $R^{d - 1}$, is defined as
	\begin{align*}
		P\left( x(t) \right) &= \int\limits_{R^d} x(t_1,\dots,t_d) \dif t_d
	\end{align*}
	\index{projection!dimension-reducing}
\end{definition}

\begin{definition}
	The slicing operator $\Q$ is defined as
	\begin{align*}
		\Q\left\{ X(f_1,\dots,f_d) \right\} &= X\left( f_1,\dots,f_{d - 1},0 \right)
	\end{align*}
	\label{def:slicing_operator}
	\index{standard operators!slicing}
\end{definition}

\begin{theorem}[Fourier transform of dimension reducing projection]
	The Fourier transform of a dimension reducing projection is
	\begin{align*}
		\FT\left\{ P\left( x(t) \right) \right\} &= \int\limits_{R^{d - 1}} \left( \int\limits_{R^d} x(t_1,\dots,t_d) \dif t_d \right) e^{-j 2 \pi (t_1 f_1 + \dots + t_{d - 1} f_{d - 1})} \dif t_1 \dots \dif t_{d - 1}\\
		&= \int\limits_{R^{d - 1}} \left( \int\limits_{R^d} x(t_1,\dots,t_d) e^{-j 2 \pi t_d (0)} \dif t_d \right) e^{-j 2 \pi (t_1 f_1 + \dots + t_{d - 1} f_{d - 1})} \dif t_1 \dots \dif t_{d - 1}\\
		&= \int\limits_{R^d} x(t) e^{-j 2 \pi \transpose{f} t} \dif t \Big|_{f_d = 0}\\
		&= \FT\left\{ x(t) \right\}(f_1,\dots,f_{d - 1},0)\\
		&= \Q\left\{ \FT\left\{ x(t) \right\} \right\}
	\end{align*}
	where $\Q$ is the slicing operator as in \cref{def:slicing_operator}.
	\label{thm:Fourier_transform_of_dimension_reducing_projection}
	\index{projection!dimension-reducing}
	\index{transform!Fourier!continuous time}
\end{theorem}

\clearpage
\part{Hilbert Spaces}

\section{Inner Product Spaces}

\begin{definition}[Inner product spaces]
	An inner product space is defined to be a space with an inner product with the following properties.
	\begin{description}
		\item[Conjugate symmetry]
			\begin{align*}
				\langle x,y \rangle &= \langle y,x \rangle^*
			\end{align*}
		\item[Linearity]
			\begin{align*}
				\langle x , a y + b z \rangle &= a \langle x,y \rangle + b \langle x,z \rangle
			\end{align*}
		\item[Non-negativity]
			\begin{equation*}
				\langle x,x \rangle = 0 \iff x = 0
			\end{equation*}
	\end{description}
	\index{spaces!inner product}
\end{definition}

\begin{definition}[Norm and distance]
	The norm corresponding to an inner product space is defined to be
	\begin{align*}
		\|x\| &= \sqrt{\langle x,x \rangle}
	\end{align*}
	Hence, the distance between $x$ and $y$ is defined to be
	\begin{align*}
		\dist(x,y) &= \|x - y\|
	\end{align*}
	\index{norm}
	\index{distance}
\end{definition}

\begin{definition}[Hilbert space]
	An inner product, normed, complete vector space is said to be a Hilbert space.
	\index{spaces!Hilbert}
\end{definition}

\section{Operators and Transformations}

\begin{theorem}
	Let $M$ be an operator from the inner product space $H$ to the inner product space $S$.
	Then,
	\begin{align*}
		\langle M x , y \rangle &= \langle x , M^* y \rangle
	\end{align*}
	for all $x \in H$ and $y \in S$.
	If $H$ and $S$ are finite, then $M$ can be represented by a matrix and
	\begin{align*}
		M^* &= \overline{M^T}
	\end{align*}
	is the complex transpose of $M$.
	\index{operators}
\end{theorem}

\begin{definition}[Linear operator]
	An operator $T$ is said to be linear if
	\begin{align*}
		T(a_1 x_1 + a_2 x_2) &= a_1 T(x_1) + a_2 T(x_2)
	\end{align*}
	\index{types of operators!linear}
\end{definition}

\begin{theorem}[Bounded linear operation]
	All linear operators are bounded, i.e. for every $T$, there exists $\alpha$ such that
	\begin{align*}
		\left\| T(x) \right\| &\le \alpha \|x\|
	\end{align*}
	for all $x$.
	\index{types of operators!linear}
\end{theorem}

\begin{theorem}[Continuous linear operation]
	If there exists a sequence $x_i$ which converges to $x$, then for any linear operator $T$, the sequence $T(x_i)$ converges to $T(x)$.
	\index{types of operators!linear}
\end{theorem}

\begin{definition}[Adjoint of linear operation]
	The adjoint of a linear operation $T$ is defined to be $T^*$, the only bounded operation such that
	\begin{align*}
		\langle T x , y \rangle &= \langle x , T^* y \rangle
	\end{align*}
	for all $x \in H$, $y \in S$.\\
	\index{adjoint of linear operation}
\end{definition}

\begin{definition}[Unitary operator]
	An operator $T$ is said to be unitary if and only if
	\begin{align*}
		T T^* &= T^* T\\
		&= I
	\end{align*}
	where $T^*$ is the conjugate transpose of $T$.
	\index{types of operators!unitary}
\end{definition}

\begin{definition}[Hermitian operator]
	An operator $T$ is said to be Hermitian if and only if
	\begin{align*}
		T^* &= T
	\end{align*}
	where $T^*$ is the conjugate transpose of $T$.
	\index{types of operators!Hermitian}
\end{definition}

\begin{theorem}
	An operator $T$ from a Hilbert space $H$ to $H$ is Hermitian if and only if
	\begin{align*}
		\langle T x , x \rangle &\in \mathbb{R}
	\end{align*}
	for all $x \in H$.
	\index{types of operators!Hermitian}
\end{theorem}

\begin{definition}[Orthogonal complement of subspace]
	The orthogonal complement of a subspace $W$ is defined to be
	\begin{align*}
		W^{\perp} &= \left\{ x \Big| \langle x,y \rangle = 0 \quad \forall y \in W \right\}
	\end{align*}
	\index{orthogonal complement}
\end{definition}

\begin{definition}[Null space of linear operator]
	The null space of a linear operator $T : H \to S$ is defined to be
	\begin{align*}
		\nullspace(T) &= \left\{ x \Big| T x = 0 \right\}\\
		&\subseteq H
	\end{align*}
	\index{null space}
\end{definition}

\begin{definition}[Range of linear operator]
	The range of a linear operator $T : H \to S$ is defined to be
	\begin{align*}
		\range(T) &= \left\{ y \Big| T x = y \, , \, x \in H \right\}\\
		&\subseteq S
	\end{align*}
	\index{range}
\end{definition}

\begin{definition}[Direct sum]
	The direct sum of two sets $A$ and $B$ is defined to be the set
	\begin{align*}
		C &= A \oplus B
	\end{align*}
	if and only if, for all $c \in C$, there exists a unique pair $a \in A$ and $b \in B$ such that
	\begin{align*}
		c &= a + b
	\end{align*}
	\index{direct sum}
\end{definition}

\begin{theorem}
	For any linear operator $T$,
	\begin{align*}
		\nullspace\left( T^* \right) &= {R(T)}^{\perp}\\
		\range\left( T^* \right) &= {N(T)}^{\perp}
	\end{align*}
	\index{null space}
	\index{range}
	\label{thm:range_and_nullspace_of_conjugate_transpose_of_transformation}
\end{theorem}

\begin{theorem}
	\begin{align*}
		\nullspace(T) &= \nullspace\left( T^* T \right)
	\end{align*}
	\index{null space}
\end{theorem}

\begin{theorem}
	For a linear operator $T: H \to S$,
	\begin{align*}
		H &= \nullspace(T) \oplus \nullspace(T)^{\perp}
	\end{align*}
	where $\nullspace(T)$ is the null space of $T$.\\
	Additionally, if $S$ is a closed set,
	\begin{align*}
		S &= \range(T)^{C} \oplus \range(T)^{\perp}
	\end{align*}
	where
	\begin{align*}
		\range(T)^{C} &= \left( \range(T)^{\perp} \right)^{\perp}
	\end{align*}
	is the closure of $R(T)$.
	\index{null space}
	\index{closure}
\end{theorem}

\begin{definition}[Injective transformation]
	A linear transformation/operation $T: H \to S$ is said to be injective (one-to-one) if
	\begin{align*}
		x \neq y &\iff T x \neq T(y)
	\end{align*}
	\index{injectivity}
	\index{one-to-one}
\end{definition}

\begin{definition}[Surjective transformation]
	A linear transformation/operation $T: H \to S$ is said to be surjective (onto) if
	\begin{align*}
		R(T) &= S
	\end{align*}
	\index{surjectivity}
	\index{onto}
\end{definition}

\begin{definition}[Bijective transformation]
	A linear transformation/operation $T: H \to S$ is said to be bijective if it is injective (one-to-one) and surjective (onto).
	\index{bijectivity}
	\index{one-to-one and onto}
\end{definition}

\section{Singular Value Decomposition (SVD)}

\begin{definition}[Eigenvalue decomposition]
	The eigenvalue decomposition for a symmetric (and hence also square) and Hermitian matrix $A$ is
	\begin{align*}
		A &= V \Lambda V^*
	\end{align*}
	where $\Lambda$ is a diagonal matrix with the eigenvalues of $A$ as diagonal elements, and $V$ contains the corresponding eigenvectors of $A$.
	\index{decomposition!eigenvalue}
\end{definition}

\begin{definition}[Singular Value Decomposition (SVD)]
	Let $A \in \mathbb{R}^{m \times n}$.
	Then, the singular value decomposition of $A$ is defined to be
	\begin{align*}
		A &= U \Sigma V^*
	\end{align*}
	where the diagonal elements of $\Sigma$ are the singular values of $A$, and the remaining elements of $\Sigma$ are zero.\\
	The columns of $V$ are called the right singular vectors, the columns of $U$ are called the left singular vectors, and the diagonal elements of $\Sigma$ are denoted by $\sigma_i$, where $1 \le i \le \min(m,n)$.
	\index{decomposition!singular value}
\end{definition}

\begin{theorem}
	\begin{align*}
		{\sigma_i}^2 &= \lambda_i \left( A^* A \right)
	\end{align*}
	where $\sigma_i$ are the singular values of $A$, and $\lambda_i$ are the eigenvalues of $A$, for $1 \le i \le \min(m,n)$.
	\index{decomposition!eigenvalue}
	\index{decomposition!singular value}
\end{theorem}

\begin{definition}[Rank of transformation]
	The rank of a transformation $T$ is the dimension of its range, i.e.
	\begin{align*}
		\rank(T) &= \dim\left( \range(T) \right)
	\end{align*}
	\index{rank}
\end{definition}

\begin{theorem}
	The rank of a matrix $A$ is the number of non-zero singular values of $A$.
	\index{rank}
\end{theorem}

\begin{proof}
	\begin{align*}
		A x &= \left( U \Sigma V^* \right) x\\
		A x &= (U \Sigma) \left( V^* x \right)\\
		&= U \Sigma
			\begin{bmatrix}
				\langle x,v_1 \rangle & \dots & \langle x,v_n \rangle
			\end{bmatrix}\\
		&= \sum\limits_{i = 1}^{\min(m,n)} \sigma_i \langle x,v_i \rangle u_i
	\end{align*}
	Hence, any $A x$ can be expressed as a linear combination of some $u_i$ for $i \le r$.
	Therefore, the range of $A$ is spanned by $u_i$.
	Hence, by definition, the rank of $A$ is $r$.\\
	Also, only the $u_i$s with corresponding non-zero $\sigma_i$s can span $\range(A)$.
	Hence, equivalently, the rank of $A$ is the number of non-zero $\sigma_i$s of $A$.
\end{proof}

\begin{theorem}
	\begin{align*}
		\nullspace(A) &= \vspan\left\{ v_i \Big| \sigma_i = 0 \right\}
	\end{align*}
	Equivalently,
	\begin{align*}
		\nullspace(A) &= \vspan\left\{ v_i \Big| i > r \right\}
	\end{align*}
	where
	\begin{align*}
		r &= \rank(A)
	\end{align*}
	Hence,
	\begin{align*}
		\nullspace(A)^{\perp} &= \vspan\left\{ v_i \Big| \sigma_i \neq 0 \right\}
	\end{align*}
	where $A \in \mathbb{R}^{m \times n}$.
\end{theorem}

\begin{theorem}
	\begin{align*}
		\range(A) &= \vspan\left\{ u_i \Big| \sigma_i \neq 0 \right\}
	\end{align*}
	Equivalently,
	\begin{align*}
		\range(A) &= \vspan\left\{ u_i \Big| i \le r \right\}
	\end{align*}
	where
	\begin{align*}
		r &= \rank(A)
	\end{align*}
	Hence,
	\begin{align*}
		\range(A)^{\perp} &= \vspan\left\{ u_i \Big| i > r \right\}
	\end{align*}
	and equivalently
	\begin{align*}
		\range(A)^{\perp} &= \vspan\left\{ u_i \Big| \sigma_i = 0 \right\}
	\end{align*}
\end{theorem}

\section{Pseudoinverse}

\begin{definition}[Moore-Penrose pseudoinverse]
	The Moore-Penrose pseudoinverse of $A \in \mathbb{R}^{m \times n}$ is defined to be $A^{\dagger}$, such that
	\begin{align*}
		A A^{\dagger} &= I
	\end{align*}
	if $A$ is of full rank, and
	\begin{align*}
		A^{\dagger} A &= I
	\end{align*}
	if $m > n$.
	\index{pseudoinverse!Moore-Penrose}
	\index{pseudoinverse!of matrices}
\end{definition}

\begin{theorem}
	If the SVD of $A$ is
	\begin{align*}
		A &= U \Sigma V^*\\
		&= U \diag(\sigma_1,\dots,\sigma_m) V^*
	\end{align*}
	then the pseudoinverse of $A$ is
	\begin{align*}
		A^{\dagger} &= V \diag\left( \frac{1}{\sigma_1},\dots,\frac{1}{\sigma_n} \right) U^*
	\end{align*}
	where if $\sigma_i = 0$ then $\frac{1}{\sigma_i}$ is replaced by $0$.
	\index{pseudoinverse!of matrices}
\end{theorem}

\section{Set Transformations}

\begin{definition}[Set transformation]
	A set transformation $X : \l2 \to H$ composed of $x_1,\dots,x_n \in H$, is defined as
	\begin{align*}
		X a &= \sum a_i x_i
	\end{align*}
	where
	\begin{align*}
		\l2 &= \left\{ a_i \Big| \sum |a_i|^2 < \infty \right\}
	\end{align*}
	is the standard $\l2$ space, and $H$ is a Hilbert space.
	If $n$ is finite, then $X$ is a matrix with columns $x_1,\dots,x_n$.
	\index{set transformation}
\end{definition}

\begin{definition}[Linear independence]
	The set $\{x_i\}$ is said to be linearly independent if and only if
	\begin{align*}
		\sum a_i x_i &= 0
	\end{align*}
	implies $a_i = 0$, $\forall i$.\\
	Equivalently, the set $\{x_i\}$ is said to be linearly independent if and only if
	\begin{align*}
		N(X) &= \{0\}
	\end{align*}
	\index{linear independence}
\end{definition}

\begin{theorem}
	A set of orthogonal vectors/functions with positive norm is always linearly independent.
	\index{linear independence}
	\index{orthogonality}
\end{theorem}

\begin{definition}[Completeness]
	As set $\{x_i \in H\}$ is said to be complete if and only if $H$ is the closure of the set spanned by $x_i$, i.e.
	\begin{align*}
		\overline{\vspan{x_i}} &= H
	\end{align*}
	Equivalently, for any $x \in H$, $\exists \varepsilon > 0$ such that
	\begin{align*}
		\left\| x - \sum\limits_{i = 1}^{n} a_i x_i \right\| &< \varepsilon
	\end{align*}
	for a large enough $n$.
	\index{completeness}
\end{definition}

\begin{theorem}
	The adjoint operator of an operator $X : \l2 \to H$ is $X^* : H \to \l2$ such that
	\begin{align*}
		X^* x &= \left\{ \langle x_i , x \rangle_{H} \right\}
	\end{align*}
	where $x_i$ are such that
	\begin{align*}
		X &= \{x_i\}
	\end{align*}
\end{theorem}

\begin{proof}
	\begin{align*}
		\langle X a , x \rangle_{H} &= \left\langle \sum a_i x_i , x \right\rangle_{H}\\
		&= \sum {a_i}^* \langle x_i , x \rangle_{H}
	\end{align*}
	Also, by definition,
	\begin{align*}
		\langle X a , x \rangle_{H} &= \left\langle a , X^* x \right\rangle_{\l2}
	\end{align*}
	Hence,
	\begin{align*}
		\sum {a_i}^* \langle x_i , x \rangle_{H} &= \left\langle a , X^* x \right\rangle_{\l2}\\
		\therefore \left\langle a , \left\{ \langle x_i , x \right\} \right\rangle_{H} &= \left\langle a , X^* x \right\rangle_{\l2}
	\end{align*}
	Hence,
	\begin{align*}
		X^* x &= \left\{ \langle x_i , x \rangle_{H} \right\}
	\end{align*}
\end{proof}

\section{Basis}

\begin{definition}[Schauter basis]
	A set $\{x_i\} \in H$ is said to be a Schauter basis of $H$ if for any $x \in H$, there are unique coefficients $a_i$ such that
	\begin{align*}
		x &= \sum\limits_{i = 1}^{n} a_i x_i
	\end{align*}
	where $n$ is the dimension of $H$, and may be infinite.
	\index{basis!Schauter}
\end{definition}

\begin{definition}[Riesz basis]
	A set $\{x_i\} \in H$ is said to be a Riesz basis of $H$ if it is complete in $H$ and there exist $0 < A \le B < \infty$ such that
	\begin{eqnarray*}
		A \|a\|^2 \le \left\| \sum a_i x_i \right\|^2 \le B \|a\|^2\\
		A \|x\|^2 \le \left\| \sum \langle x , x_i \rangle \right\|^2 \le B \|x\|^2
	\end{eqnarray*}
	Equivalently for $X : \l2 \to H$ corresponding to $\{x_i\} \in H$,
	\begin{eqnarray*}
		A \langle a , a \rangle \le \langle X a , X a \rangle \le B \langle a , a \rangle\\
		A x^* x \le x^* X X^* x \le B x^* x
	\end{eqnarray*}
	\index{basis!Riesz}
\end{definition}

\begin{theorem}
	Any orthogonal basis is also a Riesz basis.
\end{theorem}

\begin{theorem}
	The elements of a Riesz basis are linearly independent.
	\index{basis!Riesz}
\end{theorem}

\begin{theorem}
	If
	\begin{equation*}
		A I_{\l2} \le X^* X \le B I_{\l2}
	\end{equation*}
	that is,
	\begin{equation*}
		A \|a\|^2 \le \left\| \sum a_i x_i \right\|^2 \le B \|a\|^2\\
	\end{equation*}
	then, $X^* X$ is invertible, and
	\begin{align*}
		\frac{1}{B} I_{\l2} \le \left( X^* X \right)^{-1} \le \frac{1}{A} I_{\l2}
	\end{align*}
	\index{basis!Riesz}
\end{theorem}

\begin{theorem}
	If $X$ is a Riesz basis, then $\range(X)$ is closed.
	\index{basis!Riesz}
\end{theorem}

\begin{definition}[Bi-orthogonal basis]
	The bi-orthogonal basis of a Riesz basis $X$ is defined to be
	\begin{align*}
		\tilde{X} &= X \left( X^* X \right)^{-1}
	\end{align*}
	Hence,
	\begin{align*}
		\tilde{X} X^* &= I
	\end{align*}
	\index{basis!Riesz}
	\index{basis!bi-orthogonal}
\end{definition}

\begin{theorem}
	The bi-orthogonal basis of a Riesz basis is also a Riesz basis.
	\index{basis!Riesz}
	\index{basis!bi-orthogonal}
\end{theorem}

\begin{theorem}
	For a Riesz basis $\{x_i\}$ of $H$, any $x = X a$ for $x \in H$ can be expressed as a linear combination of $H$ such that
	\begin{align*}
		x &= X a\\
		&= \sum a_i x_i\\
		&= \sum \left\langle \tilde{x_i} , x \right\rangle x_i
	\end{align*}
	where $\left\{ \tilde{x_i} \right\} = \tilde{X}$ is the bi-orthogonal basis of $X$.
	\index{basis!Riesz}
	\index{basis!bi-orthogonal}
\end{theorem}

\begin{proof}
	Let
	\begin{align*}
		x &= \sum a_i x_i\\
		&= \sum \left\langle \tilde{x_i} , x \right\rangle x_i
	\end{align*}
	Also,
	\begin{align*}
		\left\langle \tilde{x_i} , x \right\rangle &= \left\langle \tilde{x_i} , \sum\limits_{j = -\infty}^{\infty} a_j x_j \right\rangle\\
		&= \sum\limits_{j = -\infty}^{\infty} a_j \left\langle \tilde{x_i} , x_j \right\rangle
	\end{align*}
	Hence,
	\begin{align*}
		a_i &= \left\langle \tilde{x_i} , x \right\rangle\\
		&= \sum\limits_{j = -\infty}^{\infty} a_j \left\langle \tilde{x_i} , x_j \right\rangle
	\end{align*}
	Therefore,
	\begin{align*}
		\left\langle \tilde{x_i} , x_j \right\rangle &= \delta_{i,j}
	\end{align*}
	Hence, for $\tilde{X} = \left\{ \tilde{x_i} \right\}$,
	\begin{align*}
		\tilde{X}^* X &= I
	\end{align*}
	Hence, $\tilde{X}$ is the bi-orthogonal basis of $X$.
\end{proof}

\section{Projections}

\begin{definition}[Projection]
	The operator $T : H \to H$ is said to be a projection if
	\begin{align*}
		T T &= T
	\end{align*}
	that is
	\begin{align*}
		T^2 &= T
	\end{align*}
	\index{projection}
\end{definition}

\begin{theorem}
	For any $y \in \range(T)$,
	\begin{align*}
		T y &= y
	\end{align*}
	\index{projection}
\end{theorem}

\begin{proof}
	For $y \in \range(T)$, there exists $x$ such that
	\begin{align*}
		y &= T x
	\end{align*}
	Therefore,
	\begin{align*}
		T y &= T T x\\
		&= T^2 x\\
		&= T x\\
		&= y
	\end{align*}
\end{proof}

\begin{theorem}
	\begin{align*}
		\range(T) \cap \nullspace(T) &= \{0\}
	\end{align*}
	where $T$ is a projection.
	\index{projection}
\end{theorem}

\begin{proof}
	Let $y \in \range(T)$ and $y \in \nullspace(T)$.
	Then,
	\begin{align*}
		T y &= y\\
		T y &= 0
	\end{align*}
	Therefore,
	\begin{align*}
		y &= 0
	\end{align*}
\end{proof}

\begin{theorem}
	\begin{align*}
		H &= R(T) \oplus N(T)
	\end{align*}
	where $T : H \to H$ is a projection.
	\index{projection}
\end{theorem}

\begin{proof}
	\begin{align*}
		x &= T x + I x - T x\\
		&= T x + (I - T) x
	\end{align*}
	where $I$ is the identity operator.\\
	Hence, $T x$ is in $\range(T)$, and $x - T x$ is in $\nullspace(T)$.
	Therefore, as $x$ is in $H$,
	\begin{align*}
		H &= \range(T) \cup \nullspace(T)
	\end{align*}
	Also,
	\begin{align*}
		\range(T) \cap \nullspace(T) &= \{0\}
	\end{align*}
	Therefore,
	\begin{align*}
		H &= \range(T) \oplus \nullspace(T)
	\end{align*}
\end{proof}

\begin{theorem}
	Any projection $T$ can be characterized by the range and the null space of $T$.
	Hence, it can be denoted as
	\begin{align*}
		T &= E_{V W}
	\end{align*}
	where
	\begin{align*}
		V &= \range(T)\\
		W &= \nullspace(T)
	\end{align*}
	Hence, for all $v \in V$,
	\begin{align*}
		E_{V W} v &= v
	\end{align*}
	and for all $w \in W$,
	\begin{align*}
		E_{V W} w &= 0
	\end{align*}
	\index{projection}
	\index{range}
	\index{null space}
\end{theorem}

\subsection{Orthogonal Projections}

\begin{definition}[Orthogonal projection]
	A projection $T$ is said to be an orthogonal projection if
	\begin{align*}
		T &= T^*
	\end{align*}
	Hence, as
	\begin{align*}
		\nullspace(T) &= \nullspace\left( T^* \right)\\
		&= \range(T)
	\end{align*}
	or equivalently
	\begin{align*}
		V &= W^{\perp}
	\end{align*}
	it is enough to specify only $V$ to describe a orthogonal projection.
	Hence,
	\begin{align*}
		T &= P_V
	\end{align*}
	\index{projection!orthogonal}
\end{definition}

\begin{theorem}
	For an orthogonal projection $P_V$,
	\begin{align*}
		\langle x_V , x_{V^{\perp}} \rangle &= 0
	\end{align*}
	where
	\begin{align*}
		x_V &= P_V x
	\end{align*}
	\index{projection!orthogonal}
\end{theorem}

\begin{theorem}
	For an orthogonal projection $P_V$,
	\begin{align*}
		\|x\|^2 &= \|x_V\|^2 + \|x_{V^{\perp}}\|^2\\
		\therefore \|x_V\|^2 &\le \|x\|^2
	\end{align*}
	where
	\begin{align*}
		x_V &= P_V x
	\end{align*}
	\index{projection!orthogonal}
\end{theorem}

\begin{proof}
	\begin{align*}
		x &= x_V + X_{V^{\perp}}\\
		\therefore \|x\|^2 &= \|x_V + X_{V^{\perp}}\|^2
	\end{align*}
	Hence, as $x_V$ and $x_{V^{\perp}}$ are orthogonal,
	\begin{align*}
		\|x\|^2 &= \|x_V\|^2 + \|X_{V^{\perp}}\|^2
	\end{align*}
	Therefore,
	\begin{align*}
		\|x_V\|^2 &\le \|x\|^2
	\end{align*}
\end{proof}

\begin{theorem}
	For an orthogonal projection $P_V$,
	\begin{align*}
		x_V &= P_V x\\
		&= \argmin\limits_{v \in V} \|v - x\|^2
	\end{align*}
	that is, $P_V x$ is the vector in $V$ which is closest to $x$.
	\index{projection!orthogonal}
\end{theorem}

\begin{theorem}
	Let $\{v_i\}$ be the Riesz basis of a Hilbert space $H_V$, and let $V : \l2 \to H_V$ be the corresponding transformation, i.e., any $v \in H_V$ can be expressed as
	\begin{align*}
		v &= V a\\
		&= \sum a_i v_i
	\end{align*}
	Then,
	\begin{align*}
		P_{H_V} &= V \tilde{V}^*\\
		&= V \left( V^* V \right)^{-1} V
	\end{align*}
	where
	\begin{align*}
		\tilde{V} &= V \left( V^* V \right)^{-1}
	\end{align*}
	is the bi-orthogonal basis of $V$, i.e., for any $v \in H_V$,
	\begin{align*}
		V a &= v
	\end{align*}
	and
	\begin{align*}
		\tilde{V}^* v &= a
	\end{align*}
	\index{projection!orthogonal}
\end{theorem}

\subsection{Oblique Projections}

\begin{definition}[Oblique projection]
	A projection $T$ is said to be an oblique projection if
	\begin{align*}
		T &\neq T^*
	\end{align*}
	Such a projection can be denoted as
	\begin{align*}
		T &= E_{V W}\\
		x &= x_V + x_W
	\end{align*}
	similar to orthogonal projections, but
	\begin{align*}
		\langle x_V , x_W \rangle &\neq 0
	\end{align*}
	\index{projection!oblique}
\end{definition}

\begin{definition}[Pseudoinverse of transformation]
	Let $T$ be a transformation with closed range.
	Then, the pseudoinverse of $T$ is defined to be $T^{\dagger}$, such that
	\begin{align*}
		T^{\dagger} T x &= x\\
		T^{\dagger} y &= 0
	\end{align*}
	for all $x \in {\nullspace(T)}^{\perp}$, and for all $y \in {\range(T)}^{\perp}$.
	\index{pseudoinverse!of transformations}
\end{definition}

\begin{theorem}
	As in \cref{thm:range_and_nullspace_of_conjugate_transpose_of_transformation},
	\begin{align*}
		\nullspace\left( T^{\dagger} \right) &= \range(T)^{\perp}\\
		\range\left( T^{\dagger} \right) &= \nullspace(T)^{\perp}
	\end{align*}
	\label{thm:range_and_nullspace_of_pseudoinverse_of_transformation}
\end{theorem}

\begin{theorem}
	\begin{align*}
		T T^{\dagger} &= P_{R(T)}
	\end{align*}
	\index{pseudoinverse!of transformations}
\end{theorem}

\begin{theorem}
	\begin{align*}
		T^{\dagger} T &= P_{{N(T)}^{\perp}}
	\end{align*}
	\index{pseudoinverse!of transformations}
\end{theorem}

\begin{theorem}
	If $T^* T$ is invertible,
	\begin{align*}
		T^{\dagger} &= \left( T^* T \right)^{-1} T^*
	\end{align*}
	and if $T T^*$ is invertible,
	\begin{align*}
		T^{\dagger} &= T^* \left( T T^* \right)^{-1}
	\end{align*}
	\index{pseudoinverse!of transformations}
\end{theorem}

\clearpage
\part{Sampling Theory}

\section{Shannon's Interpolation Formula}

\begin{theorem}[Shannon's Interpolation Formula]
	A bandlimited signal limited by $\pm\frac{1}{2 T}$ can be reconstructed from the samples $x(n T)$ from the formula
	\begin{align*}
		x(t) &= \sum\limits_{n = -\infty}^{\infty} x(n T) \sinc\left( \frac{t - n T}{T} \right)
	\end{align*}
	\label{thm:Shannons_Interpolation_Formula}
	\index{Shannon's Interpolation Formula}
\end{theorem}

\section{Generalized Sampling}

\begin{theorem}
	Let $\{x_n\}$ be a Riesz basis.
	Then, the corresponding coefficients $a_n$ are
	\begin{align*}
		a_n &= \left\langle g(t - n T) , x(t) \right\rangle\\
		&= \int\limits_{-\infty}^{\infty} g^*(t - n T) x(t) \dif t\\
		&= \left( g^*(-t) \ast x(t) \right)(n T)
	\end{align*}
	Hence, generalized sampling can be performed by convolving the signal $x(t)$ with $g^*(-n t)$.
	Also,
	\begin{align*}
		x &= X \left( X^* X \right)^{-1} X^* x\\
		&= X \left( X^* X \right)^{-1} \{a_n\}
	\end{align*}
	Hence, the signal can be reconstructed by passing the samples through a digital filter $\left( X^* X \right)^{-1}$, multiplying by a delta train $\sum \delta(t - n T)$, and convolving with $g(t)$.
\end{theorem}

\begin{proof}
	Let $\{x_n\}$ be a Riesz basis and let $X$ be the corresponding set transformation, i.e. for $c_n \in \l2$,
	\begin{align*}
		x &= X c\\
		&= \sum c_n x_n\\
		&= \sum \left\langle \tilde{x_n} , x \right\rangle x_n
	\end{align*}
	where $\left\{ \tilde{x_n} \right\}$ is the bi-orthogonal basis of $X$.
	For standard sampling with $\sinc$s, i.e., for
	\begin{align*}
		x_n &= \sinc\left( t - \frac{n}{T} \right)
	\end{align*}
	as $\{x_n\}$ is an orthogonal basis,
	\begin{align*}
		\tilde{x_n} &= x_n\\
		&= \sinc\left( t - \frac{n}{T} \right)
	\end{align*}
	and hence the samples are
	\begin{align*}
		c_n &= \left\langle \sinc\left( t - \frac{n}{T} \right) , x \right\rangle\\
		&= \int\limits_{-\infty}^{\infty} \sinc\left( t - \frac{n}{T} \right) x(t) \dif t\\
		&= \int\limits_{-\frac{T}{2}}^{\frac{T}{2}} x(f) e^{-2 \pi j \frac{n}{T} f} \dif f
	\end{align*}
	If $x$ is band limited,
	\begin{align*}
		x(f) &= 0
	\end{align*}
	for $|f| > \frac{1}{2}$.
	Therefore,
	\begin{align*}
		c_n &= \int\limits_{-\frac{1}{2 T}}^{\frac{1}{2 T}} x(f) e^{-2 \pi j \frac{n}{T} f} \dif f\\
		&= \int\limits_{-\infty}^{\infty} x(f) e^{-2 \pi j \frac{n}{T} f} \dif f\\
		&= \IFT\left\{ x(f) e^{-2 \pi j \frac{n}{T} f} \right\} \Big|_{t = 0}\\
		&= x\left( t + \frac{n}{T} \right) \Big|_{t = 0}\\
		&= x\left( \frac{n}{T} \right)
	\end{align*}
	Therefore,
	\begin{align*}
		x &= \sum c_n x_n\\
		&= \sum \left\langle \sinc\left( t - \frac{n}{T} \right) , x \right\rangle \sinc\left( t - \frac{n}{T} \right)\\
		&= \sum x\left( \frac{n}{T} \right) \sinc\left( t - \frac{n}{T} \right)\\
		&= \left( \sum x\left( \frac{n}{T} \right) \delta\left( t - \frac{n}{T} \right) \right) \ast \sinc(t)
	\end{align*}
\end{proof}

\begin{definition}[Discrete time Fourier transform]
	The discrete time Fourier transform of a signal is defined to be
	\begin{align*}
		X\left( e^{j 2 \pi f} \right) &= \sum\limits_{n = -\infty}^{\infty} x(n T) e^{-j 2 \pi f n T}
	\end{align*}
	\index{transform!Fourier!discrete time}
\end{definition}

\begin{theorem}[Poisson summation formula]
	\begin{align*}
		\FT\left\{ x(t) \right\} &= \DTFT\left\{ x(n T) \right\}\\
	\end{align*}
	i.e.
	\begin{align*}
		\sum\limits_{n = -\infty}^{\infty} X\left( f + \frac{n}{T} \right) &= T \sum\limits_{n = -\infty}^{\infty} x(n T) e^{-j 2 \pi n T f}\\
	\end{align*}
	\index{transform!Fourier!continuous time}
	\index{transform!Fourier!discrete time}
	\index{Poisson summation formula}
	\label{thm:Poisson_summation_formula}
\end{theorem}

\begin{proof}
	\begin{align*}
		\sum\limits_{n = -\infty}^{\infty} X\left( f + \frac{n}{T} \right) &= \sum\limits_{n = -\infty}^{\infty} \int\limits_{-\infty}^{\infty} x(t) e^{-j 2 \pi \left( f + \frac{n}{T} \right) t} \dif t\\
		&= \int\limits_{-\infty}^{\infty} \left( x(t) \sum\limits_{n = -\infty}^{\infty} e^{-j 2 \pi \frac{n}{T} t} \right) e^{-j 2 \pi f t} \dif t\\
		&= \int\limits_{-\infty}^{\infty} \left( x(t) T \sum\limits_{n = -\infty}^{\infty} \delta(t - n T) \right) e^{-j 2 \pi f t} \dif t\\
		&= T \int\limits_{-\infty}^{\infty} \left( x(t) \sum\limits_{n = -\infty}^{\infty} \delta(t - n T) \right) e^{-j 2 \pi f t} \dif t\\
		&= T \sum\limits_{n = -\infty}^{\infty} x(n T) \int\limits_{-\infty}^{\infty} \delta(t - n T) e^{-j 2 \pi f t} \dif t\\
		&= T \sum\limits_{n = -\infty}^{\infty} x(n T) e^{-j 2 \pi f n T}
	\end{align*}
\end{proof}

\begin{theorem}
	\begin{align*}
		\sum\limits_{n = -\infty}^{\infty} S^*\left( f - \frac{n}{T} \right) G\left( f - \frac{n}{T} \right) &= T \DTFT\left\{ \left( s^*(-t) \ast g(t) \right) \Big|_{t = n T} \right\}
	\end{align*}
	\index{transform!Fourier!discrete time}
\end{theorem}

\begin{proof}
	\begin{align*}
		&\quad \sum\limits_{n = -\infty}^{\infty} S^*\left( f - \frac{n}{T} \right) G\left( f - \frac{n}{T} \right)\\
		&= \int\limits_{-\infty}^{\infty} \left( \sum\limits_{n = -\infty}^{\infty} \left( s^*(-t) \ast g(t) \right) e^{j 2 \pi \frac{n}{T} t} \right) e^{-j 2 \pi f t} \dif t\\
		&= \int\limits_{-\infty}^{\infty} \left( s^*(t) \ast g(t) \right) \sum\limits_{n = -\infty}^{\infty} \delta(t - n T) e^{-j 2 \pi f t} \dif t\\
		&= T \int\limits_{-\infty}^{\infty} \left( s^*(-t) \ast g(t) \right) \sum\limits_{n = -\infty}^{\infty} \delta(t - n T) e^{-j 2 \pi f t} \dif t\\
		&= T \sum\limits_{n = -\infty}^{\infty} \left( s^*(-t) \ast g(t) \right) \Big|_{t = n T} e^{-j 2 \pi f n T}\\
		&= T \DTFT\left\{ \left( s^*(-t) \ast g(t) \right) \Big|_{t = n T} \right\}
	\end{align*}
\end{proof}

\begin{definition}[Shift invariant Riesz basis]
	A Riesz basis $\{x_n\}$ is said to be a shift invariant Riesz basis if
	\begin{align*}
		x_n &= g(t - n T)
	\end{align*}
\end{definition}

\begin{theorem}[Generalized sampling and reconstruction]
	Let $\{x_n\}$ be a shift invariant Riesz basis.
	Then, the corresponding coefficients $a_n$ are
	\begin{align*}
		a_n &= \left\langle g(t - n T) , x(t) \right\rangle\\
		&= \int\limits_{-\infty}^{\infty} g^*(t - n T) x(t) \dif t\\
		&= \left( g^*(-t) \ast x(t) \right)(n T)
	\end{align*}
	Hence, generalized sampling can be performed by convolving the signal $x(t)$ with $g^*(-n t)$.
	Also,
	\begin{align*}
		x &= X \left( X^* X \right)^{-1} X^* x\\
		&= X \left( X^* X \right)^{-1} \{a_n\}
	\end{align*}
	Hence, the signal can be reconstructed by passing the samples through a digital filter $\left( X^* X \right)^{-1}$, multiplying by a delta train $\sum \delta(t - n T)$, and convolving with $g(t)$.
	\label{thm:generalized_sampling_and_reconstruction}
\end{theorem}

\begin{theorem}
	Let $X : \l2 \to H$ be a set transformation corresponding to shift invariant functions
	\begin{align*}
		x_i &= g(t - i)
	\end{align*}
	Then, $X = \{x_i\}$ is a Riesz basis if and only if
	\begin{equation*}
		A \le \sum\limits_{n = -\infty}^{\infty} \left| G(f - n) \right|^2 \le B
	\end{equation*}
\end{theorem}

\begin{proof}
	Let $T = 1$.
	Therefore,
	\begin{align*}
		\left\| \sum\limits_{n = -\infty}^{\infty} c_n g(t - n) \right\|^2 &= \sum\limits_{n = -\infty}^{\infty} \sum\limits_{m = -\infty}^{\infty} {c_n}^* c_m \int\limits_{-\infty}^{\infty} g^*(t - n) g(t - m) \dif t
	\end{align*}
	Hence, substituting $t' = t - n$,
	\begin{align*}
		\left\| \sum\limits_{n = -\infty}^{\infty} c_n g(t - n) \right\|^2 &= \sum\limits_{n = -\infty}^{\infty} \sum\limits_{m = -\infty}^{\infty} {c_n}^* c_m \int\limits_{-\infty}^{\infty} g^*(t') g(t' - m + n) \dif t'
	\end{align*}
	Hence, substituting $k = m - n$,
	\begin{align*}
		\left\| \sum\limits_{n = -\infty}^{\infty} c_n g(t - n) \right\|^2 &= \sum\limits_{n = -\infty}^{\infty} \sum\limits_{k = -\infty}^{\infty} {c_n}^* c_{n + k} \int\limits_{-\infty}^{\infty} g^*(t') q(t' - k) \dif t'
	\end{align*}
	Hence, by Parseval's Identity,
	\begin{align*}
		\left\| \sum\limits_{n = -\infty}^{\infty} c_n g(t - n) \right\|^2 &= \int\limits_{-\infty}^{\infty} \sum\limits_{k = -\infty}^{\infty} \left( \sum\limits_{n = -\infty}^{\infty} {c_n}^* c_{n + k} \right) e^{-j 2 \pi f k} \left| G(f) \right|^2 \dif f
	\end{align*}
	Let
	\begin{align*}
		q_k &= \sum\limits_{n = -\infty}^{\infty} {c_n}^* c_{n + k}\\
		&= {c_n}^* \ast c_{-n}\\
		\therefore \DTFT\left\{ q[k] \right\} &= \DTFT\left\{ {c[n]}^* c[n + k] \right\}\\
		&= \DTFT\left\{ {c[n]}^* \right\} \DTFT\left\{ c[n + k] \right\}\\
		\therefore Q\left( e^{j 2 \pi f} \right) &= C\left( e^{-j 2 \pi f} \right) C^*\left( e^{-j 2 \pi f} \right)\\
		&= \left| C\left( e^{-j 2 \pi f} \right)^2 \right|
	\end{align*}
	Therefore,
	\begin{align*}
		\left\| \sum\limits_{n = -\infty}^{\infty} c_n g(t - n) \right\|^2 &= \int\limits_{-\infty}^{\infty} \sum\limits_{k = -\infty}^{\infty} q_k e^{-j 2 \pi f k} \left| G(f) \right|^2 \dif f\\
		&= \int\limits_{-\infty}^{\infty} Q\left( e^{j 2 \pi f t} \right) \left| G(f) \right|^2 \dif t\\
		&= \int\limits_{-\infty}^{\infty} \left| C\left( e^{-j 2 \pi f t} \right) \right|^2 \left| G(f) \right|^2 \dif f\\
		&= \sum\limits_{n = -\infty}^{\infty} \int\limits_{-\frac{1}{L}}^{\frac{1}{L}} \left| G(f - n) \right|^2 \left| C\left( e^{-j 2 \pi f} \right) \right|^2 \dif f\\
		&= \int\limits_{-\frac{1}{L}}^{\frac{1}{L}} \left| C\left( e^{-j 2 \pi f} \right) \right|^2 \sum\limits_{n = -\infty}^{\infty} \left| G(f - n) \right|^2 \dif f
	\end{align*}
	Hence, for $x_n$ to form a Riesz basis, $c_n$ must satisfy
	\begin{equation*}
		A \int\limits_{-\frac{1}{L}}^{\frac{1}{L}} \left| C\left( e^{-j 2 \pi f} \right) \right|^2 \le \int\limits_{-\frac{1}{L}}^{\frac{1}{L}} \left| C\left( e^{-j 2 \pi f} \right) \right|^2 \sum\limits_{n = -\infty}^{\infty} \left| G(f - n) \right|^2 \dif f \le B \int\limits_{-\frac{1}{L}}^{\frac{1}{L}} \left| C\left( e^{-j 2 \pi f} \right) \right|^2 \dif f
	\end{equation*}
	which holds if and only if
	\begin{equation*}
		A \le \sum\limits_{n = -\infty}^{\infty} \left| G(f - n) \right|^2 \le B
	\end{equation*}
\end{proof}

\begin{theorem}
	Let $X : \l2 \to H$ be a set transformation corresponding to shift invariant functions
	\begin{align*}
		x_i &= g(t - i)
	\end{align*}
	Then, a generalized sampling and reconstruction using these basis elements can be performed as in \cref{fig:generalized_sampling_and_reconstruction}.
	\begin{figure}[H]
		\centering
		\begin{adjustbox}{max width=\columnwidth}
		\begin{tikzpicture}[auto, node distance=2cm,]
			\node [block] (sampler) {$g(-t)$};
			\node [block, right = of sampler] (digital_filter) {$\left( X^* X \right)^{-1} = \frac{1}{\sum\limits_{n = -\infty}^{\infty} \left| G(f - n) \right|^2}$};
			\node [mixer, right = of digital_filter] (mixer) {};
			\node [block, right = of mixer] (reconstructor) {$g(t)$};

			\draw [stealth-] (sampler.west) -- ++(-1,0) node [left] {$x(t)$};
			\draw (sampler.east) to [switch] ++(1,0) [-stealth] -- (digital_filter.west) node [midway, above] {$c[n]$};
			\draw [-stealth] (digital_filter.east) -- (mixer.west) node [midway, above] {$d[n]$};
			\draw [stealth-] (mixer.south) -- ++(0,-1) node [below] {$\sum\limits_{n = -\infty}^{\infty} \delta(t - n)$};
			\draw [-stealth] (mixer.east) -- (reconstructor.west) node [right] {};
			\draw [-stealth] (reconstructor.east) -- ++(1,0) node [right] {$\tilde{x}(t)$};
		\end{tikzpicture}
		\end{adjustbox}
		\caption{Generalized Sampling and Reconstruction}
		\label{fig:generalized_sampling_and_reconstruction}
	\end{figure}
\end{theorem}

\begin{proof}
	\begin{align*}
		c_i &= \left\langle x_i , \sum\limits_{n = -\infty}^{\infty} x_n d_n \right\rangle\\
		&= \left\langle g(t - i) , \sum\limits_{n = -\infty}^{\infty} d_n g(t - n) \right\rangle\\
		&= \sum\limits_{n = -\infty}^{\infty} d_n \left\langle g(t - i) , g(t - n) \right\rangle\\
		&= \sum\limits_{n = -\infty}^{\infty} d_n \int\limits_{-\infty}^{\infty} g(t - i)^* g(t - n) \dif t
	\end{align*}
	Substituting $t' = t - i$,
	\begin{align*}
		c_i &= \sum\limits_{n = -\infty}^{\infty} d_n \int\limits_{-\infty}^{\infty} {g(t')}^* g(t' - n + i) \dif t'
	\end{align*}
	Let
	\begin{align*}
		y(t) &= g^*(t) \ast g(-t)
	\end{align*}
	Therefore,
	\begin{align*}
		c_i &= \sum\limits_{n = -\infty}^{\infty} d_n y(i - n)\\
		&= (d \ast y)[i]
	\end{align*}
	Hence,
	\begin{align*}
		c[n] &= d[i] \ast y[i]
	\end{align*}
	Therefore,
	\begin{align*}
		C\left( e^{j 2 \pi f} \right) &= D\left( e^{j 2 \pi f} \right) \DTFT\left\{ y[n] \right\}\\
		&= D\left( e^{j 2 \pi f} \right) \DTFT\left\{ g^*(t) \ast g(-t) \right\}\\
		&= D\left( e^{j 2 \pi f} \right) \sum\limits_{n = -\infty}^{\infty} G^*(f - n) G(f - n)\\
		&= D\left( e^{j 2 \pi f} \right) \sum\limits_{n = -\infty}^{\infty} \left| G(f - n) \right|^2
	\end{align*}
	Therefore,
	\begin{align*}
		D\left( e^{j 2 \pi f} \right) &= \frac{1}{\sum\limits_{n = -\infty}^{\infty} \left| G(f - n) \right|^2} C\left( e^{j 2 \pi f} \right)
	\end{align*}
\end{proof}

\begin{theorem}
	Let $X : \l2 \to H$ and $S : \l2 \to H$ be a set transformations corresponding to shift invariant functions
	\begin{align*}
		x_i &= g(t - i)
	\end{align*}
	and
	\begin{align*}
		y_i &= s(t - i)
	\end{align*}
	respectively.
	Then, if $S^* X$ is invertible, a generalized sampling and reconstruction using these basis elements can be performed as in \cref{fig:generalized_sampling_and_reconstruction_with_different_sampling_and_reconstruction_bases}.
	\begin{figure}[H]
		\centering
		\begin{adjustbox}{max width=\columnwidth}
		\begin{tikzpicture}[auto, node distance=2cm,]
			\node [block] (sampler) {$s(-t)$};
			\node [block, right = of sampler] (W) {$W = \left( S^* X \right)^{-1}$};
			\node [mixer, right = of W] (mixer) {};
			\node [block, right = of mixer] (reconstructor) {$g(t)$};

			\draw [stealth-] (sampler.west) -- ++(-1,0) node [left] {$x(t)$};
			\draw (sampler.east) to [switch] ++(1,0) [-stealth] -- (W.west) node [midway, above] {$c[n]$};
			\draw [-stealth] (W.east) -- (mixer.west) node [midway, above] {$d[n]$};
			\draw [stealth-] (mixer.south) -- ++(0,-1) node [below] {$\sum\limits_{n = -\infty}^{\infty} \delta(t - n)$};
			\draw [-stealth] (mixer.east) -- (reconstructor.west) node [right] {};
			\draw [-stealth] (reconstructor.east) -- ++(1,0) node [right] {$\tilde{x}(t)$};
		\end{tikzpicture}
		\end{adjustbox}
		\caption{Generalized Sampling and Reconstruction with Different Sampling and Reconstruction Bases}
		\label{fig:generalized_sampling_and_reconstruction_with_different_sampling_and_reconstruction_bases}
	\end{figure}
\end{theorem}

\begin{theorem}
	Let $\{s_n\}$ and $\{g_n\}$ span the Riesz bases $S$ and $X$ respectively.
	Assume
	\begin{align*}
		H &= {\range(S)}^{\perp} \cup \range(X)
	\end{align*}
	Then, $S^* X : \l2 \to \l2$ is invertible iff
	\begin{align*}
		{\range(S)}^{\perp} \cap \range(X) &= \{0\}
	\end{align*}
	Hence,
	\begin{align*}
		H &= {\range(S)}^{\perp} \oplus X
	\end{align*}
\end{theorem}

\section{Common Shift Invariant Riesz Bases}

\begin{definition}[Spline]
	A spline of order $n$ is defined as
	\begin{align*}
		x(t) &= \sum\limits_{k = -\infty}^{\infty} d_k \beta^n(t - k)
	\end{align*}
	where
	\begin{align*}
		\beta^0(t) &=
			\begin{cases}
				1 &;\quad |t| < 0.5\\
				\frac{1}{2} &;\quad |t| = 0.5\\
				0 &;\quad \text{otherwise}\\
			\end{cases}\\
		\beta^n(t) &= \beta^0(t) \ast \beta^{n - 1}(t)
	\end{align*}
	Hence, splines are a shift invariant Riesz basis.
	\index{spline}
\end{definition}

\begin{definition}[Lanczos interpolation]
	Lanczos interpolation with parameter $a$ is defined with basis elements $g(t)$ such that
	\begin{align*}
		g(t) &=
			\begin{cases}
				\sinc(t) \sinc\left( \frac{t}{a} \right) &;\quad |t| < a\\
				0 &;\quad \text{otherwise}\\
			\end{cases}
	\end{align*}
	Hence, the basis is a shift invariant Riesz basis.
	\index{Lanczos interpolation}
\end{definition}

\clearpage
\part{Resampling}

\section{Downsampling}

\begin{definition}[Downsampling operator]
	The downsampling operator $\downarrow$ is defined such that if
	\begin{align*}
		g &= \downsample{M} f
	\end{align*}
	then
	\begin{align*}
		g[n] &= f[M n]
	\end{align*}
	\index{downsampling}
\end{definition}

\begin{theorem}
	\begin{align*}
		\downsample{M} (\alpha f + \beta g) &= \alpha \downsample{M} f + \beta \downsample{M} g
	\end{align*}
	\index{downsampling}
\end{theorem}

\begin{theorem}
	\begin{align*}
		\downsample{M_1} \downsample{M_2} f &= \downsample{M_1 M_2} f
	\end{align*}
	\index{downsampling}
\end{theorem}

\begin{theorem}
	Let $D_p$ be a translation operator, i.e.
	\begin{align*}
		D_p f[n] &= f[n - p]
	\end{align*}
	Then,
	\begin{align*}
		D_p \downsample{M} f &= \downsample{M} D_{M p} f
	\end{align*}
	\index{downsampling}
\end{theorem}

\begin{theorem}
	Let $H$ be an LTI filter.
	Then,
	\begin{align*}
		H \downsample{M} &= \downsample{M} G
	\end{align*}
	where
	\begin{align*}
		G &= \sum\limits_{m = -\infty}^{\infty} h[m] D_{M m}
	\end{align*}
	with impulse response
	\begin{align*}
		g[m] &=
			\begin{cases}
				h\left[ \frac{m}{M} \right] &;\quad \frac{m}{M} \in \mathbb{Z}\\
				0 &;\quad \text{otherwise}\\
			\end{cases}
	\end{align*}
	where $h[n]$ is the impulse response of $H$.
	Also,
	\begin{align*}
		G(f) &= H(M f)
	\end{align*}
\end{theorem}

\begin{proof}
	\begin{align*}
		(H f)[n] &= (h \ast f)[n]\\
		&= \sum\limits_{m = -\infty}^{\infty} h[m] f[n - m]\\
		&= \left( \sum\limits_{m = -\infty}^{\infty} h[m] \delta[n - m] \right) \ast f[n]\\
		&= \left( \sum\limits_{m = -\infty}^{\infty} h[m] D_n \right) f[n]
	\end{align*}
	Therefore,
	\begin{align*}
		H \downsample{M} &= \sum\limits_{m = -\infty}^{\infty} h[m] (D_M \downsample{M})\\
		&= \sum\limits_{m = -\infty}^{\infty} (\downsample{M} D_{M m})\\
		&= \downsample{M} \left( \sum\limits_{m = -\infty}^{\infty} h[m] D_{M m} \right)\\
		&= \downsample{M} G
	\end{align*}
	where
	\begin{align*}
		G &= \sum\limits_{m = -\infty}^{\infty} h[m] D_{M m}
	\end{align*}
	Hence,
	\begin{align*}
		G &= \sum\limits_{m = -\infty}^{\infty} h[m] D_{M m}\\
		&= \sum\limits_{\frac{m'}{M} = -\infty}^{\infty} h\left[ \frac{m'}{M} \right] D_{m'}
	\end{align*}
	Hence,
	\begin{align*}
		g[m] &=
			\begin{cases}
				h\left[ \frac{m}{M} \right] &;\quad \frac{m}{M} \in \mathbb{Z}\\
				0 &;\quad \text{otherwise}\\
			\end{cases}
	\end{align*}
	Hence,
	\begin{align*}
		G(f) &= \sum\limits_{n = -\infty}^{\infty} g[n] e^{-2 \pi j f n}\\
		&= \sum\limits_{n = -\infty}^{\infty} h[n] e^{-2 \pi j f M n}\\
		&= H(M f)
	\end{align*}
\end{proof}

\begin{theorem}[Noble Identity for Downsampling]
	\begin{align*}
		h \ast (\downsample{M} f) &= \downsample{M} \left( (\upsample{M} h) \ast f \right)
	\end{align*}
	Equivalently, downsampling by $M$ and then applying $H(f)$ is equivalent to applying $H(f)$ and then downsampling by $M$.
	\index{Noble Identity!downsampling}
	\label{thm:Noble_identity_for_downsampling}
\end{theorem}

\section{Upsampling}

\begin{definition}[Upsampling operator]
	The upsampling operator $\uparrow$ is defined such that if
	\begin{align*}
		g &= \upsample{M} f
	\end{align*}
	then
	\begin{align*}
		g[n] &=
			\begin{cases}
				f\left[ \frac{n}{M} \right] &;\quad \frac{n}{M} \in \mathbb{Z}\\
				0 &;\quad \text{otherwise}\\
			\end{cases}
	\end{align*}
	\index{upsampling}
\end{definition}

\begin{theorem}
	\begin{align*}
		\upsample{M_1} \upsample{M_2} f &= \upsample{M_1 M_2} f
	\end{align*}
	\index{upsampling}
\end{theorem}

\begin{theorem}
	\begin{align*}
		\upsample{M} (\alpha f + \beta g) &= \alpha \upsample{M} f + \beta \upsample{M} g
	\end{align*}
	\index{upsampling}
\end{theorem}

\begin{theorem}
	Let $D_p$ be a translation operator, i.e.
	\begin{align*}
		D_p f[n] &= f[n - p]
	\end{align*}
	Then,
	\begin{align*}
		\upsample{M} D_p f &= D_{M p} \upsample{M} f
	\end{align*}
	\index{upsampling}
\end{theorem}

\begin{theorem}
	Let $H$ be an LTI filter.
	Then,
	\begin{align*}
		\upsample{M} H &= G \upsample{M}
	\end{align*}
	where
	\begin{align*}
		G &= \sum\limits_{m = -\infty}^{\infty} h[m] D_{M m}
	\end{align*}
	with impulse response
	\begin{align*}
		g[m] &=
			\begin{cases}
				h\left[ \frac{m}{M} \right] &;\quad \frac{m}{M} \in \mathbb{Z}\\
				0 &;\quad \text{otherwise}\\
			\end{cases}
	\end{align*}
	where $h[n]$ is the impulse response of $H$.
	Also,
	\begin{align*}
		G(f) &= H(M f)
	\end{align*}
\end{theorem}

\begin{theorem}[Noble Identity for Upsampling]
	\begin{align*}
		\upsample{M} (h \ast f) &= \left( \upsample{M} h \right) \ast \left( \upsample{M} f \right)
	\end{align*}
	Equivalently, applying $H(f)$ and then upsampling by $M$ is equivalent to upsampling by $M$ and then applying $H(M f)$.
	\index{Noble Identity!upsampling}
	\label{thm:Noble_identity_for_upsampling}
\end{theorem}

\section{Polyphase Representation}

\begin{theorem}[Type 1 Polyphase Decomposition]
	\begin{align*}
		\sum\limits_{n = -\infty}^{\infty} f[n] &= \sum\limits_{m = 0}^{M - 1} \sum\limits_{n = -\infty}^{\infty} f[M n + m]
	\end{align*}
	Therefore, for a linear shift invariant system
	\begin{align*}
		H &= \sum\limits_{n = -\infty}^{\infty} h[n] D_n
	\end{align*}
	the polyphase component of $h[n]$ with respect to $M$ can be defined as
	\begin{align*}
		e_m[n] &= h[M n + m]
	\end{align*}
	Hence,
	\begin{align*}
		(h \ast f)[n] &= \sum\limits_{m = 0}^{M - 1} (\upsample{M} e_m) \ast f[n - m]
	\end{align*}
	and equivalently,
	\begin{align*}
		H(f) &= \sum\limits_{m = 0}^{M - 1} E_m(M f) e^{-2 \pi j f n}
	\end{align*}
	where
	\begin{align*}
		E_m &= \FT\left\{ e_m \right\}
	\end{align*}
	\index{polyphase decomposition!type 1}
	\label{thm:type_1_polyphase_decomposition}
\end{theorem}

\begin{proof}
	Let $\varepsilon_m$ be a system with impulse response $e_m[n]$.
	Therefore,
	\begin{align*}
		\varepsilon_m &= \sum\limits_{n = -\infty}^{\infty} e_m[n] D_n\\
		&= \sum\limits_{n = -\infty}^{\infty} h[M n + m]
	\end{align*}
	Therefore,
	\begin{align*}
		H &= \sum\limits_{m = -}^{M - 1} \sum\limits_{n = -\infty}^{\infty} h[M n + m] D_{M n + m}\\
		&= \sum\limits_{m = 0}^{M - 1} \left( \sum\limits_{n = -\infty}^{\infty} h[M n + m] D_{M n} \right) D_m\\
		&= \sum\limits_{m = 0}^{M - 1} \left( \sum\limits_{n = -\infty}^{\infty} e_m[n] D_{M n} \right) D_m
	\end{align*}
	Therefore,
	\begin{align*}
		(h \ast f)[n] &= \sum\limits_{m = 0}^{M - 1} (\upsample{M} e_m) \ast f[n - m]
	\end{align*}
	and equivalently,
	\begin{align*}
		H(f) &= \sum\limits_{m = 0}^{M - 1} E_m(M f) e^{-2 \pi j f m}
	\end{align*}
	where
	\begin{align*}
		E_m &= \FT\left\{ e_m \right\}
	\end{align*}
\end{proof}

\begin{theorem}[Type 2 Polyphase Decomposition]
	\begin{align*}
		\sum\limits_{n = -\infty}^{\infty} f[n] &= \sum\limits_{m = 0}^{M - 1} \sum\limits_{n = -\infty}^{\infty} f[M n + m]
	\end{align*}
	Therefore, for a linear shift invariant system
	\begin{align*}
		H &= \sum\limits_{n = -\infty}^{\infty} h[n] D_n
	\end{align*}
	the polyphase component of $h[n]$ with respect to $M$ can be defined as
	\begin{align*}
		r_m[n] &= h[M n - m]
	\end{align*}
	Hence,
	\begin{align*}
		(h \ast f)[n] &= \sum\limits_{m = 0}^{M - 1} \left( \left( \upsample{M} r_m \right) \ast f \right) [n + m]
	\end{align*}
	and equivalently,
	\begin{align*}
		H(f) &= \sum\limits_{m = 0}^{M - 1} R_m(M f) e^{-j 2 \pi f (M - 1 - m)}
	\end{align*}
	where
	\begin{align*}
		R_m &= \FT\left\{ r_m \right\}
	\end{align*}
	\index{polyphase decomposition!type 2}
	\label{thm:type_2_polyphase_decomposition}
\end{theorem}

\begin{proof}
	Let $\varepsilon_m$ be a system with impulse response $e_m[n]$.
	Therefore,
	\begin{align*}
		\varepsilon_m &= \sum\limits_{n = -\infty}^{\infty} e_m[n] D_n\\
		&= \sum\limits_{n = -\infty}^{\infty} h[M n + m]
	\end{align*}
	Therefore,
	\begin{align*}
		H &= \sum\limits_{m = -}^{M - 1} \sum\limits_{n = -\infty}^{\infty} h[M n + m] D_{M n + m}\\
		&= \sum\limits_{m = 0}^{M - 1} \left( \sum\limits_{n = -\infty}^{\infty} h[M n + m] D_{M n} \right) D_m\\
		&= \sum\limits_{m = 0}^{M - 1} \left( \sum\limits_{n = -\infty}^{\infty} e_m[n] D_{M n} \right) D_m
	\end{align*}
	Therefore,
	\begin{align*}
		(h \ast f)[n] &= \sum\limits_{m = 0}^{M - 1} (\upsample{M} e_m) \ast f[n - m]
	\end{align*}
	and equivalently,
	\begin{align*}
		H(f) &= \sum\limits_{m = 0}^{M - 1} E_m(M f) e^{-2 \pi j f m}
	\end{align*}
	where
	\begin{align*}
		E_m &= \FT\left\{ e_m \right\}
	\end{align*}
	Let
	\begin{align*}
		r_m[n] &= e_{-m}[n]\\
		&= h[M n - m]
	\end{align*}
	Therefore,
	\begin{align*}
		H &= \sum\limits_{m = 0}^{M - 1} D_{-m} \left( \sum\limits_{n = -\infty}^{\infty} r_m[n] D_{M n} \right)\\
		&= \sum\limits_{m = 0}^{M - 1} D_{-m} \left( \sum\limits_{n = -\infty}^{\infty} \left( \upsample{M} r_m \right)[n] D_n \right)
	\end{align*}
	Therefore,
	\begin{align*}
		(h \ast f)[n] &= \sum\limits_{m = 0}^{M - 1} \left( \left( \upsample{M} r_m \right) \ast f \right)[n + m]
	\end{align*}
\end{proof}

\section{Efficient Implementation of Decimators}

\begin{theorem}
	Let $H$ be a linear shift invariant system with impulse response $h[n]$, and polyphase component
	\begin{align*}
		e_m[n] &= h[M n - m]
	\end{align*}
	Then,
	\begin{align*}
		\downsample{M} H &= \sum\limits_{m = 0}^{M - 1} \varepsilon_m \downsample{M} D_m
	\end{align*}
	where $\varepsilon_m$ is the system with impulse response $e_m[n]$.\\
	Therefore, for a signal $x \in \mathbb{R}^{N^d}$ and a filter $h \in \mathbb{R}^{k^d}$, the cost of applying the LHS is $\O\left( N^d k^d \right)$, while the cost of applying the RHS is $\O\left( \frac{N^d k^d}{M^d} \right)$, i.e. the RHS is more efficient by a factor of $M^d$.
\end{theorem}

\begin{proof}
	\begin{align*}
		\downsample{M} H f &= \sum\limits_{m = 0}^{M - 1} \upsample{M} \varepsilon_m \ast D_m f\\
	\end{align*}
	Therefore, by \cref{thm:Noble_identity_for_downsampling},
	\begin{align*}
		\downsample{M} H f &= \sum\limits_{m = 0}^{M - 1} e_m \ast \downsample{M} D_m f\\
		&= \sum\limits_{m = 0}^{M - 1} \varepsilon_m \downsample{M} D_m f
	\end{align*}
	Hence,
	\begin{align*}
		\downsample{M} H &= \sum\limits_{m = 0}^{M - 1} \varepsilon_m \downsample{M} D_m
	\end{align*}
\end{proof}

\section{Efficient Implementation of Interpolation}

\begin{theorem}
	Let $H$ be a linear shift invariant system with impulse response $h[n]$, and polyphase component
	\begin{align*}
		e_m[n] &= h[M n - m]
	\end{align*}
	Then,
	\begin{align*}
		H \upsample{M} &= \sum\limits_{m = 0}^{M - 1} D_{-m} \upsample{M} \mathcal{R}_m
	\end{align*}
	where $\mathcal{R}_m$ is the system with impulse response $r_m[n]$.\\
\end{theorem}

\begin{proof}
	\begin{align*}
		H \upsample{M} f &= \sum\limits_{m = 0}^{M - 1} D_{-m} \left( \upsample{M} r_m \ast \upsample{M} f \right)
	\end{align*}
	Therefore, by \cref{thm:Noble_identity_for_upsampling},
	\begin{align*}
		H \upsample{M} f &= \sum\limits_{m = 0}^{M - 1} D_{-m} \upsample{M} r_m \ast f\\
		&= \sum\limits_{m = 0}^{M - 1} D_{-m} \upsample{M} \mathcal{R}_m f
	\end{align*}
	Hence,
	\begin{align*}
		H \upsample{M} &= \sum\limits_{m = 0}^{M - 1} D_{-m} \upsample{M} \mathcal{R}_m
	\end{align*}
\end{proof}

\clearpage
\part{Short Time Fourier Transform}

\section{Short Time Fourier Transform}

\begin{definition}[Short time Fourier transform]
	The short time Fourier transform of a signal $f(t)$ is defined to be
	\begin{align*}
		\hat{x}(f,\tau) &= \int\limits_{-\infty}^{\infty} x(t) w(t - \tau) e^{-j 2 \pi f t} \dif t
	\end{align*}
	where $w(t)$ is the window function.\\
	Equivalently,
	\begin{align*}
		x(t) w(t - \tau) &= \int\limits_{-\infty}^{\infty} \hat{x}(f,\tau) e^{j 2 \pi f t} \dif f
	\end{align*}
	This allows for localization in the time domain which is not possible when using Fourier transform.
	\index{transform!Fourier!short time}
\end{definition}

\begin{definition}[Gabor transform]
	The short time Fourier transform with window function
	\begin{align*}
		w(t) &= e^{-\frac{t^2}{\sigma^2}}
	\end{align*}
	is called the Gabor transform.
	\index{transform!Gabor}
\end{definition}

\section{Inverse Short Time Fourier Transform}

\begin{theorem}
	\begin{align*}
		x(t) &= \IFT\left\{ \int\limits_{-\infty}^{\infty} x(f,\tau) \dif \tau \right\}
	\end{align*}
	\index{transform!Fourier!short time}
\end{theorem}

\begin{proof}
	Let $w(t)$ be a window function such that
	\begin{align*}
		\int\limits_{-\infty}^{\infty} w(t) \dif t &= 1
	\end{align*}
	Therefore, for any $\tau \in \mathbb{R}$,
	\begin{align*}
		\int\limits_{-\infty}^{\infty} w(t - \tau) \dif t &= 1\\
		\int\limits_{-\infty}^{\infty} w(t - \tau) \dif \tau &= 1
	\end{align*}
	Therefore,
	\begin{align*}
		\int\limits_{-\infty}^{\infty} \hat{x}(f,\tau) \dif \tau &= \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} x(t) w(t - \tau) e^{-j 2 \pi f t} \dif t \dif \tau\\
		&= \int\limits_{-\infty}^{\infty} x(t) e^{-j 2 \pi f t} \left( \int\limits_{-\infty}^{\infty} w(t - \tau) \dif \tau \right) \dif t\\
		&= \int\limits_{-\infty}^{\infty} x(t) e^{-j 2 \pi f t} (1) \dif t\\
		&= X(f)
	\end{align*}
	Therefore,
	\begin{align*}
		x(t) &= \IFT\left\{ \int\limits_{-\infty}^{\infty} x(f,\tau) \dif \tau \right\}
	\end{align*}
\end{proof}

\begin{theorem}
	\begin{align*}
		\IFT\left\{ x(t,\tau) \right\} &= x(t) w(t - \tau)
	\end{align*}
	\index{transform!Fourier!short time}
\end{theorem}

\begin{proof}
	\begin{align*}
		x(t) &= \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} x(f,\tau) e^{2 \pi f t} \dif \tau \dif f\\
		&= \int\limits_{-\infty}^{\infty} \left( \int\limits_{-\infty}^{\infty} x(f,\tau) e^{j 2 \pi f t} \dif f \right) \dif \tau
	\end{align*}
	Also,
	\begin{align*}
		x(t) &= \int\limits_{-\infty}^{\infty} x(t) w(t - \tau) \dif \tau
	\end{align*}
	Therefore,
	\begin{align*}
		\IFT\left\{ x(t,\tau) \right\} &= x(t) w(t - \tau)
	\end{align*}
\end{proof}

\section{Discrete Short Time Fourier Transform}

\begin{theorem}
	Assume constant overlap-add (COLA) for hop size $R$, i.e.
	\begin{align*}
		\sum\limits_{m = -\infty}^{\infty} w(n - m R) &= 1
	\end{align*}
	Then,
	\begin{align*}
		x(n) &= \IDTFT\left\{ \sum\limits_{m = -\infty}^{\infty} X_m(f) \right\}
	\end{align*}
	where
	\begin{align*}
		X_m(f) &= \DTFT\left\{ x(t) D_{m R}\left( w(t) \right) \right\}
	\end{align*}
\end{theorem}

\begin{proof}
	For simplicity, assume that the sampling is performed with $T = 1$.
	Hence, let
	\begin{align*}
		X_m(f) &= \DTFT\left\{ x(t) D_{m R}\left( w(t) \right) \right\}\\
		&= \sum\limits_{n = -\infty}^{\infty} x(n) w(n - m R) e^{-j 2 \pi f n}
	\end{align*}
	where
	\begin{align*}
		D_{m R}(w) &= w(n - m R)
	\end{align*}
	and $R$ is called the hop size.\\
	Assume constant overlap-add (COLA) for hop size $R$, i.e.
	\begin{align*}
		\sum\limits_{m = -\infty}^{\infty} w(n - m R) &= 1
	\end{align*}
	Therefore,
	\begin{align*}
		\sum\limits_{m = -\infty}^{\infty} X_m(f) &= \sum\limits_{m = -\infty}^{\infty} \sum\limits_{n = -\infty}^{\infty} x(n) w(n - m R) e^{-j 2 \pi f n}\\
		&= \sum\limits_{n = -\infty}^{\infty} x(n) e^{-j 2 \pi f n} \sum\limits_{m = -\infty}^{\infty} w(n - m R)\\
		&= \sum\limits_{n = -\infty}^{\infty} x(n) e^{-j 2 \pi f n} (1)\\
		&= \DTFT\left\{ x(n) \right\}
	\end{align*}
	Therefore,
	\begin{align*}
		x(n) &= \IDTFT\left\{ \sum\limits_{m = -\infty}^{\infty} X_m(f) \right\}
	\end{align*}
\end{proof}

\subsection{Discrete STFT using Fast Fourier Transform}

\begin{definition}[Discrete short time Fourier transform using FFT]
	Let
	\begin{align*}
		\tilde{x_m(n)} &= x(n + m R)
	\end{align*}
	for $n \in \mathbb{Z}$, and $-M \le n \le M$, where $M$ is called the frame length.
	Then,
	\begin{align*}
		X_m(f) &= \FT\left\{ x_m(t) \right\}\\
		&= e^{-j 2 \pi f m R} \DTFT\left\{ D_{-m R}\left( x(t) \right) w(t) \right\}\\
		&= e^{-j 2 \pi f m R} \DTFT\left\{ \tilde{x_m}(n) w(n) \right\}
	\end{align*}
	Hence, let
	\begin{align*}
		\tilde{x_m}^{w}(n) &=
			\begin{cases}
				\tilde{x_m}(n) w(n) &;\quad |n| \le M\\
				0 &;\quad M < n \le \frac{N}{2} - 1\\
				0 &;\quad -\frac{N}{2} \le n < -M\\
			\end{cases}
	\end{align*}
	Then,
	\begin{align*}
		\FFT_{k}\left\{ \tilde{x_m}^{w}(n) \right\} &= e^{-j 2 \pi \frac{k}{N} m R} \FFT\left\{ \tilde{x_m}^w(n) \right\}
	\end{align*}
	is called the time normalized STFT, where $k$ is called the bin number, and $m$ is called the frame number, $N$ is the size of the FFT, and $M_w = 2 M + 1$ is the size of the filter $w(n)$.
	\index{transform!Fourier!discrete short time!using FFT}
\end{definition}

\subsection{Discrete STFT using Filter Bank Summation Interpolation}

\begin{definition}[Discrete short time Fourier transform using filter banks]
	\begin{align*}
		x_m(k) &= \sum\limits_{n = -\infty}^{\infty} x(n) e^{-j 2 \pi \frac{k}{N} n} w(n - m R)
	\end{align*}
	Hence, let
	\begin{align*}
		x_k(n) &= x(n) e^{-j 2 \pi \frac{k}{N} n}
	\end{align*}
	Therefore,
	\begin{align*}
		x_m(k) &= x_k(m R) \ast w(-m R)
	\end{align*}
	Hence, $x_m(k)$ can be obtained by passing $x(n)$ through a filter bank as in \cref{fig:discrete_STFT_implementation_using_filter_banks}.
	\begin{figure}[H]
		\centering
		\begin{adjustbox}{max width=\columnwidth}
		\begin{tikzpicture}[auto, node distance=2cm,]
			\node [mixer] (mixer1) at (0,2) {};
			\node [mixer] (mixern) at (0,-2) {};
			\node at (0,0) {$\vdots$};

			\node [block, right = of mixer1] (filter1) {$w[-n]$};
			\node [block, right = of mixern] (filtern) {$w[-n]$};

			\node [block, right = of filter1] (downsampler1) {$\downsample{R}$};
			\node [block, right = of filtern] (downsamplern) {$\downsample{R}$};

			\draw [-stealth] (-2,0) node [left] {$x[n]$} -| (-1,2) -- (mixer1.west);
			\draw [-stealth] (-2,0) node [left] {$x[n]$} -| (-1,-2) -- (mixern.west);

			\draw [-stealth] (mixer1.east) -- (filter1.west);
			\draw [-stealth] (mixern.east) -- (filtern.west);

			\draw [-stealth] (filter1.east) -- (downsampler1.west);
			\draw [-stealth] (filtern.east) -- (downsamplern.west);

			\draw [-stealth] (downsampler1.east) -- ++(1,0) node [right] {$x_m(0)$};
			\draw [-stealth] (downsamplern.east) -- ++(1,0) node [right] {$x_m(n - 1)$};

			\draw [stealth-] (mixer1.north) -- ++(0,1) node [above] {$e^{-j 2 \pi \frac{0}{N} n}$};
			\draw [stealth-] (mixern.south) -- ++(0,-1) node [below] {$e^{-j 2 \pi \frac{N - 1}{N} n}$};
		\end{tikzpicture}
		\end{adjustbox}
		\caption{Discrete STFT Implementation using Filter Banks}
		\label{fig:discrete_STFT_implementation_using_filter_banks}
	\end{figure}
	\index{transform!Fourier!discrete short time!using filter banks}
\end{definition}

\clearpage
\part{Wavelets}

\begin{definition}[Orthogonal wavelets]
	Let $\psi: \mathbb{R} \to \mathbb{C}$, and
	\begin{align*}
		\psi_{n,k}(t) &= 2^{\frac{n}{2}} \psi\left( 2^n t - k \right)
	\end{align*}
	where $n, k \in \mathbb{Z}$.
	Such a family of $\psi_{n,k}$, which is orthonormal, and
	\begin{align*}
		L_2 &= {\vspan{\left\{ \psi_{n,k} \right\}}}^{C}
	\end{align*}
	is called a family of orthonormal wavelets, and $\psi$ called the mother wavelet.
	\index{wavelet!orthogonal}
	\index{wavelet!mother}
\end{definition}

\section{The Haar Wavelet}

\begin{definition}
	The family of wavelets defined by the mother wavelet
	\begin{align*}
		\psi(t) &= u_{\left[ 0, \frac{1}{2} \right]}(t) - u_{\left[ \frac{1}{2},1 \right]}(t)
	\end{align*}
	and
	\begin{align*}
		\psi_{n,k}(t) &= 2^{\frac{n}{2}} \psi\left( 2^n t - k \right)\\
		&= 2^{\frac{n}{2}} \left( u_{\left[ \frac{k}{2^n} , \frac{k + \frac{1}{2}}{2^n} \right]} - u_{\left[ \frac{k + \frac{1}{2}}{2^n} , \frac{k + 1}{2^n} \right]} \right)
	\end{align*}
	is called the family of Haar wavelets.
	\index{wavelet!Haar}
\end{definition}

\begin{theorem}
	The set of Haar wavelets $\left\{ \psi_{i,k} \right\}$ is orthonormal.
	\index{wavelet!Haar}
\end{theorem}

\begin{theorem}
	For Haar wavelets,
	\begin{align*}
		u_{[0,1]}(t) &= \sum\limits_{i < 0} 2^{\frac{i}{2}} \psi_{i,0}(t)
	\end{align*}
	\index{wavelet!Haar}
\end{theorem}

\begin{theorem}
	Let $U$ be a subspace of $V$.
	Then,
	\begin{align*}
		P_V P_U &= P_U
	\end{align*}
	and
	\begin{align*}
		P_U P_V &= P_V
	\end{align*}
	where $P_U$ and $P_V$ are orthogonal projections onto $U$ and $V$ respectively.
\end{theorem}

\begin{proof}
	Let $x \in H$, where $H$ is an Hilbert space.\\
	Hence, let
	\begin{align*}
		P_U x &= x_U
	\end{align*}
	Therefore,
	\begin{align*}
		P_V P_U x &= P_V x_U
	\end{align*}
	Also, $U \subset V$.
	Hence,
	\begin{align*}
		P_V x_U &= x_u
	\end{align*}
	Therefore,
	\begin{align*}
		P_V P_U x &= P_U x
	\end{align*}
	~\\
	$P_U$ and $P_V$ are orthogonal projections.
	Hence,
	\begin{align*}
		{P_U}^* &= P_U\\
		{P_V}^* &= P_V
	\end{align*}
	Therefore,
	\begin{align*}
		P_U &= {P_U}^*\\
		&= (P_V P_U)^*\\
		&= {P_U}^* {P_V}^*\\
		&= P_U P_V
	\end{align*}
\end{proof}

\section{Multiresolution Analysis}

\begin{definition}
	Let $\{V_j\}$ be a sequence of subspaces of $L_2(\mathbb{R})$.
	It is said to be a multiresolution analysis (MRA) if
	\begin{enumerate}
		\item
			\begin{align*}
				V_j &\subset V_{j + 1}
			\end{align*}
			for all $j \in \mathbb{Z}$.
		\item
			\begin{align*}
				\bigcup_{j \in \mathbb{Z}} V_j &= L_2(\mathbb{R})
			\end{align*}
		\item
			\begin{align*}
				\bigcap_{j \in \mathbb{Z}} V_j &= \{0\}
			\end{align*}
		\item
			\begin{align*}
				f(t) &\in V_i
			\end{align*}
			if and only if
			\begin{align*}
				f\left( 2^{-i} t \right) &\in V_0
			\end{align*}
		\item
			\begin{align*}
				f(t) &\in V_0
			\end{align*}
			if and only if
			\begin{align*}
				f(t - k) &\in V_0
			\end{align*}
			for all $k \in \mathbb{Z}$.
		\item
			$\exists \varphi \in V_0$ such that $\left\{ \varphi(t - k) \right\}_{k \in \mathbb{Z}}$ is an orthonormal basis of $V_0$.
			Such a $\varphi(t)$ is said to be the scaling function of the MRA.
	\end{enumerate}
	\index{multiresolution analysis}
	\index{scaling function}
\end{definition}

\section{Scaling Function}

\begin{theorem}[Scaling equations]
	For a scaling function $\varphi(t)$,
	\begin{align*}
		\varphi(t) &= \sum\limits_{k = -\infty}^{\infty} a_k \varphi(2 t - k)
	\end{align*}
	Equivalently,
	\begin{align*}
		\varphi\left( \frac{t}{2} \right) &= \sum\limits_{k = -\infty}^{\infty} \varphi(t - k)
	\end{align*}
	Hence, taking the Fourier transform,
	\begin{align*}
		\hat{\varphi}(2 f) &= m_{\varphi}(f) \hat{\varphi}(f)
	\end{align*}
	and equivalently,
	\begin{align*}
		\hat{\varphi}(f) &= m_{\varphi}\left( \frac{f}{2} \right) \hat{\varphi}\left( \frac{f}{2} \right)
	\end{align*}
	where
	\begin{align*}
		m_{\varphi}(f) &= \frac{1}{2} \sum\limits_{k = -\infty}^{\infty} a_k e^{-j 2 \pi f k}
	\end{align*}
	is the DTFT of the sequence $a_k$, multiplied by $\frac{1}{2}$.
	\label{thm:scaling_equations}
	\index{scaling equations}
	\index{scaling function}
\end{theorem}

\begin{theorem}
	Let $\left\{ \varphi(t - n) \right\}$ be orthonormal in $L_2$.
	Then,
	\begin{align*}
		\left| m_{\varphi}(f) \right|^2 + \left| m_{\varphi}\left( f + \frac{1}{2} \right) \right|^2 &= 1
	\end{align*}
	\index{scaling function}
\end{theorem}

\begin{proof}
	As $\varphi(t - n)$ are orthonormal,
	\begin{align*}
		1 &= \sum\limits_{k = -\infty}^{\infty} \left| \hat{\varphi}(f + k) \right|^2\\
		&= \sum\limits_{k = -\infty}^{\infty} \left| \hat{\varphi}\left( \frac{f}{2} + \frac{k}{2} \right) \right|^2 \left| m_{\varphi}\left( \frac{f}{2} + \frac{k}{2} \right) \right|^2\\
		&= \quad\sum\limits_{k = -\infty}^{\infty} \left| \hat{\varphi}\left( \frac{f}{2} + k \right) \right|^2 \left| m_{\varphi}\left( \frac{f}{2} + k \right) \right|^2\\
		&\quad + \sum\limits_{k = -\infty}^{\infty} \left| \hat{\varphi}\left( \frac{f}{2} + k + \frac{1}{2} \right) \right|^2 \left| m_{\varphi}\left( \frac{f}{2} + k + \frac{1}{2} \right) \right|^2
	\end{align*}
	Hence, as $m_{\varphi}(f + k) = m_{\varphi}(f)$,
	\begin{align*}
		1 &= \left| m_{\varphi}\left( \frac{f}{2} \right) \right|^2 \sum\limits_{k = -\infty}^{\infty} \left| \hat{\varphi}\left( \frac{f}{2} + k \right) \right| + \left| m_{\varphi}\left( \frac{f}{2} + \frac{1}{2} \right) \right|^2 \sum\limits_{k = -\infty}^{\infty} \left| \hat{\varphi}\left( \frac{f}{2} + \frac{1}{2} + k \right) \right|^2
	\end{align*}
	Hence, as $\varphi(t - n)$ are orthonormal,
	\begin{align*}
		1 &= \left| m_{\varphi}\left( \frac{f}{2} \right) \right|^2 + \left| m_{\varphi}\left( \frac{f}{2} + \frac{1}{2} \right) \right|^2
	\end{align*}
	Hence, replacing $\frac{f}{2}$ by $f$,
	\begin{align*}
		1 &= \left| m_{\varphi}(f) \right|^2 + \left| m_{\varphi}\left( f + \frac{1}{2} \right) \right|^2
	\end{align*}
\end{proof}

\begin{theorem}
	Let $\varphi$ be an orthonormal basis.
	Given the scaling equations with $m_{\varphi}$ and given that $\hat{\varphi}(f)$ is continuous at $f = 0$ and
	\begin{align*}
		\hat{\varphi}(0) &\neq 0
	\end{align*}
	then
	\begin{align*}
		V_0 &= {\vspan\left\{ \varphi(t - n) \right\}}^{C}
	\end{align*}
	and
	\begin{align*}
		V_j &= {\left\{ g\left( 2^j t \right) \Big| g \in V_0 \right\}}^{C}\\
		&= \vspan\left\{ \varphi\left( 2^j t - k \right) \right\}^{C}
	\end{align*}
	is a MRA.
	\index{multiresolution analysis}
\end{theorem}

\begin{theorem}[Poisson summation formula]
	Let $\varphi(t) \in L_2$, and let
	\begin{align*}
		f(t) &= \lim\limits_{N \to \infty} \sum\limits_{k = -N}^{N} \varphi(t - k)
	\end{align*}
	Then,
	\begin{align*}
		\int\limits_{0}^{1} f(t) e^{-j 2 \pi n t} &= \int\limits_{-\infty}^{\infty} \varphi(t) e^{-j 2 \pi n t} \dif t\\
		&= \hat{\varphi}(n)
	\end{align*}
	\label{thm:Poisson_summation_formula}
\end{theorem}

\begin{theorem}
	Let $\{V_j\}$ be a MRA with scaling function $\varphi(t)$.
	Assume $\hat{\varphi}(f)$ is continuous at $f = 0$.
	Then,
	\begin{enumerate}
		\item
			\begin{align*}
				\left| \hat{\varphi}(0) \right| &= 1
			\end{align*}
		\item
			\begin{align*}
				\hat{\varphi}(k) &= 0
			\end{align*}
			for all $k \in \mathbb{Z} \setminus \{0\}$.
		\item
			\begin{align*}
				\sum\limits_{k = -\infty}^{\infty} \varphi(t + k) &= \alpha
			\end{align*}
			where $|\alpha| = 1$.
	\end{enumerate}
	\index{multiresolution analysis}
	\index{scaling function}
\end{theorem}

\section{Wavelets and Projections}

\begin{theorem}
	Let $U$ be a subspace of $V$.
	Then,
	\begin{align*}
		P_V P_U &= P_U
	\end{align*}
	and
	\begin{align*}
		P_U P_V &= P_V
	\end{align*}
	where $P_U$ and $P_V$ are orthogonal projections onto $U$ and $V$ respectively.
	\index{projection!orthogonal}
\end{theorem}

\begin{proof}
	Let $x \in H$, where $H$ is an Hilbert space.\\
	Hence, let
	\begin{align*}
		P_U x &= x_U
	\end{align*}
	Therefore,
	\begin{align*}
		P_V P_U x &= P_V x_U
	\end{align*}
	Also, $U \subset V$.
	Hence,
	\begin{align*}
		P_V x_U &= x_u
	\end{align*}
	Therefore,
	\begin{align*}
		P_V P_U x &= P_U x
	\end{align*}
	~\\
	$P_U$ and $P_V$ are orthogonal projections.
	Hence,
	\begin{align*}
		{P_U}^* &= P_U\\
		{P_V}^* &= P_V
	\end{align*}
	Therefore,
	\begin{align*}
		P_U &= {P_U}^*\\
		&= (P_V P_U)^*\\
		&= {P_U}^* {P_V}^*\\
		&= P_U P_V
	\end{align*}
\end{proof}

\begin{theorem}
	Let $U \subset V$,
	Then, the orthogonal projection onto $V \setminus U$ is $P_V - P_U$.
	\index{projection!orthogonal}
\end{theorem}

\begin{proof}
	If $x \in V \setminus U$, then $x \in V$ and $x \in U^{\perp}$.
	Therefore,
	\begin{align*}
		(P_V - P_U) x &= P_V x - P_U x\\
		&= x - 0\\
		&= x
	\end{align*}
	~\\
	If $x \notin V \setminus U$, then $x \in V^{\perp} \cup U$.
	Therefore,
	\begin{align*}
		(P_V - P_U) x &= P_V x - P_U x
	\end{align*}
	Also, as $U \subset V$, $V^{\perp} \subset U^{\perp}$.
	Hence, if $x \in V^{\perp}$, then $x \in U^{\perp}$.
	Therefore,
	\begin{align*}
		(P_V - P_U) x &= P_V x - P_U x\\
		&= 0 - 0\\
		&= 0
	\end{align*}
	Similarly, if $x \in U$, then $x \in V$.
	Therefore,
	\begin{align*}
		(P_V - P_U) x &= P_V x - P_U x\\
		&= x - x\\
		&= 0
	\end{align*}
\end{proof}

\begin{theorem}
	\begin{align*}
		W_n &\perp W_k
	\end{align*}
	for $n \neq k$.
	\index{wavelets}
\end{theorem}

\begin{proof}
	Any function in $W_n$ is of the form $(P_{n + 1} - P_n) f$.
	Therefore, for all $n \neq k$,
	\begin{align*}
		\left\langle (P_{n + 1} - P_n) f , (P_{k + 1} - P_k) f \right\rangle &= \left\langle f , (P_{n + 1} - P_n)^* (P_{k + 1} - P_k) f \right\rangle\\
		&= \left\langle f , (P_{n + 1} P_{k + 1} - P_n P_{k + 1} - P_{n + 1} P_k + P_n P_k) \right\rangle\\
		&= \left\langle f , (P_{k + 1} - P_{k + 1} - P_k + P_k) \right\rangle\\
		&= \langle f , 0 \rangle\\
		&= 0
	\end{align*}
	Hence $W_n \perp W_k$.
\end{proof}
%
\begin{theorem}
	\begin{align*}
		\bigoplus W_n &= L_2
	\end{align*}
	that is, for any $f \in L_2$, $\exists f_i \in W_i$ such that
	\begin{align*}
		f &= \sum\limits_{i = -\infty}^{\infty} f_i
	\end{align*}
	and the set of $f_i$s is unique.
	\index{wavelets}
\end{theorem}

\begin{proof}
	For
	\begin{align*}
		f &= \sum\limits_{i = -\infty}^{\infty} f_i
	\end{align*}
	to hold,
	\begin{align*}
		\lim\limits_{m, n \to \infty} \left\| f - \sum\limits_{i = m}^{n - 1} f_i \right\| &= 0
	\end{align*}
	Also,
	\begin{align*}
		f &= (f - P_n f) + P_n f - P_m f + (P_m f)
	\end{align*}
	Hence, as $\bigcup_{i \in \mathbb{Z}} V_i = L_2$,
	\begin{align*}
		\lim\limits_{n \to \infty} \left\| f - P_n f \right\| &= 0
	\end{align*}
	Similarly, as $\bigcap_{m \to \infty} V_i = \{0\}$,
	\begin{align*}
		\lim\limits_{m \to \infty} \|P_m f\| &= 0
	\end{align*}
	Also,
	\begin{align*}
		P_n f - P_m f &= \sum\limits_{i = m}^{n - 1} (P_{i + 1} - P_i) f
	\end{align*}
	Hence, let
	\begin{align*}
		f_i &= (P_{i + 1} - P_i) f
	\end{align*}
	Therefore, $f_i \in W_i$.
	Therefore,
	\begin{align*}
		f - (P_n f - P_m f) &= (f - P_n f) + (P_m f)
	\end{align*}
	Therefore, using the Triangle Inequality, for $m,n \to \infty$,
	\begin{align*}
		\lim\limits_{m, n \to \infty} \left\| f - \sum\limits_{i = m}^{n - 1} f_i \right\| &= 0
	\end{align*}
	~\\
	If possible, let
	\begin{align*}
		f &= \sum\limits_{i = -\infty}^{\infty} f_i\\
		&= \sum\limits_{i = -\infty}^{\infty} g_i
	\end{align*}
	Then,
	\begin{align*}
		g_k &= (P_{k + 1} - P_k) \sum\limits_{i = -\infty}^{\infty} g_i\\
		&= (P_{k + 1} - P_k) f\\
		&= f_k
	\end{align*}
	Hence, the set $\{f_i\}$ is unique.
\end{proof}

\begin{theorem}
	Let $\psi \in W_0$ and let $\left\{ \psi(t - k) \right\}$ be an orthonormal basis for $W_0$.
	Then, $\left\{ 2^{\frac{i}{2}} \psi\left( 2^i t - k \right) \right\}$ for $k \in \mathbb{Z}$ is an orthonormal basis for $W_i$.
	\index{wavelets}
\end{theorem}

\begin{theorem}
	$\left\{ 2^{\frac{i}{2}} \psi\left( 2^i t - k \right) \right\}$ is an orthonormal basis of $L_2$.
\end{theorem}

\begin{proof}
	The set of all $\left\{ \psi_{i,k}(t) \right\}$ spans $W_i$.
	Also,
	\begin{align*}
		L_2 &= \bigoplus_{i \in \mathbb{Z}} W_i
	\end{align*}
	Therefore, $\left\{ \psi_{i,k}(t) \right\}$ spans $L_2$.
	Also, as the set is orthonormal, it is a orthonormal basis of $L_2$.
	\index{wavelets}
\end{proof}

\section{Characteristics of Wavelets}

\begin{theorem}
	$g \in V_0$ if and only if
	\begin{align*}
		g(t) &= \sum\limits_{n = -\infty}^{\infty} \alpha_n \varphi(t - n)
	\end{align*}
	if and only if
	\begin{align*}
		\hat{g}(f) &= \gamma(f) \hat{\varphi}(f)
	\end{align*}
	where
	\begin{align*}
		\gamma(f) &= \sum\limits_{n = -\infty}^{\infty} \alpha_n e^{-j 2 \pi f n}
	\end{align*}
	is the DTFT of the sequence $\alpha_n$, and hence is $L_2[0,1]$, i.e.
	\begin{align*}
		\left\| g(t) \right\| &= \sqrt{\int\limits_{0}^{1} \left| \gamma(f) \right|^2 \dif f}
	\end{align*}
	and is periodic with a period of $1$.
	\index{wavelets}
\end{theorem}

\begin{theorem}
	$z \in V_1$ iff
	\begin{align*}
		\hat{z}(f) &= m_z\left( \frac{f}{2} \right) \hat{\varphi}\left( \frac{f}{2} \right)
	\end{align*}
	where
	\begin{align*}
		m_z\left( \frac{f}{2} \right) &= \frac{1}{\sqrt{2}} \gamma(f)
	\end{align*}
	Also,
	\begin{align*}
		\|z\|_{L_2} &= \sqrt{2} \sqrt{\int\limits_{0}^{1} \left| m_z(f) \right|^2 \dif f}
	\end{align*}
	\index{wavelets}
\end{theorem}

\begin{proof}
	$z \in V_1$ if and only if $\exists g \in V_0$ such that
	\begin{align*}
		z(t) &= \sqrt{2} g(2 t)
	\end{align*}
	Also,
	\begin{align*}
		\|z\|_{L_2} &= \|g\|_{L_2}
	\end{align*}
	Equivalently,
	\begin{align*}
		\hat{z}(f) &= \frac{1}{\sqrt{2}} \hat{g}\left( \frac{f}{2} \right)\\
		&= m_z\left( \frac{f}{2} \right) \hat{\varphi}\left( \frac{f}{2} \right)
	\end{align*}
	where
	\begin{align*}
		m_z(f) &= \frac{1}{\sqrt{2}} \gamma(f)
	\end{align*}
	is in $L_2[0,1]$ and is periodic with a period of $1$.\\
	Also,
	\begin{align*}
		\|z\|_{L_2} &= \sqrt{2} \sqrt{\int\limits_{0}^{1} \left| m_z(f) \right|^2 \dif f}
	\end{align*}
\end{proof}

\begin{theorem}
	Let $z \in L_2$.
	Then, $z \in W_0$ if and only if
	\begin{align*}
		\hat{z}(f) &= e^{j \pi f} v(f) m_{\varphi}\left( \frac{f}{2} + \frac{1}{2} \right) \hat{\varphi}\left( \frac{f}{2} \right)
	\end{align*}
	where $v$ is periodic with period $1$,
	\begin{align*}
		\|z\|_{L_2} &= \sqrt{\int\limits_{0}^{1} \left| v(f) \right|^2 \dif f}
	\end{align*}
	and
	\begin{align*}
		m_z\left( \frac{f}{2} \right) &= e^{j \pi f} \overline{m_{\varphi}\left( \frac{f}{2} + \frac{1}{2} \right)}
	\end{align*}
	where $m_{\varphi}(f)$ is as in \cref{thm:scaling_equations},
	\index{wavelets}
	\index{scaling equations}
\end{theorem}

\begin{proof}
	Let
	\begin{align*}
		\hat{z}(f) &= e^{j \pi f} v(f) m_{\varphi}\left( \frac{f}{2} + \frac{1}{2} \right) \hat{\varphi}\left( \frac{f}{2} \right)
	\end{align*}
	Then,
	\begin{align*}
		m_z\left( \frac{f}{2} \right) &= e^{j \pi f} v(f) \overline{m_{\varphi}\left( \frac{f}{2} + \frac{1}{2} \right)}\\
		\therefore m_z(f) &= e^{j 2 \pi f} v(2 f) \overline{m_{\varphi}\left( f + \frac{1}{2} \right)}
	\end{align*}
	Hence, as all of $e^{j 2 \pi f}$, $v(2 f)$, and $m_{\varphi}\left( f + \frac{1}{2} \right)$ are periodic with period $1$, $m_z$ is also periodic with period $1$.
	Hence, $z \in V_1$.\\
	Also, $\forall k$,
	\begin{align*}
		\left\langle \varphi(t - k) , z \right\rangle &= \int\limits_{-\infty}^{\infty} z(t) \overline{\varphi(t - k)} \dif t\\
		&= \int\limits_{-\infty}^{\infty} \hat{z}(f) e^{j 2 \pi f k} \overline{\hat{\varphi}(f)} \dif f\\
		&= \sum\limits_{i = -\infty}^{\infty} \int\limits_{0}^{1} \hat{z}(f + i) e^{j 2 \pi f k} \overline{\hat{\varphi}(f + i)} \dif f\\
		&= \sum\limits_{i = -\infty}^{\infty} \int\limits_{i}^{i + 1} \hat{z}(f) e^{j 2 \pi f k} \overline{\hat{\varphi}(f)} \dif f
	\end{align*}
	Let
	\begin{align*}
		Z(f) &= \sum\limits_{i = -\infty}^{\infty} \hat{z}(f + i) \overline{\hat{\varphi}(f + i)}
	\end{align*}
	Hence, $z \in {V_0}^{\perp}$ iff
	\begin{align*}
		\left\langle \varphi(t - k) , z \right\rangle &= 0\\
		\iff \sum\limits_{i = -\infty}^{\infty} \int\limits_{i}^{i + 1} \hat{z}(f) e^{j 2 \pi f k} \overline{\hat{\varphi}(f)} \dif f &= 0\\
		\iff \int\limits_{0}^{1} Z(f) e^{j 2 \pi k f} &= 0
	\end{align*}
	Also, $\int\limits_{0}^{1} Z(f) e^{j 2 \pi k f}$ are the DTFT coefficients of $Z$.
	Hence, all DTFT coefficients can be zero iff
	\begin{align*}
		Z(f) &\equiv 0
	\end{align*}
	Therefore, $z \in {V_0}^{\perp}$ iff
	\begin{align*}
		\sum\limits_{i = -\infty}^{\infty} \hat{z}(f + i) \overline{\hat{\varphi}(f + 1)} &= 0
	\end{align*}
	As $z \in V_1$,
	\begin{align*}
		\hat{z}(f) &= m_z\left( \frac{f}{2} \right) \hat{\varphi}\left( \frac{f}{2} \right)\\
		\hat{\varphi}(f) &= m_{\varphi}\left( \frac{f}{2} \right) \hat{\varphi}\left( \frac{f}{2} \right)
	\end{align*}
	Therefore,
	\begin{align*}
		Z(f) &= \sum\limits_{i = -\infty}^{\infty} m_z\left( \frac{f}{2} + \frac{i}{2} \right) \overline{m_{\varphi}\left( \frac{f}{2} + \frac{i}{2} \right)} \left| \hat{\varphi}\left( \frac{f}{2} + \frac{i}{2} \right) \right|^2\\
		&= \quad \sum\limits_{i = -\infty}^{\infty} m_z\left( \frac{f}{2} + i \right) \overline{m_{\varphi}\left( \frac{f}{2} + i \right)} \left| \hat{\varphi}\left( \frac{f}{2} + i \right) \right|^2\\
		&\quad + \sum\limits_{i = -\infty}^{\infty} m_z\left( \frac{f}{2} + i + \frac{1}{2} \right) \overline{m_{\varphi}\left( \frac{f}{2} + i + \frac{1}{2} \right)} \left| \hat{\varphi}\left( \frac{f}{2} + i + \frac{1}{2} \right) \right|^2\\
		&= \quad m_z\left( \frac{f}{2} \right) \overline{m_{\varphi}\left( \frac{f}{2} \right)} \sum\limits_{i = -\infty}^{\infty} \left| \hat{\varphi}\left( \frac{f}{2} + i \right) \right|^2\\
		&\quad + m_z\left( \frac{f}{2} + \frac{1}{2} \right) \overline{m_{\varphi}\left( \frac{f}{2} + \frac{1}{2} \right)} \sum\limits_{i = -\infty}^{\infty} \left| \hat{\varphi}\left( \frac{f}{2} + i + \frac{1}{2} \right) \right|^2
	\end{align*}
	Hence, as the set $\left\{ \hat{\varphi}\left( \frac{f}{2} + i \right) \right\}$ is orthogonal,
	\begin{align*}
		Z(f) &= m_z\left( \frac{f}{2} \right) \overline{m_{\varphi}\left( \frac{f}{2} \right)} + m_z\left( \frac{f}{2} + \frac{1}{2} \right) \overline{m_{\varphi}\left( \frac{f}{2} + \frac{1}{2} \right)}
	\end{align*}
	Hence, $z \in {V_0}^{\perp}$ iff
	\begin{align*}
		0 &= Z(f)\\
		\iff 0 &= \left\langle \left( m_{\varphi}(f) , m_{\varphi}\left( f + \frac{1}{2} \right) \right) , \left( m_z(f) , m_z\left( f + \frac{1}{2} \right) \right) \right\rangle\\
		\iff 0 &= \left\langle \left( m_{\varphi}(f) , m_{\varphi}\left( f + \frac{1}{2} \right) \right) , \left( -\overline{m_{\varphi}\left( f + \frac{1}{2} \right)} , \overline{m_{\varphi}(f)} \right) \right\rangle
	\end{align*}
	Let
	\begin{align*}
		M &= \left( m_{\varphi}(f) , m_{\varphi}\left( f + \frac{1}{2} \right) \right)
	\end{align*}
	Therefore, as $\left( m_z(f) , m_z\left( f + \frac{1}{2} \right) \right)$ and $\left( -\overline{m_{\varphi}\left( f + \frac{1}{2} \right)} , \overline{m_{\varphi}(f)} \right)$ are orthogonal to $M$, they are collinear, as long as $M \neq 0$.
	Also,
	\begin{align*}
		\|M\| &= \left| m_{\varphi}(f) \right|^2 + \left| m_{\varphi}\left( f + \frac{1}{2} \right) \right|^2\\
		&= 1
	\end{align*}
	Therefore, $M \neq 0$.
	Hence, $\exists \alpha(f)$ such that
	\begin{align*}
		m_z(f) &= \alpha(f) \overline{m_{\varphi}\left( f + \frac{1}{2} \right)}\\
		m_z\left( f + \frac{1}{2} \right) &= -\alpha(f) \overline{m_{\varphi}(f)}
	\end{align*}
	iff $z \in {V_0}^{\perp}$.
	Therefore,
	\begin{align*}
		m_z(f) &= \alpha(f) \overline{m_{\varphi}\left( f + \frac{1}{2} \right)}\\
		\therefore m_z\left( f + \frac{1}{2} \right) &= \alpha\left( f + \frac{1}{2} \right) \overline{m_{\varphi}(f)}\\
		m_z\left( f + \frac{1}{2} \right) &= -\alpha(f) \overline{m_{\varphi}(f)}
	\end{align*}
	Therefore,
	\begin{align*}
		\alpha\left( f + \frac{1}{2} \right) &= -\alpha(f)
	\end{align*}
	Let
	\begin{align*}
		h(f) &= e^{-j 2 \pi f} \alpha(f)
	\end{align*}
	Therefore,
	\begin{align*}
		v(f) &= h\left( \frac{f}{2} \right)
	\end{align*}
	Therefore,
	\begin{align*}
		v(f + 1) &= h\left( \frac{f + 1}{2} \right)\\
		&= e^{-j 2 \pi \frac{f + 1}{2}} \alpha\left( \frac{f}{2} + \frac{1}{2} \right)\\
		&= e^{-j \pi f} e^{-j \pi} \left( -\alpha\left( \frac{f}{2} \right) \right)\\
		&= e^{j 2 \pi \frac{f}{2}} \alpha\left( \frac{f}{2} \right)\\
		&= h\left( \frac{f}{2} \right)\\
		&= v(f)
	\end{align*}
	Therefore, $z \in {V_0}^{\perp}$ iff
	\begin{align*}
		m_z(f) &= e^{j 2 \pi f} h(f) \overline{m_{\varphi}\left( f + \frac{1}{2} \right)}\\
		m_z\left( \frac{f}{2} \right) &= e^{j \pi f} v(f) \overline{m_{\varphi}\left( \frac{f}{2} + \frac{1}{2} \right)}
	\end{align*}
\end{proof}

\begin{theorem}
	For a $z \in W_0$, such that
	\begin{align*}
		\hat{z}(f) &= e^{j \pi f} v(f) m_{\varphi}\left( \frac{f}{2} + \frac{1}{2} \right) \hat{\varphi}\left( \frac{f}{2} \right)
	\end{align*}
	the set $\left\{ z(t - k) \right\}$ is orthogonal if
	\begin{align*}
		\left| v(f) \right| &= 1
	\end{align*}
	where
\end{theorem}

\begin{theorem}
	If
	\begin{align*}
		\hat{\psi}(f) &= e^{j \pi f} v(f) \overline{m_{\varphi}\left( \frac{f}{2} + \frac{1}{2} \right)} \hat{\varphi}\left( \frac{f}{2} \right)
	\end{align*}
	and $v(f)$ is periodic with period $1$ and satisfies,
	\begin{align*}
		\left| v(f) \right| &= 1
	\end{align*}
	then $\psi(t)$ is the mother wavelet for a set of orthogonal wavelets if $v(f)$ is periodic with period $1$ and
\end{theorem}

\begin{theorem}
	Assuming $m_{\varphi}(f)$ is bounded around $\frac{1}{2}$,
	\begin{align*}
		\hat{\psi}(2 k) &= 0
	\end{align*}
	for all $k \in \mathbb{Z}$.
\end{theorem}

\begin{proof}
	\begin{align*}
		\left| \hat{\psi}(2 k) \right| &= \left| e^{j 2 \pi k} v(2 k) \overline{m_{\varphi}\left( k + \frac{1}{2} \right)} \hat{\varphi}(k) \right|
	\end{align*}
	Also,
	\begin{align*}
		m_{\varphi}\left( \frac{1}{2} \right) &= 0
	\end{align*}
	for $k = 0$, and
	\begin{align*}
		\hat{\varphi}(k) &= 0
	\end{align*}
	for $k \neq 0$.\\
	Hence,
	\begin{align*}
		\hat{\psi}(2 k) &= 0
	\end{align*}
\end{proof}

\section{Wavelet Transform}

\begin{theorem}
	Assuming $\varphi(t)$ has a bounded support,
	\begin{align*}
		\varphi(t) &= \sum\limits_{n = 0}^{2 T - 1} a_n \varphi(2 t - n)\\
		\psi(t) &= \sum\limits_{n = 0}^{2 T - 1} b_n \varphi(2 t - n)
	\end{align*}
	where
	\begin{align*}
		a_n &= \left\langle \varphi\left( \frac{t}{2} \right) , \varphi(t - n) \right\rangle\\
		b_n &= \left\langle \psi\left( \frac{t}{2} \right) , \varphi(t - n) \right\rangle
	\end{align*}
\end{theorem}

\begin{proof}
	Let
	\begin{align*}
		v(f) &= e^{-j 2 \pi f T}
	\end{align*}
	where $T \in \mathbb{Z}$.
	Hence,
	\begin{align*}
		\left| v(f) \right| &= 1
	\end{align*}
	and is periodic with period $1$.
	Therefore,
	\begin{align*}
		\hat{\psi(f)} &= e^{j \pi f} v(f) \overline{m_{\varphi}\left( \frac{f}{2} + \frac{1}{2} \right)} \hat{\varphi}\left( \frac{f}{2} \right)\\
		&= e^{j \pi f} e^{-j 2 \pi f T} \frac{1}{2} \sum\limits_{n = -\infty}^{\infty} \overline{a_n} e^{j 2 \pi n \left( \frac{f}{2} + \frac{1}{2} \right)} \hat{\varphi}\left( \frac{f}{2} \right)\\
		&= \frac{1}{2} \sum\limits_{n = -\infty}^{\infty} e^{j \pi n f + j \pi n + j \pi f + j 2 \pi f T} \hat{\varphi}\left( \frac{f}{2} \right)\\
		&= \frac{1}{2} \sum\limits_{n = -\infty}^{\infty} (-1)^n \overline{a_n} e^{j 2 \pi \frac{f}{2} (n + 1 + 2 T)} \hat{\varphi}\left( \frac{f}{2} \right)
	\end{align*}
	Therefore,
	\begin{align*}
		\psi(t) &= \sum\limits_{n = -\infty}^{\infty} (-1)^n \overline{a_n} \varphi(2 t + n + 1 + 2 T)
	\end{align*}
	Hence, replacing $T$ by $-T$,
	\begin{align*}
		\psi(t) &= \sum\limits_{n = -\infty}^{\infty} (-1)^n \overline{a_n} \varphi(2 t + n + 1 - 2 T)
	\end{align*}
	Hence, let
	\begin{align*}
		-n' &= n + 1 - 2 T\\
		\therefore n &= -n' + 2 T - 1
	\end{align*}
	Therefore,
	\begin{align*}
		\psi(t) &= \sum\limits_{n' = -\infty}^{\infty} (-1)^{-n' + 2 T - 1} \overline{a_{2 T - 1 - n}} \varphi(2 t - n)
	\end{align*}
	Therefore,
	\begin{align*}
		\psi(t) &= -\sum\limits_{n = -\infty}^{\infty} (-1)^n \overline{a_{2 T - 1 - n}} \varphi(2 t - n)
	\end{align*}
	Hence, as $\varphi(t)$ has a bounded support, let $a_n = 0$ for all $n \ge 2 T$ and $n < 0$.\\
	Let
	\begin{align*}
		b_n &= (-1)^{n + 1} \overline{a_{2 T - 1 - n}}
	\end{align*}
	Therefore,
	\begin{align*}
		\varphi(t) &= \sum\limits_{n = 0}^{2 T - 1} a_n \varphi(2 t - n)\\
		\psi(t) &= -\sum\limits_{n = 0}^{2 T - 1} (-1)^n \overline{a_{2 T - 1 - n}} \varphi(2 t - n)\\
		&= \sum\limits_{n = 0}^{2 T - 1} b_n \varphi(2 t - n)
	\end{align*}
	Hence, equivalently,
	\begin{align*}
		a_n &= \left\langle \varphi\left( \frac{t}{2} \right) , \varphi(t - n) \right\rangle\\
		b_n &= \left\langle \psi\left( \frac{t}{2} \right) , \varphi(t - n) \right\rangle
	\end{align*}
\end{proof}

\section{Wavelets for Digital Systems}

\begin{theorem}
	Let $f \in V_i$, such that
	\begin{align*}
		f(t) &= \sum\limits_{n = -\infty}^{\infty} \alpha_n 2^{\frac{i}{2}} \varphi\left( 2^i t - n \right)
	\end{align*}
	Let
	\begin{align*}
		a_n &= \left\langle \varphi\left( \frac{t}{2} \right) , \varphi(t - n) \right\rangle\\
		b_n &= \left\langle \psi\left( \frac{t}{2} \right) , \varphi(t - n) \right\rangle
	\end{align*}
	Then,
	\begin{align*}
		f(t) &= \sum\limits_{n = -\infty}^{\infty} \gamma_n 2^{\frac{i - 1}{2}} \varphi\left( 2^{i - 1} t - n \right) + \sum\limits_{n = -\infty}^{\infty} \beta_n 2^{\frac{i - 1}{2}} \psi\left( 2^{i - 1} t - n \right)
	\end{align*}
	where
	\begin{align*}
		\gamma_n &= \sum\limits_{k = -\infty}^{\infty} \overline{\alpha_k} a_{k - 2 n}\\
		\beta_n &= \sum\limits_{k = -\infty}^{\infty} \overline{\alpha_k} b_{k - 2 n}
	\end{align*}
\end{theorem}

\begin{proof}
	Let
	\begin{align*}
		f(t) &= \sum\limits_{n = -\infty}^{\infty} \alpha_n 2^{\frac{i}{2}} \varphi\left( 2^i t - n \right)
	\end{align*}
	Also,
	\begin{align*}
		V_i &= V_{i - 1} \oplus W_{i - 1}
	\end{align*}
	Therefore,
	\begin{align*}
		f(t) &= \sum\limits_{n = -\infty}^{\infty} \gamma_n 2^{\frac{i - 1}{2}} \varphi\left( 2^{i - 1} t - n \right) + \sum\limits_{n = -\infty}^{\infty} \beta_n 2^{\frac{i - 1}{2}} \psi\left( 2^{i - 1} t - n \right)
	\end{align*}
	where
	\begin{align*}
		\gamma_n &= \left\langle f , 2^{\frac{i - 1}{2}} \varphi\left( 2^{i - 1} t - n \right) \right\rangle\\
		\beta_n &= \left\langle f , 2^{\frac{i - 1}{2}} \psi\left( 2^{i - 1} t - n \right) \right\rangle
	\end{align*}
	Hence, substituting
	\begin{align*}
		f(t) &= \sum\limits_{n = -\infty}^{\infty} \alpha_n 2^{\frac{i}{2}} \varphi\left( 2^i t - n \right)\\
		\varphi(t) &= \sum\limits_{n = -\infty}^{\infty} a_n \varphi(2 t - n)
	\end{align*}
	in $\gamma_n$,
	\begin{align*}
		\gamma_n &= \left\langle \sum\limits_{k = -\infty}^{\infty} \alpha_k 2^{\frac{i}{2}} \varphi\left( 2^i t - k \right) , \sum\limits_{k = -\infty}^{\infty} a_k 2^{\frac{i}{2}} \varphi\left( 2^i t - k - 2 n \right) \right\rangle
	\end{align*}
	Also,
	\begin{align*}
		\varphi\left( 2^{i - 1} t - n \right) &= \varphi\left( 2^{i - 1} \left( t - \frac{n}{2^{i - 1}} \right) \right)\\
		&= \sum\limits_{k = -\infty}^{\infty} a_k \sqrt{2} \varphi\left( 2^i \left( t - \frac{n}{2^{i - 1}} \right) - k \right)\\
		&= \sum\limits_{k = -\infty}^{\infty} a_k \sqrt{2} \varphi\left( 2^i t - 2 n - k \right)
	\end{align*}
	Let
	\begin{align*}
		-k' &= -2 n - k\\
		\therefore k &= k' - 2 n
	\end{align*}
	Therefore,
	\begin{align*}
		\varphi\left( 2^{i - 1} t - n \right) &= \sum\limits_{k = -\infty}^{\infty} a_k \sqrt{2} \varphi\left( 2^i t - 2 n - k \right)\\
		&= \sum\limits_{k' = -\infty}^{\infty} a_{k' - 2 n} \sqrt{2} \varphi\left( 2^i t - k' \right)
	\end{align*}
	Therefore,
	\begin{align*}
		\gamma_n &= \left\langle \sum\limits_{k = -\infty}^{\infty} \alpha_k 2^{\frac{i}{2}} \varphi\left( 2^i t - k \right) , \sum\limits_{k = -\infty}^{\infty} a_{k - 2 n} 2^{\frac{i}{2}} \varphi\left( 2^i t - k \right) \right\rangle\\
		&= \sum\limits_{k = -\infty}^{\infty} \overline{\alpha_k} a_{k - 2 n}
	\end{align*}
	Similarly,
	\begin{align*}
		\beta_n &= \sum\limits_{k = -\infty}^{\infty} \overline{\alpha_k} b_{k - 2 n}
	\end{align*}
\end{proof}

\begin{theorem}
	Let ${\gamma_n}^i$ be the coefficients of the scaling function $\varphi(t)$ at the $i$th level, and let and ${\beta_n}^i$ be the coefficients of the mother wavelet $\psi(t)$ at the $i$th level, i.e. coefficients corresponding to $\varphi\left( 2^i t - n \right) \in V_i$ and $\psi\left( 2^i t - n \right) \in W_{i}$.
	Then,
	\begin{align*}
		{\gamma_n}^{i - 1} &= \overline{{\gamma_k}^i} a_{k - 2 n}\\
		{\beta}^{i - 1} &= \overline{{\gamma_k}^i} b_{k - 2 n}
	\end{align*}
\end{theorem}

\section{Applications of Wavelets}

Let $W$ denote the wavelet operation.
Hence, as $W$ is orthogonal, let $W^*$ be the inverse of $W$.

Hence, for orthogonal Haar wavelets, the wavelet operation can be performed as in \cref{fig:filter_bank_implementation_of_1}.
\begin{figure}[H]
	\centering
	\begin{tikzpicture}
		\node [block] (HP1) {
			$
			b_{-n} =
			\begin{pmatrix}
				-\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
			\end{pmatrix}
			$
		};

		\node [block, right = of HP1] (downsampler_HP1) {$\downsample{2}$};

		\node [block, below = of HP1] (LP1) {
			$
			a_{-n} =
			\begin{pmatrix}
				\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
			\end{pmatrix}
			$
		};

		\node [block, right = of LP1] (downsampler_LP1) {$\downsample{2}$};

		\node [block, right = of downsampler_LP1] (HP2) {
			$
			b_{-n} =
			\begin{pmatrix}
				-\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
			\end{pmatrix}
			$
		};

		\node [block, right = of HP2] (downsampler_HP2) {$\downsample{2}$};

		\node [block, below right = of downsampler_LP1] (LP2) {
			$
			a_{-n} =
			\begin{pmatrix}
				\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
			\end{pmatrix}
			$
		};

		\node [block, right = of LP2] (downsampler_LP2) {$\downsample{2}$};

		\begin{scope}[-stealth]
			\draw ($ (HP1.west) + (-1,0) $) node [left] {$x$} -- ++(0.5,0) |- (HP1.west);
			\draw ($ (HP1.west) + (-1,0) $) node [left] {$x$} -- ++(0.5,0) |- (LP1.west);
			\draw (HP1.east) -- (downsampler_HP1.west);
			\draw (downsampler_HP1.east) -- ++(1,0) node [right] {$\beta^{i - 1}$};
			\draw (LP1.east) -- (downsampler_LP1.west);
			\draw (downsampler_LP1.east) -- ++(0.5,0) -- (HP2.west);
			\draw (downsampler_LP1.east) -- ++(0.5,0) |- (LP2.west);
			\draw (HP2.east) -- (downsampler_HP2.west);
			\draw (downsampler_HP2.east) -- ++(1,0) node [right] {$\beta^{i - 2}$};
			\draw (LP2.east) -- (downsampler_LP2.west);
			\draw (downsampler_LP2.east) -- ++(1,0) node [right] {$\gamma^{i - 2}$};
		\end{scope}
	\end{tikzpicture}
	\caption{Filter Bank Implementation of 1-D Wavelet Transform}
	\label{fig:filter_bank_implementation_of_1-D_wavelet_transform}
\end{figure}
Hence, for a $w \in \mathbb{R}^{4 \times 4}$, this is equivalent to multiplying $x$ by
\begin{align*}
	W &=
		\begin{pmatrix}
			\frac{1}{2} & \frac{1}{2} & \frac{1}{2} & \frac{1}{2}\\
			\frac{1}{2} & \frac{1}{2} & -\frac{1}{2} & -\frac{1}{2}\\
			\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} & 0 & 0\\
			0 & 0 & \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}\\
		\end{pmatrix}
\end{align*}

$W$ and $W^*$ are also called the analysis wavelet operation and synthesis wavelet operation, respectively.

\begin{definition}[Wavelet frame]
	Let $W \in \mathbb{R}^{p \times n}$ be a wavelet operation, where $p > n$.
	Let $x \in \mathbb{R}^n$.
	Hence, $W x \in \mathbb{R}^p$.
	$W$ is said to be a wavelet frame if for all $x \in \mathbb{R}^n$,
	\begin{equation*}
		A {\|x\|_2}^2 \le {\|W x\|_2}^2 \le B {\|x\|_2}^2
	\end{equation*}
	\index{frame!wavelet}
\end{definition}

\begin{definition}[Tight wavelet frame]
	A wavelet frame for which $A = B$ is called a tight wavelet frame.
	\index{frame!wavelet!tight}
\end{definition}

\begin{definition}[Vanishing moments]
	A function $g(t)$ is said to have $n$ vanishing moments if
	\begin{align*}
		\int\limits_{-\infty}^{\infty} t^k g(t) \dif t &= 0
	\end{align*}
	for $0 \le k < n$.
	\index{vanishing moment}
\end{definition}

If the mother wavelet $\psi$ has $n$ vanishing moments, then it is orthogonal to all polynomials upto order $n$.
Hence, $n - 1$th order polynomial functions are spanned by the scaling function only.
Hence, the wavelet coefficients of smooth functions will be mostly zero in high frequencies.

\subsection{Compression}

If $W x$ has many zero coefficients in positions which can be predicted, this can be used for compression.

\subsection{Denoising}

Consider a noisy signal
\begin{align*}
	y &= x + n
\end{align*}
where
\begin{align*}
	n &\sim \normal\left( 0,\sigma^2 I \right)
\end{align*}
Assume $W x$ is sparse, i.e. $W x$ has at most $k$ non-zero coefficients.
Then,
\begin{align*}
	W y &= W x + W n
\end{align*}
Let
\begin{align*}
	n' &= W n
\end{align*}
Therefore, as $n$ is Gaussian with zero mean and variance $\sigma$,
\begin{align*}
	\expct[n'] &= \expct[W n]\\
	&= 0\\
	\expct\left[ n' {n'}^* \right] &= \expct\left[ W n n^* W^* \right]\\
	&= W \expct\left[ n n^* \right] W^*\\
	&= W \sigma^2 I W^*\\
	&= \sigma^2 I
\end{align*}
Hence,
\begin{align*}
	n' &\sim \normal\left( 0,\sigma^2 I \right)
\end{align*}
% Hence, a threshold can be applied to $W y$ to keep only the largest $k$ elements.
If the locations $T_x$ of the $k$ non-zero elements in $W x$ are given, the reconstructed signal is
\begin{align*}
	\hat{x}_{\text{oracle}} &= W^* [W y]_{T_x}
\end{align*}
where
\begin{align*}
	[W y]_{T_x}[i] &=
		\begin{cases}
			x[i] &;\quad i \in T_x\\
			0 &;\quad \text{otherwise}\\
		\end{cases}
\end{align*}
Hence, in the oracle case, the error is
\begin{align*}
	\text{MSE} &= \expct\left[ {\left\| x - \hat{x}_{\text{oracle}} \right\|_2}^2 \right]\\
	&= \expct\left[ {\left\| W x - W \hat{x}_{\text{oracle}} \right\|_2}^2 \right]\\
	&= \expct\left[ {\left\| W x - [W y]_{T_x} \right\|_2}^2 \right]\\
	&= \expct\left[ {\left\| W x - [W x]_{T_x} - [n']_{T_x} \right\|_2}^2 \right]\\
	&= \expct\left[ {\left\| [n']_{T_x} \right\|_2}^2 \right]\\
	&= k \sigma^2
\end{align*}
Hence, as the noise power in $y$ is $n \sigma^2$, and that in $\hat{x}_{\text{oracle}}$ is $k \sigma^2$, the oracle provides an improvement of $\frac{n}{k}$ in the SNR.\\
However, in reality, as $T_x$ are not known, let $T_y$ be the positions of the $k$ largest elements in $W y$, and let
\begin{align*}
	\hat{x} &= W^* [W y]_{T_y}
\end{align*}
Therefore,
\begin{align*}
	\expct\left[ {\left\| \hat{x} - x \right\|_2}^2 \right] &= \expct\left[ {\left\| W \hat{x} - W x \right\|_2}^2 \right]\\
	&= \expct\left[ {\left\| [W y]_{T_y} - W x \right\|_2}^2 \right]\\
	&= \expct\left[ {\left\| [W x + n']_{T_y} - W x \right\|_2}^2 \right]\\
\end{align*}
If an element is in $T_x \cap T_y$,
\begin{align*}
	W y - W x &= W x + n' - W x\\
	&= n'
\end{align*}
If an element is in $T_y \setminus T_x$, as $W_x = 0$ for all elements not in $T_x$,
\begin{align*}
	W y - W x &= W x + n' - W x\\
	&= n'
\end{align*}
If an element is in $T_x \setminus T_y$, as $W_y = 0$ for all elements not in $T_y$,
\begin{align*}
	W y - W x &= -W x
\end{align*}
Therefore,
\begin{align*}
	\expct\left[ {\left\| \hat{x} - x \right\|_2}^2 \right] &= \expct\left[ {\left\| [W x + n' - W x]_{T_x \cap T_y} + [W x + n']_{T_y \setminus T_x} - [W x]_{T_x \setminus T_y} \right\|_2}^2 \right]\\
	&= \expct\left[ {\left\| [W x + n' - W x]_{T_x \cap T_y} + [n']_{T_y \setminus T_x} - [W x]_{T_x \setminus T_y} \right\|_2}^2 \right]\\
	&= \expct\left[ {\left\| [n']_{T_x \cap T_y} \right\|_2}^2 \right] + \expct\left[ {\left\| [n']_{T_y \setminus T_x} \right\|_2}^2 \right] + \expct\left[ {\left\| [W x]_{T_x \setminus T_y} \right\|_2}^2 \right]
\end{align*}
Elements in $T_y \setminus T_x$ are selected only if the noise at those positions dominates the signal which ideally should have been selected, i.e. the signal corresponding to $T_x \setminus T_y$.
Hence,
\begin{align*}
	{\left\| [n']_{T_y \setminus T_x} \right\|_2}^2 &\ge {\left\| [W y]_{T_x \setminus T_y} \right\|_2}^2
\end{align*}
Also,
\begin{align*}
	\expct\left[ {\left\| [W y]_{T_x \setminus T_y} \right\|_2}^2 \right] &= \expct\left[ {\left\| [W x]_{T_x \setminus T_y} + [n']_{T_x \setminus T_y} \right\|_2}^2 \right]\\
	&=\quad \expct\left[ {\left\| [W x]_{T_x \setminus T_y} \right\|_2}^2 \right]\\
	&\quad + 2 \expct\left[ \transpose{[W x]_{T_x \setminus T_y}} [n]_{T_x \setminus T_y} \right]\\
	&\quad + \expct\left[ {\left\| [n]_{T_x \setminus T_y} \right\|_2}^2 \right]
\end{align*}
Also, for any $p$ and $q$,
\begin{align*}
	p q &\le \frac{p^2}{2} + \frac{q^2}{2}
\end{align*}
Therefore,
\begin{align*}
	\expct\left[ {\left| [W x]_{T_x \setminus T_y} \right|_2}^2 \right] &\le 2 \expct\left[ {\left| [n]_{T_y \setminus T_x} \right|_2}^2 \right]
\end{align*}
Therefore,
\begin{align*}
	\text{MSE} &= \expct\left[ {\left\| x - \hat{x} \right\|_2}^2 \right]\\
	&\le 2 k \max\limits_{i} {{n'}_i}^2
\end{align*}
As the noise is real and Gaussian noise of dimension $d$,
\begin{align*}
	\expct\left[ \max\limits_{i} {{n'}_i}^2 \right] &\propto \log d
\end{align*}
Hence, solving,
\begin{align*}
	\text{MSE} &= k \sigma^2 \log d
\end{align*}

\clearpage
\part{Sparsity}

\section{General Linear Inverse Problem}

\begin{definition}[General linear inverse problem]
	The general linear inverse problem is defined as
	\begin{align*}
		y &= M(x) + e
	\end{align*}
	where $y$ is the measured data, $x$ is the original data, $M$ is the measurement operation, and $e$ is the noise.
\end{definition}

\section{Sparse Representations}

\begin{definition}[$l_0$ pseudonorm]
	The $l_0$ pseudonorm of a vector $\alpha$ is defined to be the number of non-zero elements of $\alpha$.
	It is denoted by $\|\alpha\|_0$.
	\index{l@$l_0$ pseudonorm}
\end{definition}

\begin{definition}[Sparse representation of vector]
	A vector $x \in \mathbb{R}^d$ can be represented as
	\begin{align*}
		x &= D \alpha
	\end{align*}
	where $D \in \mathbb{R}^{d \times n}$ is called the dictionary and $\alpha \in \mathbb{R}$ is a sparse representation of $x$.\\
	Hence, if $D$ is unitary, then
	\begin{align*}
		\alpha &= D^* x
	\end{align*}
	\index{sparse representation}
\end{definition}

\begin{theorem}
	$\alpha$, the sparse representation of a vector $x$ is unique if every $2 k$ columns of $D$ are linearly independent, where
	\begin{align*}
		k &= \|\alpha\|_0
	\end{align*}
	\index{sparse representation}
\end{theorem}

\begin{proof}
	Assume $\alpha_0$ is $k$-sparse and
	\begin{align*}
		\alpha_0 &\neq \alpha
	\end{align*}
	such that
	\begin{align*}
		D \alpha_0 &= D \alpha
	\end{align*}
	Therefore,
	\begin{align*}
		D(\alpha - \alpha_0) &= 0
	\end{align*}
	where $\alpha - \alpha_0$ is $2 k$-sparse.
	Hence, there exist $2 k$ columns in $D$ which are linearly independent.
	Hence, the sparse representation $\alpha$ is unique if every $2 k$ columns in $D$ are linearly independent.
\end{proof}

\begin{theorem}
	Let
	\begin{align*}
		y &= M x + e\\
		&= M D \alpha + e\\
		&= A \alpha + e
	\end{align*}
	Let $\hat{\alpha}$ be the solution to the $P_{0,\varepsilon}$ problem, i.e. let $\hat{\alpha}$ be the sparsest vector such that $M D \hat{\alpha}$ is at least $\varepsilon$ close to $y$, i.e.
	\begin{align*}
		\hat{\alpha} &= \min\limits_{\tilde{\alpha}} \left\| \tilde{\alpha} \right\|_0
	\end{align*}
	such that
	\begin{align*}
		\left| y - A \tilde{\alpha} \right|_2 &\le \varepsilon
	\end{align*}
	Then, then the optimal estimation of $x$ is
	\begin{align*}
		\hat{x} &= D \hat{\alpha}
	\end{align*}
	This problem is NP hard and hence must be solved using approximation algorithms as in \cref{sec:approximation_algorithms}.
	\index{P@$P_{0,\varepsilon}$}
\end{theorem}

\subsection{Restricted Isometry Property}

\begin{definition}[Restricted isometry property (RIP)]
	A matrix $A$ is said to satisfy the restricted isometry property with $\delta_{2 k}$ if for any $2 k$-sparse vector $v$,
	\begin{equation*}
		(1 - \delta_{2 k}) {\|v\|_2}^2 \le {\|A v\|_2}^2 \le (1 + \delta_{2 k}) {\|v\|_2}^2
	\end{equation*}
	Also, if
	\begin{align*}
		\delta_{2 k} &< 1
	\end{align*}
	then every $2 k$ columns in $A$ are linearly independent.
	Hence, equivalently, if $\delta_{2 k} < 1$, then $\hat{\alpha}$ is unique.
	\index{restricted isometry property}
	\index{RIP|see {restricted isometry property}}
\end{definition}

\begin{theorem}
	Let $\hat{\alpha}$ be the solution to the $P_{0,\varepsilon}$ problem, i.e. let
	\begin{align*}
		\hat{\alpha} &= \min\limits_{\tilde{\alpha}} \left\| \tilde{\alpha} \right\|_0
	\end{align*}
	such that
	\begin{align*}
		\left| y - A \tilde{\alpha} \right|_2 &\le \varepsilon
	\end{align*}
	Then,
	\begin{align*}
		\left\| \alpha - \hat{\alpha} \right\|_2 &\le \frac{2 \varepsilon}{\sqrt{1 - \delta_{2 k}}}
	\end{align*}
	\index{P@$P_{0,\varepsilon}$}
\end{theorem}

\section{Approximation Algorithms}
\label{sec:approximation_algorithms}

\subsection{Relaxation Based Approximation}

\begin{definition}[Relaxation of $P_{0,\varepsilon}$ to $P_{1,\varepsilon}$]
	The relaxation of the $P_{0,\varepsilon}$ problem to the $P_{1,\varepsilon}$ is obtained by relaxation of the $l_0$ norm to the $l_1$ norm, i.e.
	\begin{align*}
		\hat{\alpha} &= \min\limits_{\tilde{\alpha}} \left\| \tilde{\alpha} \right\|_1
	\end{align*}
	such that
	\begin{align*}
		\left| y - A \tilde{\alpha} \right|_2 &\le \varepsilon
	\end{align*}
	\index{P@$P_{1,\varepsilon}$}
\end{definition}

\begin{theorem}
	If
	\begin{align*}
		\delta_{2 k} &\le \frac{1}{\sqrt{2}}
	\end{align*}
	then
	\begin{align*}
		\left\| \hat{\alpha}_{l_1} - \alpha \right\|_2 &\le c \varepsilon
	\end{align*}
	where $c$ is a function of $\delta_{2 k}$.
\end{theorem}

\begin{theorem}
	If $A \in \mathbb{R}^{m \times n}$ is a matrix with independent identically distributed random Gaussian or sub-Gaussian entries, and
	\begin{align*}
		m &\ge \frac{c' k}{\xi^2} \log n
	\end{align*}
	where $c'$ is a constant that depends on the distribution, then with very high probability,
	\begin{align*}
		\delta_{2 k} &\le \xi
	\end{align*}
\end{theorem}

\subsection{Greedy Techniques}

\subsubsection{Thresholding}

Let
\begin{align*}
	\hat{T} &= \supp\left( A^* y , k \right)
\end{align*}
Then,
\begin{align*}
	\hat{\alpha}_{\hat{T}} &= {A_{\hat{T}}}^{\dagger}\\
	&= \argmin\limits_{\tilde{\alpha}} {\left\| y - A_{\hat{T}} \tilde{\alpha}_{\hat{T}} \right\|_2}^2
\end{align*}
For $k = 1$ and $e = 0$,
\begin{align*}
	y &= A \alpha\\
	&= a_i \alpha_i
\end{align*}
where $a_i$ is the $i$th column of $A$ and $\alpha_i$ is the $i$th element of $\alpha$.
Let
\begin{align*}
	z &= A^* y
\end{align*}
Therefore, the $j$th element of $z$ is
\begin{align*}
	z_j &= {a_j}^* y\\
	&= {a_j}^* a_i \alpha_i
\end{align*}
Hence, if
\begin{align*}
	\|a_j\|_2 &= 1
\end{align*}
then
\begin{align*}
	z_j &=
		\begin{cases}
			\alpha_i &;\quad j = i\\
			\alpha_i {a_j}^* a_i &;\quad j \neq 1\\
		\end{cases}
\end{align*}
If $k > 1$, then
\begin{align*}
	y &= \sum\limits_{i \in T} \alpha_i a_i
\end{align*}
where $T$ is the true support of $\alpha$.

\subsubsection{Orthogonal Matching Pursuit (OMP)}

\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\State{$r^1 \gets y$}
		\State{$t \gets 1$}
		\State{$T^0 \gets \emptyset$}
		\While{$\left| T^t \right| \le k \And \left\| r^t \right\|_2 \le \varepsilon$}
			\State{$i \gets \argmax\limits_{i} \left| {a_i}^* r^t \right|$}
			\State{$T^t \gets T^{t - 1} \cup \{i\}$}
			\State{$\alpha^t \gets \argmax\limits_{\tilde{\alpha}} {\left\| A_{T^t} \tilde{\alpha} - y \right\|_2}^2$}
			\State{$r^{t + 1} \gets r^t - A \alpha^t$}\\
			\State{$t \gets t + 1$}
		\EndWhile
	\end{algorithmic}
	\caption{Orthogonal Matching Pursuit Algorithm}
	\label{alg:OMP_algorithm}
\end{algorithm}

\begin{definition}[Coherence]
	The coherence of $A$ is defined to be
	\begin{align*}
		\mu(A) &= \max\left| {a_i}^* a_j \right|
	\end{align*}
\end{definition}

\begin{theorem}
	In the noiseless case,
	\begin{align*}
		y &= A x
	\end{align*}
	and if
	\begin{align*}
		k &\le \frac{1}{2} \left( 1 + \frac{1}{\mu(A)} \right)
	\end{align*}
	where $\mu(A)$ is the coherence of $A$, then assuming the columns of $A$ are normalized, the OMP and $P_{1,\varepsilon}$ approximations are guaranteed to find the sparsest solution.
	In the noisy case of
	\begin{align*}
		y &= A \alpha + e
	\end{align*}
	where $y \in \mathbb{R}^m$ and $\alpha \in \mathbb{R}^n$, these conditions can be satisfied only if
	\begin{align*}
		m &\ge c k^2
	\end{align*}
\end{theorem}

\subsection{Projected Gradient Descent}

\begin{align*}
	\hat{\alpha} &= \min\limits_{\tilde{\alpha}} {\left\| y - A \tilde{\alpha} \right\|_2}^2
\end{align*}
such that
\begin{align*}
	\left\| \tilde{\alpha} \right\|_0 &\le k
\end{align*}
where $\tilde{\alpha}$ can be obtained by iterative hard thresholding, i.e.
\begin{align*}
	{\tilde{\alpha}}^{t + 1} &= \alpha^t - \mu \transpose{A} \left( A \alpha^t - y \right)\\
	\alpha^{t + 1} &= [{\tilde{\alpha}}^{t + 1}]_k
\end{align*}
where $[ \cdot ]_k$ represents hard thresholding by $k$, i.e., $[x]_k$ consists of only the $k$ largest elements of $x$.

\section{Dictionary Learning}

\begin{definition}[Standard sparse coding]
	The standard sparse representation of a signal $x = D \alpha$ is defined to be
	\begin{align*}
		\hat{\alpha} &= \min\limits_{\alpha} \frac{1}{2} {\left\| y - M D \alpha \right\|_2}^2 + \lambda \|\alpha\|_1
	\end{align*}
\end{definition}

\begin{theorem}
	Assume $M = I$.
	Given a set of signals $\{x_i\}$, let $D$ be a dictionary which provides sparse representations $\{\alpha_i\}$ for the set $\{x_i\}$.
	Then, $D$ and $\{\alpha_i\}$ are
	\begin{align*}
		\left( \hat{D},\left\{ \hat{\alpha_i}\right \} \right) &= \min\limits_{D,\{\alpha_i\}} F\left( D , \{\alpha_i\} \right)\\
		&= \min\limits_{D,\{\alpha_i\}} \sum\limits_{i} \frac{1}{2} {\left\| x_i - D \alpha_i \right\|_2}^2 + \lambda \|\alpha_i\|_1
	\end{align*}
\end{theorem}

\subsection{Gradient Descent}

The minimization problem
\begin{align*}
	\left( \hat{D},\left\{ \hat{\alpha_i}\right \} \right) &= \min\limits_{D,\{\alpha_i\}} F\left( D , \{\alpha_i\} \right)\\
	&= \min\limits_{D,\{\alpha_i\}} \sum\limits_{i} \frac{1}{2} {\left\| x_i - D \alpha_i \right\|_2}^2 + \lambda \|\alpha_i\|_1
\end{align*}
can be solved using gradient descent, i.e.
\begin{align*}
	D^{t + 1} &= D^t - \mu \nabla_D F\left( D , \{\alpha_i\} \right)\\
	\alpha^{t + 1} &= \alpha^t - \mu \nabla_{\alpha_i} F\left( D , \{\alpha_i\} \right)
\end{align*}
or using stochastic gradient descent, i.e.
\begin{align*}
	\hat{\alpha_i} &= \argmin\limits_{\alpha_i} \frac{1}{2} {\left\| x_i - D^t \alpha_i \right\|_2}^2 + \lambda \|\alpha_i\|_1\\
	D^{t + 1} &= D^t - \mu \nabla_D F_i\left( D^t , \hat{\alpha_i} \right)
\end{align*}
where $i$ is chosen randomly for each iteration.

\subsection{Dictionary Learning Given Signal and Sparse Representation}

Assume $\{x_i\}$ and $\{\alpha_i\}$ are known.
Hence, let
\begin{align*}
	X &=
		\begin{pmatrix}
			x_1 & \dots & x_N\\
		\end{pmatrix}\\
	\Lambda &=
		\begin{pmatrix}
			\alpha_1 & \dots & \alpha_N\\
		\end{pmatrix}
\end{align*}
Then the minimization problem is equivalent to
\begin{align*}
	\hat{D} &= \min\limits_{D} {\left\| X - D \Lambda \right\|_F}^2
\end{align*}
where $D \in \mathbb{R}^{d \times n}$, $\Lambda \in \mathbb{R}^{n \times N}$, $X \in \mathbb{R}^{d \times N}$, and $\|\cdot\|_F$ is the Frobinus norm.
This minimization can be performed by differentiating the expression and equating to zero.
Hence,
\begin{align*}
	D &= X \Lambda^* \left( \Lambda \Lambda^* \right)^{-1}
\end{align*}

\subsection{Proximal Gradient Algorithm}

\begin{definition}[Proximal map of a function]
	The proximal map of a function $R(\alpha)$ is defined to be
	\begin{align*}
		S_{\lambda}(z) &= \argmin\limits_{\alpha} \frac{1}{2} {\|z - \alpha\|_2}^2 + \lambda R(\alpha)
	\end{align*}
\end{definition}

The solution to the minimization problem
\begin{align*}
	\tilde{\alpha} &= \argmin\limits_{\alpha} \frac{1}{2} {\|y - A \alpha\|_2}^2 + \lambda R(\alpha)
\end{align*}
where $R(\alpha)$ is called the regularization problem can be obtained using proximal gradient descent, i.e.
\begin{align*}
	{\tilde{\alpha}}^{t + 1} &= \alpha^t - \mu \transpose{A} \left( A \alpha^t - y \right)\\
	\alpha^{t + 1} &= S_{\lambda \mu} \left( {\tilde{\alpha}}^{t + 1} \right)
\end{align*}
This solution converges to the solution of the original problem if
\begin{align*}
	\mu &< \frac{2}{\|A\|_2}
\end{align*}
where $\|A\|_2$ is the spectral norm of $A$, i.e. the largest singular value of $A$.\\

\subsubsection{Iterative Soft Thresholding Algorithm}

If the regularization function is
\begin{align*}
	R(\cdot) &= \|\cdot\|_1
\end{align*}
the proximal gradient minimization problem is
\begin{align*}
	\hat{\alpha} &= \argmin\limits_{\alpha} \frac{1}{2} {\|y - A \alpha\|_2}^2 + \lambda \|\alpha\|_1
\end{align*}
Hence, the proximal map for the $l_1$ norm is
\begin{align*}
	S_{\lambda}(z) &= \argmin\limits_{\alpha} \frac{1}{2} {\|z - \alpha\|_2}^2 + \lambda \|\alpha\|_1\\
	&= \argmin\limits_{\alpha} \frac{1}{2} \sum\limits_{i} (z_i - \alpha_i)^2 + \lambda \lambda |\alpha_i|
\end{align*}
Hence, as this problem is separable, the solution can be calculated for each $\alpha_i$ separately, i.e.
\begin{align*}
	\hat{\alpha_i} &= \argmin\limits_{\alpha_i} \frac{1}{2} (z_i - \alpha_i)^2 + \lambda |\alpha_i|
\end{align*}
Hence, differentiating with respect to $\alpha_i$,
\begin{align*}
	\dod{\hat{\alpha_i}}{\alpha_i} &= \alpha_i - z_i + \lambda \dod{|\alpha_i|}{\alpha_i}
\end{align*}
Also,
\begin{align*}
	\dod{|\alpha_i|}{\alpha_i} &=
		\begin{cases}
			1 &;\quad \alpha_i > 0\\
			-1 &;\quad \alpha_i < 0\\
			(-1,1) &;\quad \alpha_i = 0\\
		\end{cases}
\end{align*}
Hence, equating the derivative to $0$ and solving, if $z_i > \lambda$,
\begin{align*}
	\hat{\alpha_i} &= z_i - \lambda
\end{align*}
if $z_i < \lambda$,
\begin{align*}
	\hat{\alpha_i} &= z_i + \lambda
\end{align*}
and if $-\lambda \le z_i \le \lambda$,
\begin{align*}
	\hat{\alpha_i} &\in \left( -z_i \pm \lambda \right)
\end{align*}
Hence,
\begin{align*}
	S_{\lambda}(z_i) &=
		\begin{cases}
			0 &;\quad |z_i| < \lambda\\
			z_i - \lambda \sgn(z_i) &;\quad \text{otherwise}\\
		\end{cases}
\end{align*}
Such a function is called a soft thresholding function, and hence this algorithm is called the iterative soft thresholding algorithm.

\subsubsection{Iterative Hard Thresholding Algorithm}

Similar to the Iterative Soft Thresholding Algorithm, if
\begin{align*}
	R(\cdot) &= \|\cdot\|_0
\end{align*}
then
\begin{align*}
	S_{\lambda}(z_i) &=
		\begin{cases}
			0 &;\quad |z_i| < \lambda\\
			z_i &;\quad \text{otherwise}\\
		\end{cases}
\end{align*}
However, as this problem is not convex, the solution of this problem is not guaranteed to converge.

\clearpage
\part{Low Rank Model}

\begin{definition}[Low rank model]
	The low rank model is defined to be
	\begin{align*}
		Y &= M(X) + E
	\end{align*}
	where $X$ is the original signal and has low rank, $Y$ is the measured signal, and $E$ is the noise.
\end{definition}

\section{Solution Using Singular Value Decomposition}

Assume $M = I$.
Hence,
\begin{align*}
	Y &= X + E
\end{align*}
Therefore, the reconstructed signal is
\begin{align*}
	\hat{X} &= \argmin\limits_{X} {\|X - Y\|_2}^2
\end{align*}
such that
\begin{align*}
	\rank(X) &\le r
\end{align*}
Let the singular value decomposition of $Y$ be
\begin{align*}
	Y &= U \Sigma_Y V^*
\end{align*}
where $U$ and $V$ are unitary matrices, and $\Sigma$ is a diagonal matrix with $\sigma_i$s, the eigenvalues of $Y$ as the diagonal elements.
Hence, as the rank of a matrix is equal to the number of its non-zero singular values,
\begin{align*}
	\rank(X) &= \left\| \diag(\Sigma_X) \right\|_0
\end{align*}
where $\Sigma_X$ is the $\Sigma$ from the SVD of $X$.
Hence,
\begin{align*}
	{\|X - Y\|_F}^2 &= {\left\| U^* (X - Y) V \right\|_F}^2\\
	&= {\left\| U^* X V - U^* Y V \right\|_F}^2\\
	&= {\left\| U^* X V - \Sigma_Y \right\|_F}^2
\end{align*}
Also, as $U$ and $V$ are unitary,
\begin{align*}
	\rank(X) &= \rank(U^* X V)
\end{align*}
Let
\begin{align*}
	\tilde{X} &= U^* X V
\end{align*}
Then, solving the original minimization problem is equivalent to solving
\begin{align*}
	\hat{\tilde{X}} &= \argmin\limits_{\tilde{X}} {\left\| \tilde{X} - \Sigma_Y \right\|_2}^2
\end{align*}
such that
\begin{align*}
	\rank\left( \tilde{X} \right) &\le r
\end{align*}
with
\begin{align*}
	X &= U \tilde{X} V^*
\end{align*}

\section{Solution Using Proximal Gradient}

\begin{align*}
	Y &= M(X) + E
\end{align*}
Hence, the minimization problem is
\begin{align*}
	\hat{X} &= \argmin\limits_{X} {\|X - Y\|_2}^2
\end{align*}
such that
\begin{align*}
	\rank(X) &\le r
\end{align*}
Hence, as
\begin{align*}
	\rank(X) &= \left\| \diag(\Sigma_X) \right\|_0
\end{align*}
Hence, as with the sparsity model, relaxing the minimization problem from the $l_0$ norm to the $l_1$ norm, the nuclear norm of $X$ can be defined as
\begin{align*}
	\|X\|_* &= \sum \sigma_i
\end{align*}
Hence, the minimization problem is
\begin{align*}
	\hat{X} &= {\argmin\limits_{X} \frac{1}{2} \left\| Y - M(X) \right\|_F}^2 + \lambda \|X\|_*
\end{align*}
This can be solved using the proximal gradient algorithm as for the sparsity problem.

\clearpage
\printindex

\end{document}
