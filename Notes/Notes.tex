\documentclass[titlepage, fleqn, a4paper, 12pt, twoside]{article}
\usepackage{geometry}
\usepackage{exsheets} %question and solution environments
\usepackage{amsmath, amssymb, amsthm} %standard AMS packages
\usepackage[utf8]{inputenc}
\usepackage{esint} %integral signs
\usepackage{marginnote} %marginnotes
\usepackage{gensymb} %miscellaneous symbols
\usepackage{commath} %differential symbols
\usepackage{xcolor} %colours
\usepackage{cancel} %cancelling terms
\usepackage[free-standing-units,space-before-unit]{siunitx} %formatting units
	\sisetup
	{
		per-mode=fraction,
		fraction-function=\frac
	}
\usepackage{tikz, pgfplots} %diagrams
	\usetikzlibrary{calc, hobby, patterns, intersections, angles, quotes, spy}
	\usetikzlibrary{circuits.logic.US,circuits.logic.IEC}
\usepackage{graphicx} %inserting graphics
\usepackage{imakeidx}
\makeindex
\usepackage{hyperref} %hyperlinks
\usepackage{datetime} %date and time
\usepackage{enumerate, enumitem} %numbered lists
\usepackage{float} %inserting floats
\usepackage[american voltages]{circuitikz} %circuit diagrams
\usepackage{setspace} %double spacing
\usepackage{microtype} %micro-typography
\usepackage{listings} %formatting code
	\lstset{language=Matlab}
	\lstdefinestyle{standardMatlab}
	{
		belowcaptionskip=1\baselineskip,
		breaklines=true,
		frame=L,
		xleftmargin=\parindent,
		language=C,
		showstringspaces=false,
		basicstyle=\footnotesize\ttfamily,
		keywordstyle=\bfseries\color{green!40!black},
		commentstyle=\itshape\color{purple!40!black},
		identifierstyle=\color{blue},
		stringstyle=\color{orange},
	}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{todonotes}
\usepackage[noabbrev,capitalize]{cleveref}
\usepackage[section]{placeins}
\usepackage[style=numeric, backend=biber]{biblatex}
\usepackage{adjustbox}
\usepackage{algpseudocode} %algorithms
\usepackage{algorithm} %algorithms

% \bibliography{<mybibfile>}% ONLY selects .bib file; syntax for version <= 1.1b
\addbibresource{bibliography.bib}% Syntax for version >= 1.2

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} %adds numbers to specific equations in non-numbered list of equations

\DeclareMathAlphabet{\mathcal}{OT1}{pzc}{m}{it}

\theoremstyle{definition}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}

\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}
\newtheorem{law}{Law}

\tikzset{
block/.style = {draw, rectangle, minimum height=3em, minimum width=3em},
tmp/.style  = {coordinate},
sum/.style = {draw, circle, node distance=1cm},
input/.style = {coordinate},
output/.style = {coordinate},
pinstyle/.style = {pin edge={to-,thin,black}}
}

\makeatletter
\@addtoreset{section}{part} %resets section numbers in new part
\makeatother

\newcommand\blfootnote[1]{%
	\begingroup
	\renewcommand\thefootnote{}\footnote{#1}%
	\addtocounter{footnote}{-1}%
	\endgroup
}

\renewcommand{\marginfont}{\scriptsize \color{blue}}

\renewcommand{\tilde}{\widetilde}

\def\doubleunderline#1{\underline{\underline{#1}}}

\SetupExSheets{solution/print = true} %prints all solutions by default

\DeclareMathOperator{\FT}{\mathcal{FT}}
\DeclareMathOperator{\DTFT}{\mathcal{DTFT}}
\DeclareMathOperator{\IFT}{\mathcal{FT}^{-1}}
\DeclareMathOperator{\DFT}{\mathcal{DFT}}

\DeclareMathOperator{\Q}{\mathcal{Q}}

\DeclareMathOperator{\rect}{\mathrm{rect}}
\DeclareMathOperator{\sinc}{\mathrm{sinc}}

\DeclareMathOperator{\boxfunc}{\mathrm{box}}

\DeclareMathOperator{\vspan}{\mathrm{span}}

\DeclareMathOperator{\argmin}{\mathrm{argmin}}

\DeclareMathOperator{\Db}{\mathrm{Db}}

\DeclareMathOperator{\normal}{\mathrm{N}}

\DeclareMathOperator{\expct}{\mathrm{E}}

\DeclareMathOperator{\sgn}{\mathrm{sgn}}

\DeclareMathOperator{\dist}{\mathrm{d}}

\DeclareMathOperator{\nullspace}{\mathrm{N}}
\DeclareMathOperator{\range}{\mathrm{R}}

\DeclareMathOperator{\l2}{\ell_2}

\DeclareMathOperator{\rank}{\mathrm{rank}}
\DeclareMathOperator{\diag}{\mathrm{diag}}

\def\transpose#1{{#1}^{\mathsf{T}}}
\def\minustranspose#1{{#1}^{\mathsf{-T}}}

\def\downsample#1{\downarrow_{#1}}
\def\upsample#1{\uparrow_{#1}}

%opening
\title{Digital Processing of Single and Multidimensional Signals}
\author{Aakash Jog}
\date{2018-19}

\begin{document}

\pagenumbering{roman}
\begin{titlepage}
\newgeometry{margin=0cm}
\maketitle
\end{titlepage}
\restoregeometry
%\setlength{\mathindent}{0pt}

\blfootnote
{
	\begin{figure}[H]
		\includegraphics[height = 12pt]{cc.pdf}
		\includegraphics[height = 12pt]{by.pdf}
		\includegraphics[height = 12pt]{nc.pdf}
		\includegraphics[height = 12pt]{sa.pdf}
	\end{figure}
	This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy of this license, visit \url{http://creativecommons.org/licenses/by-nc-sa/4.0/}.
} %CC-BY-NC-SA license

\tableofcontents

\clearpage
\section{Lecturer Information}

\textbf{Dr. Raja Giryes}\\
~\\
E-mail: \href{mailto:raja@tauex.tau.ac.il}{raja@tauex.tau.ac.il}\\
~\\

\section{Grading}

\begin{enumerate}
	\item Homework (MATLAB or NumPy): 20\%
	\item Final exam: 80\%
\end{enumerate}

\clearpage
\pagenumbering{arabic}

\part{Basic Definition and Theorems}

\begin{definition}[Fourier transform]
	For an LTI system, the Fourier transform is defined as
	\begin{align*}
		\FT\left\{ x(t) \right\} &= \left\langle x,e^{2 \pi j f t} \right\rangle\\
		&= \int\limits_{t \in \mathrm{R}^d} x(t) \overline{e^{2 \pi j f t}} \dif t\\
		&= \int\limits_{t \in \mathrm{R}^d} x(t) e^{-2 \pi j \transpose{f} t} \dif t
	\end{align*}
	\label{def:Fourier_transform}
	\index{transform!Fourier!continuous time}
\end{definition}

\begin{definition}[Tensor product]
	\begin{align*}
		(x_1 \otimes \dots \otimes x_d)(t) &= x_1(t_1) \cdot \dots \cdot x_d(t_d)
	\end{align*}
	for $t \in R^d$.
	\index{product!tensor}
\end{definition}

\begin{definition}[Fourier transform of tensor product]
	The Fourier transform of a tensor product is defined as
	\begin{align*}
		\FT\left\{ x_1 \otimes \dots \otimes x_d \right\}(f) &= \int\limits_{R^d} x_1(t_1) \cdot \dots \cdot x_d(t_d) e^{-2 \pi j (t_1 f_1 + \dots + t_d f_d)} \dif t_1 \dots \dif t_d\\
		&= \left( \FT\left\{ x_1(t) \right\}(f_1) \right) \dots \left( \FT\left\{ x_d(t) \right\}(f_d) \right)
	\end{align*}
	\index{transform!Fourier!continuous time}
	\index{product!tensor}
\end{definition}

\begin{definition}[$\rect$]
	\begin{align*}
		\rect(t) &=
			\begin{cases}
				0 &;\quad |t| > \frac{1}{2}\\
				1 &;\quad |t| < \frac{1}{2}\\
			\end{cases}
	\end{align*}
	\index{standard functions!rect}
\end{definition}

\begin{definition}[$u_{[a,b]}$]
	\begin{align*}
		u_{[a,b]}(t) &=
			\begin{cases}
				1 &;\quad a \le t \le b\\
				0 &;\quad \text{otherwise}\\
			\end{cases}
	\end{align*}
	\index{standard functions!u@$u_{[a,b]}$}
\end{definition}

\begin{definition}[$\sinc$]
	\begin{align*}
		\sinc(t) &= \frac{\sin(\pi t)}{\pi t}
	\end{align*}
	\index{standard functions!sinc}
\end{definition}

\begin{theorem}
	\begin{align*}
		\FT\left\{ \rect(t) \right\} &= \sinc(f)
	\end{align*}
	\index{standard functions!rect}
	\index{standard functions!sinc}
\end{theorem}

\begin{definition}[$\boxfunc$]
	A box function is defined to be the product of rect functions in multiple dimensions.
	\begin{align*}
		\boxfunc(t_1,\dots,t_n) &= \rect(t_1) \cdot \dots \cdot \rect(t_n)\\
		&=
			\begin{cases}
				1 &;\quad |t_1| < \frac{1}{2}, \dots, |t_n| < \frac{1}{2}\\
				0 &;\quad \text{otherwise}\\
			\end{cases}
	\end{align*}
	\index{standard functions!box}
	\index{standard functions!rect}
\end{definition}

\begin{definition}[Signum function]
	The signum function is defined to be
	\begin{align*}
		\sgn(t) &=
			\begin{cases}
				1 &;\quad t > 0\\
				[-1,1] &;\quad t = 0\\
				-1 &;\quad t < 0\\
			\end{cases}
	\end{align*}
	\index{standard functions!sgn}
\end{definition}

\begin{theorem}[Shifting in time]
	\begin{align*}
		\FT\left\{ D_p\left( x(t) \right) \right\}(f) &= e^{-2 \pi j \transpose{f} p} \FT\left\{ x(t) \right\}(f)
	\end{align*}
	where
	\begin{align*}
		D_p\left( x(t) \right) &= x(t - p)
	\end{align*}
	\label{thm:shifting_in_time}
	\index{shift!in time}
\end{theorem}

\begin{theorem}
	\begin{align*}
		x(t) \ast h(t) &= \FT{x}(f) \FT{h}(f)
	\end{align*}
	\label{thm:convolution_in_time_multiplication_in_frequency}
	\index{convolution!in time}
	\index{multiplication!in frequency}
\end{theorem}

\begin{theorem}[Stretching in time]
	\begin{align*}
		\FT\left\{ S_A\left( x(t) \right) \right\}(f) &= \int\limits_{R^d} x(A t) e^{-j 2 \pi \transpose{f} t} \dif t\\
		&= \int\limits_{R^d} x(q) ^{-j 2 \pi \transpose{f} A^{-1} q} \frac{\dif q}{|\det A|}\\
		&= \frac{1}{|\det A|} \int\limits_{R^d}  x(q) e^{-j 2 \pi \left( \minustranspose{A} f \right)^T q} \dif q\\
		&= \frac{\FT\left\{ x(t) \right\}\left( \minustranspose{A} f \right)}{|\det A|}\\
		&= \frac{S_{\minustranspose{A}} X(f)}{|\det A|}
	\end{align*}
	where
	\begin{align*}
		S_A\left( x(t) \right) &= x(A t)
	\end{align*}
	where $A$ is a matrix which may represent stretching, rotating, etc.\\
	Hence, if $A$ is equal to a scalar $a$,
	\begin{align*}
		\FT\left\{ x(a t) \right\} &= \frac{\FT\left\{ x(t) \right\}\left( \frac{f}{a} \right)}{|a|}
	\end{align*}
	\label{thm:stretching_in_time}
	\index{stretch!in time}
\end{theorem}

\begin{theorem}[Fourier transform of Gaussian]
	Using \cref{thm:stretching_in_time}, for a single dimensional Gaussian signal,
	\begin{align*}
		\FT\left\{ e^{-t^2} \right\} &= e^{-\pi f^2}
	\end{align*}
	and for a orthogonal multidimensional Gaussian,
	\begin{align*}
		\FT\left\{ e^{-\transpose{t} t} \right\} &= e^{-\pi \transpose{f} f}
	\end{align*}
	Hence, using \cref{thm:stretching_in_time}, for a general multidimensional Gaussian,
	\begin{align*}
		\FT\left\{ e^{-\transpose{t} C^{-1} t} \right\} &= e^{-\pi \transpose{f} C f} |\det C|
	\end{align*}
	where $C$ is the covariance matrix.
	\label{thm:Fourier_transform_of_Gaussian}
	\index{transform!Fourier!continuous time}
	\index{standard functions!Gaussian}
\end{theorem}

\begin{definition}[Unitary matrix]
	A matrix $A$ is said to be unitary if
	\begin{align*}
		A^{-1} &= A^*
	\end{align*}
	where $A^*$ is the conjugate transpose of $A$.
	\index{types of matrices!unitary}
	\index{conjugate transpose}
\end{definition}

\begin{theorem}[Rotation]
	Stretching with by a unitary matrix $A$ is equivalent to rotation, and hence denoting $S_A$ by $R_R$,
	\begin{align*}
		\FT\left\{ R_R\left( x(t) \right) \right\} &= R_R \FT\left\{ x(t) \right\}
	\end{align*}
	\label{thm:rotation}
	\index{stretch!in time}
	\index{rotation!in time}
\end{theorem}

\begin{definition}
	A dimension-reducing projection $P$ from $R^d$ to $R^{d - 1}$, is defined as
	\begin{align*}
		P\left( x(t) \right) &= \int\limits_{R^d} x(t_1,\dots,t_d) \dif t_d
	\end{align*}
	\index{projection!dimension-reducing}
\end{definition}

\begin{definition}
	The slicing operator $\Q$ is defined as
	\begin{align*}
		\Q\left\{ X(f_1,\dots,f_d) \right\} &= X\left( f_1,\dots,f_{d - 1},0 \right)
	\end{align*}
	\label{def:slicing_operator}
	\index{standard operators!slicing}
\end{definition}

\begin{theorem}[Fourier transform of dimension reducing projection]
	The Fourier transform of a dimension reducing projection is
	\begin{align*}
		\FT\left\{ P\left( x(t) \right) \right\} &= \int\limits_{R^{d - 1}} \left( \int\limits_{R^d} x(t_1,\dots,t_d) \dif t_d \right) e^{-j 2 \pi (t_1 f_1 + \dots + t_{d - 1} f_{d - 1})} \dif t_1 \dots \dif t_{d - 1}\\
		&= \int\limits_{R^{d - 1}} \left( \int\limits_{R^d} x(t_1,\dots,t_d) e^{-j 2 \pi t_d (0)} \dif t_d \right) e^{-j 2 \pi (t_1 f_1 + \dots + t_{d - 1} f_{d - 1})} \dif t_1 \dots \dif t_{d - 1}\\
		&= \int\limits_{R^d} x(t) e^{-j 2 \pi \transpose{f} t} \dif t \Big|_{f_d = 0}\\
		&= \FT\left\{ x(t) \right\}(f_1,\dots,f_{d - 1},0)\\
		&= \Q\left\{ \FT\left\{ x(t) \right\} \right\}
	\end{align*}
	where $\Q$ is the slicing operator as in \cref{def:slicing_operator}.
	\label{thm:Fourier_transform_of_dimension_reducing_projection}
	\index{projection!dimension-reducing}
	\index{transform!Fourier!continuous time}
\end{theorem}

\clearpage
\part{Hilbert Spaces}

\section{Inner Product Spaces}

\begin{definition}[Inner product spaces]
	An inner product space is defined to be a space with an inner product with the following properties.
	\begin{description}
		\item[Conjugate symmetry]
			\begin{align*}
				\langle x,y \rangle &= \langle y,x \rangle^*
			\end{align*}
		\item[Linearity]
			\begin{align*}
				\langle x , a y + b z \rangle &= a \langle x,y \rangle + b \langle x,z \rangle
			\end{align*}
		\item[Non-negativity]
			\begin{equation*}
				\langle x,x \rangle = 0 \iff x = 0
			\end{equation*}
	\end{description}
	\index{spaces!inner product}
\end{definition}

\begin{definition}[Norm and distance]
	The norm corresponding to an inner product space is defined to be
	\begin{align*}
		\|x\| &= \sqrt{\langle x,x \rangle}
	\end{align*}
	Hence, the distance between $x$ and $y$ is defined to be
	\begin{align*}
		\dist(x,y) &= \|x - y\|
	\end{align*}
	\index{norm}
	\index{distance}
\end{definition}

\begin{definition}[Hilbert space]
	An inner product, normed, complete vector space is said to be a Hilbert space.
	\index{spaces!Hilbert}
\end{definition}

\section{Operators and Transformations}

\begin{theorem}
	Let $M$ be an operator from the inner product space $H$ to the inner product space $S$.
	Then,
	\begin{align*}
		\langle M x , y \rangle &= \langle x , M^* y \rangle
	\end{align*}
	for all $x \in H$ and $y \in S$.
	If $H$ and $S$ are finite, then $M$ can be represented by a matrix and
	\begin{align*}
		M^* &= \overline{M^T}
	\end{align*}
	is the complex transpose of $M$.
	\index{operators}
\end{theorem}

\begin{definition}[Linear operator]
	An operator $T$ is said to be linear if
	\begin{align*}
		T(a_1 x_1 + a_2 x_2) &= a_1 T(x_1) + a_2 T(x_2)
	\end{align*}
	\index{types of operators!linear}
\end{definition}

\begin{theorem}[Bounded linear operation]
	All linear operators are bounded, i.e. for every $T$, there exists $\alpha$ such that
	\begin{align*}
		\left\| T(x) \right\| &\le \alpha \|x\|
	\end{align*}
	for all $x$.
	\index{types of operators!linear}
\end{theorem}

\begin{theorem}[Continuous linear operation]
	If there exists a sequence $x_i$ which converges to $x$, then for any linear operator $T$, the sequence $T(x_i)$ converges to $T(x)$.
	\index{types of operators!linear}
\end{theorem}

\begin{definition}[Adjoint of linear operation]
	The adjoint of a linear operation $T$ is defined to be $T^*$, the only bounded operation such that
	\begin{align*}
		\langle T x , y \rangle &= \langle x , T^* y \rangle
	\end{align*}
	for all $x \in H$, $y \in S$.\\
	\index{adjoint of linear operation}
\end{definition}

\begin{definition}[Unitary operator]
	An operator $T$ is said to be unitary if and only if
	\begin{align*}
		T T^* &= T^* T\\
		&= I
	\end{align*}
	where $T^*$ is the conjugate transpose of $T$.
	\index{types of operators!unitary}
\end{definition}

\begin{definition}[Hermitian operator]
	An operator $T$ is said to be Hermitian if and only if
	\begin{align*}
		T^* &= T
	\end{align*}
	where $T^*$ is the conjugate transpose of $T$.
	\index{types of operators!Hermitian}
\end{definition}

\begin{theorem}
	An operator $T$ from a Hilbert space $H$ to $H$ is Hermitian if and only if
	\begin{align*}
		\langle T x , x \rangle &\in \mathbb{R}
	\end{align*}
	for all $x \in H$.
	\index{types of operators!Hermitian}
\end{theorem}

\begin{definition}[Orthogonal complement of subspace]
	The orthogonal complement of a subspace $W$ is defined to be
	\begin{align*}
		W^{\perp} &= \left\{ x \Big| \langle x,y \rangle = 0 \quad \forall y \in W \right\}
	\end{align*}
	\index{orthogonal complement}
\end{definition}

\begin{definition}[Null space of linear operator]
	The null space of a linear operator $T : H \to S$ is defined to be
	\begin{align*}
		\nullspace(T) &= \left\{ x \Big| T x = 0 \right\}\\
		&\subseteq H
	\end{align*}
	\index{null space}
\end{definition}

\begin{definition}[Range of linear operator]
	The range of a linear operator $T : H \to S$ is defined to be
	\begin{align*}
		\range(T) &= \left\{ y \Big| T x = y \, , \, x \in H \right\}\\
		&\subseteq S
	\end{align*}
	\index{range}
\end{definition}

\begin{definition}[Direct sum]
	The direct sum of two sets $A$ and $B$ is defined to be the set
	\begin{align*}
		C &= A \oplus B
	\end{align*}
	if and only if, for all $c \in C$, there exists a unique pair $a \in A$ and $b \in B$ such that
	\begin{align*}
		c &= a + b
	\end{align*}
	\index{direct sum}
\end{definition}

\begin{theorem}
	For any linear operator $T$,
	\begin{align*}
		\nullspace\left( T^* \right) &= {R(T)}^{\perp}\\
		\range\left( T^* \right) &= {N(T)}^{\perp}
	\end{align*}
	\index{null space}
	\index{range}
	\label{thm:range_and_nullspace_of_conjugate_transpose_of_transformation}
\end{theorem}

\begin{theorem}
	\begin{align*}
		\nullspace(T) &= \nullspace\left( T^* T \right)
	\end{align*}
	\index{null space}
\end{theorem}

\begin{theorem}
	For a linear operator $T: H \to S$,
	\begin{align*}
		H &= \nullspace(T) \oplus \nullspace(T)^{\perp}
	\end{align*}
	where $\nullspace(T)$ is the null space of $T$.\\
	Additionally, if $S$ is a closed set,
	\begin{align*}
		S &= \range(T)^{C} \oplus \range(T)^{\perp}
	\end{align*}
	where
	\begin{align*}
		\range(T)^{C} &= \left( \range(T)^{\perp} \right)^{\perp}
	\end{align*}
	is the closure of $R(T)$.
	\index{null space}
	\index{closure}
\end{theorem}

\begin{definition}[Injective transformation]
	A linear transformation/operation $T: H \to S$ is said to be injective (one-to-one) if
	\begin{align*}
		x \neq y &\iff T x \neq T(y)
	\end{align*}
	\index{injectivity}
	\index{one-to-one}
\end{definition}

\begin{definition}[Surjective transformation]
	A linear transformation/operation $T: H \to S$ is said to be surjective (onto) if
	\begin{align*}
		R(T) &= S
	\end{align*}
	\index{surjectivity}
	\index{onto}
\end{definition}

\begin{definition}[Bijective transformation]
	A linear transformation/operation $T: H \to S$ is said to be bijective if it is injective (one-to-one) and surjective (onto).
	\index{bijectivity}
	\index{one-to-one and onto}
\end{definition}

\section{Singular Value Decomposition (SVD)}

\begin{definition}[Eigenvalue decomposition]
	The eigenvalue decomposition for a symmetric (and hence also square) and Hermitian matrix $A$ is
	\begin{align*}
		A &= V \Lambda V^*
	\end{align*}
	where $\Lambda$ is a diagonal matrix with the eigenvalues of $A$ as diagonal elements, and $V$ contains the corresponding eigenvectors of $A$.
	\index{decomposition!eigenvalue}
\end{definition}

\begin{definition}[Singular Value Decomposition (SVD)]
	Let $A \in \mathbb{R}^{m \times n}$.
	Then, the singular value decomposition of $A$ is defined to be
	\begin{align*}
		A &= U \Sigma V^*
	\end{align*}
	where the diagonal elements of $\Sigma$ are the singular values of $A$, and the remaining elements of $\Sigma$ are zero.\\
	The columns of $V$ are called the right singular vectors, the columns of $U$ are called the left singular vectors, and the diagonal elements of $\Sigma$ are denoted by $\sigma_i$, where $1 \le i \le \min(m,n)$.
	\index{decomposition!singular value}
\end{definition}

\begin{theorem}
	\begin{align*}
		{\sigma_i}^2 &= \lambda_i \left( A^* A \right)
	\end{align*}
	where $\sigma_i$ are the singular values of $A$, and $\lambda_i$ are the eigenvalues of $A$, for $1 \le i \le \min(m,n)$.
	\index{decomposition!eigenvalue}
	\index{decomposition!singular value}
\end{theorem}

\begin{definition}[Rank of transformation]
	The rank of a transformation $T$ is the dimension of its range, i.e.
	\begin{align*}
		\rank(T) &= \dim\left( \range(T) \right)
	\end{align*}
	\index{rank}
\end{definition}

\begin{theorem}
	The rank of a matrix $A$ is the number of non-zero singular values of $A$.
	\index{rank}
\end{theorem}

\begin{proof}
	\begin{align*}
		A x &= \left( U \Sigma V^* \right) x\\
		A x &= (U \Sigma) \left( V^* x \right)\\
		&= U \Sigma
			\begin{bmatrix}
				\langle x,v_1 \rangle & \dots & \langle x,v_n \rangle
			\end{bmatrix}\\
		&= \sum\limits_{i = 1}^{\min(m,n)} \sigma_i \langle x,v_i \rangle u_i
	\end{align*}
	Hence, any $A x$ can be expressed as a linear combination of some $u_i$ for $i \le r$.
	Therefore, the range of $A$ is spanned by $u_i$.
	Hence, by definition, the rank of $A$ is $r$.\\
	Also, only the $u_i$s with corresponding non-zero $\sigma_i$s can span $\range(A)$.
	Hence, equivalently, the rank of $A$ is the number of non-zero $\sigma_i$s of $A$.
\end{proof}

\begin{theorem}
	\begin{align*}
		\nullspace(A) &= \vspan\left\{ v_i \Big| \sigma_i = 0 \right\}
	\end{align*}
	Equivalently,
	\begin{align*}
		\nullspace(A) &= \vspan\left\{ v_i \Big| i > r \right\}
	\end{align*}
	where
	\begin{align*}
		r &= \rank(A)
	\end{align*}
	Hence,
	\begin{align*}
		\nullspace(A)^{\perp} &= \vspan\left\{ v_i \Big| \sigma_i \neq 0 \right\}
	\end{align*}
	where $A \in \mathbb{R}^{m \times n}$.
\end{theorem}

\begin{theorem}
	\begin{align*}
		\range(A) &= \vspan\left\{ u_i \Big| \sigma_i \neq 0 \right\}
	\end{align*}
	Equivalently,
	\begin{align*}
		\range(A) &= \vspan\left\{ u_i \Big| i \le r \right\}
	\end{align*}
	where
	\begin{align*}
		r &= \rank(A)
	\end{align*}
	Hence,
	\begin{align*}
		\range(A)^{\perp} &= \vspan\left\{ u_i \Big| i > r \right\}
	\end{align*}
	and equivalently
	\begin{align*}
		\range(A)^{\perp} &= \vspan\left\{ u_i \Big| \sigma_i = 0 \right\}
	\end{align*}
\end{theorem}

\section{Pseudoinverse}

\begin{definition}[Moore-Penrose pseudoinverse]
	The Moore-Penrose pseudoinverse of $A \in \mathbb{R}^{m \times n}$ is defined to be $A^{\dagger}$, such that
	\begin{align*}
		A A^{\dagger} &= I
	\end{align*}
	if $A$ is of full rank, and
	\begin{align*}
		A^{\dagger} A &= I
	\end{align*}
	if $m > n$.
	\index{pseudoinverse!Moore-Penrose}
	\index{pseudoinverse!of matrices}
\end{definition}

\begin{theorem}
	If the SVD of $A$ is
	\begin{align*}
		A &= U \Sigma V^*\\
		&= U \diag(\sigma_1,\dots,\sigma_m) V^*
	\end{align*}
	then the pseudoinverse of $A$ is
	\begin{align*}
		A^{\dagger} &= V \diag\left( \frac{1}{\sigma_1},\dots,\frac{1}{\sigma_n} \right) U^*
	\end{align*}
	where if $\sigma_i = 0$ then $\frac{1}{\sigma_i}$ is replaced by $0$.
	\index{pseudoinverse!of matrices}
\end{theorem}

\section{Set Transformations}

\begin{definition}[Set transformation]
	A set transformation $X : \l2 \to H$ composed of $x_1,\dots,x_n \in H$, is defined as
	\begin{align*}
		X a &= \sum a_i x_i
	\end{align*}
	where
	\begin{align*}
		\l2 &= \left\{ a_i \Big| \sum |a_i|^2 < \infty \right\}
	\end{align*}
	is the standard $\l2$ space, and $H$ is a Hilbert space.
	If $n$ is finite, then $X$ is a matrix with columns $x_1,\dots,x_n$.
	\index{set transformation}
\end{definition}

\begin{definition}[Linear independence]
	The set $\{x_i\}$ is said to be linearly independent if and only if
	\begin{align*}
		\sum a_i x_i &= 0
	\end{align*}
	implies $a_i = 0$, $\forall i$.\\
	Equivalently, the set $\{x_i\}$ is said to be linearly independent if and only if
	\begin{align*}
		N(X) &= \{0\}
	\end{align*}
	\index{linear independence}
\end{definition}

\begin{theorem}
	A set of orthogonal vectors/functions with positive norm is always linearly independent.
	\index{linear independence}
	\index{orthogonality}
\end{theorem}

\begin{definition}[Completeness]
	As set $\{x_i \in H\}$ is said to be complete if and only if $H$ is the closure of the set spanned by $x_i$, i.e.
	\begin{align*}
		\overline{\vspan{x_i}} &= H
	\end{align*}
	Equivalently, for any $x \in H$, $\exists \varepsilon > 0$ such that
	\begin{align*}
		\left\| x - \sum\limits_{i = 1}^{n} a_i x_i \right\| &< \varepsilon
	\end{align*}
	for a large enough $n$.
	\index{completeness}
\end{definition}

\begin{theorem}
	The adjoint operator of an operator $X : \l2 \to H$ is $X^* : H \to \l2$ such that
	\begin{align*}
		X^* x &= \left\{ \langle x_i , x \rangle_{H} \right\}
	\end{align*}
	where $x_i$ are such that
	\begin{align*}
		X &= \{x_i\}
	\end{align*}
\end{theorem}

\begin{proof}
	\begin{align*}
		\langle X a , x \rangle_{H} &= \left\langle \sum a_i x_i , x \right\rangle_{H}\\
		&= \sum {a_i}^* \langle x_i , x \rangle_{H}
	\end{align*}
	Also, by definition,
	\begin{align*}
		\langle X a , x \rangle_{H} &= \left\langle a , X^* x \right\rangle_{\l2}
	\end{align*}
	Hence,
	\begin{align*}
		\sum {a_i}^* \langle x_i , x \rangle_{H} &= \left\langle a , X^* x \right\rangle_{\l2}\\
		\therefore \left\langle a , \left\{ \langle x_i , x \right\} \right\rangle_{H} &= \left\langle a , X^* x \right\rangle_{\l2}
	\end{align*}
	Hence,
	\begin{align*}
		X^* x &= \left\{ \langle x_i , x \rangle_{H} \right\}
	\end{align*}
\end{proof}

\section{Basis}

\begin{definition}[Schauter basis]
	A set $\{x_i\} \in H$ is said to be a Schauter basis of $H$ if for any $x \in H$, there are unique coefficients $a_i$ such that
	\begin{align*}
		x &= \sum\limits_{i = 1}^{n} a_i x_i
	\end{align*}
	where $n$ is the dimension of $H$, and may be infinite.
	\index{basis!Schauter}
\end{definition}

\begin{definition}[Riesz basis]
	A set $\{x_i\} \in H$ is said to be a Riesz basis of $H$ if it is complete in $H$ and there exist $0 < A \le B < \infty$ such that
	\begin{eqnarray*}
		A \|a\|^2 \le \left\| \sum a_i x_i \right\|^2 \le B \|a\|^2\\
		A \|x\|^2 \le \left\| \sum \langle x , x_i \rangle \right\|^2 \le B \|x\|^2
	\end{eqnarray*}
	Equivalently for $X : \l2 \to H$ corresponding to $\{x_i\} \in H$,
	\begin{eqnarray*}
		A \langle a , a \rangle \le \langle X a , X a \rangle \le B \langle a , a \rangle\\
		A x^* x \le x^* X X^* x \le B x^* x
	\end{eqnarray*}
	\index{basis!Riesz}
\end{definition}

\begin{theorem}
	Any orthogonal basis is also a Riesz basis.
\end{theorem}

\begin{theorem}
	The elements of a Riesz basis are linearly independent.
	\index{basis!Riesz}
\end{theorem}

\begin{theorem}
	If
	\begin{equation*}
		A I_{\l2} \le X^* X \le B I_{\l2}
	\end{equation*}
	that is,
	\begin{equation*}
		A \|a\|^2 \le \left\| \sum a_i x_i \right\|^2 \le B \|a\|^2\\
	\end{equation*}
	then, $X^* X$ is invertible, and
	\begin{align*}
		\frac{1}{B} I_{\l2} \le \left( X^* X \right)^{-1} \le \frac{1}{A} I_{\l2}
	\end{align*}
	\index{basis!Riesz}
\end{theorem}

\begin{theorem}
	If $X$ is a Riesz basis, then $\range(X)$ is closed.
	\index{basis!Riesz}
\end{theorem}

\begin{definition}[Bi-orthogonal basis]
	The bi-orthogonal basis of a Riesz basis $X$ is defined to be
	\begin{align*}
		\tilde{X} &= X \left( X^* X \right)^{-1}
	\end{align*}
	Hence,
	\begin{align*}
		\tilde{X} X^* &= I
	\end{align*}
	\index{basis!Riesz}
	\index{basis!bi-orthogonal}
\end{definition}

\begin{theorem}
	The bi-orthogonal basis of a Riesz basis is also a Riesz basis.
	\index{basis!Riesz}
	\index{basis!bi-orthogonal}
\end{theorem}

\begin{theorem}
	For a Riesz basis $\{x_i\}$ of $H$, any $x = X a$ for $x \in H$ can be expressed as a linear combination of $H$ such that
	\begin{align*}
		x &= X a\\
		&= \sum a_i x_i\\
		&= \sum \left\langle \tilde{x_i} , x \right\rangle x_i
	\end{align*}
	where $\left\{ \tilde{x_i} \right\} = \tilde{X}$ is the bi-orthogonal basis of $X$.
	\index{basis!Riesz}
	\index{basis!bi-orthogonal}
\end{theorem}

\begin{proof}
	Let
	\begin{align*}
		x &= \sum a_i x_i\\
		&= \sum \left\langle \tilde{x_i} , x \right\rangle x_i
	\end{align*}
	Also,
	\begin{align*}
		\left\langle \tilde{x_i} , x \right\rangle &= \left\langle \tilde{x_i} , \sum\limits_{j = -\infty}^{\infty} a_j x_j \right\rangle\\
		&= \sum\limits_{j = -\infty}^{\infty} a_j \left\langle \tilde{x_i} , x_j \right\rangle
	\end{align*}
	Hence,
	\begin{align*}
		a_i &= \left\langle \tilde{x_i} , x \right\rangle\\
		&= \sum\limits_{j = -\infty}^{\infty} a_j \left\langle \tilde{x_i} , x_j \right\rangle
	\end{align*}
	Therefore,
	\begin{align*}
		\left\langle \tilde{x_i} , x_j \right\rangle &= \delta_{i,j}
	\end{align*}
	Hence, for $\tilde{X} = \left\{ \tilde{x_i} \right\}$,
	\begin{align*}
		\tilde{X}^* X &= I
	\end{align*}
	Hence, $\tilde{X}$ is the bi-orthogonal basis of $X$.
\end{proof}

\section{Projections}

\begin{definition}[Projection]
	The operator $T : H \to H$ is said to be a projection if
	\begin{align*}
		T T &= T
	\end{align*}
	that is
	\begin{align*}
		T^2 &= T
	\end{align*}
	\index{projection}
\end{definition}

\begin{theorem}
	For any $y \in \range(T)$,
	\begin{align*}
		T y &= y
	\end{align*}
	\index{projection}
\end{theorem}

\begin{proof}
	For $y \in \range(T)$, there exists $x$ such that
	\begin{align*}
		y &= T x
	\end{align*}
	Therefore,
	\begin{align*}
		T y &= T T x\\
		&= T^2 x\\
		&= T x\\
		&= y
	\end{align*}
\end{proof}

\begin{theorem}
	\begin{align*}
		\range(T) \cap \nullspace(T) &= \{0\}
	\end{align*}
	where $T$ is a projection.
	\index{projection}
\end{theorem}

\begin{proof}
	Let $y \in \range(T)$ and $y \in \nullspace(T)$.
	Then,
	\begin{align*}
		T y &= y\\
		T y &= 0
	\end{align*}
	Therefore,
	\begin{align*}
		y &= 0
	\end{align*}
\end{proof}

\begin{theorem}
	\begin{align*}
		H &= R(T) \oplus N(T)
	\end{align*}
	where $T : H \to H$ is a projection.
	\index{projection}
\end{theorem}

\begin{proof}
	\begin{align*}
		x &= T x + I x - T x\\
		&= T x + (I - T) x
	\end{align*}
	where $I$ is the identity operator.\\
	Hence, $T x$ is in $\range(T)$, and $x - T x$ is in $\nullspace(T)$.
	Therefore, as $x$ is in $H$,
	\begin{align*}
		H &= \range(T) \cup \nullspace(T)
	\end{align*}
	Also,
	\begin{align*}
		\range(T) \cap \nullspace(T) &= \{0\}
	\end{align*}
	Therefore,
	\begin{align*}
		H &= \range(T) \oplus \nullspace(T)
	\end{align*}
\end{proof}

\begin{theorem}
	Any projection $T$ can be characterized by the range and the null space of $T$.
	Hence, it can be denoted as
	\begin{align*}
		T &= E_{V W}
	\end{align*}
	where
	\begin{align*}
		V &= \range(T)\\
		W &= \nullspace(T)
	\end{align*}
	Hence, for all $v \in V$,
	\begin{align*}
		E_{V W} v &= v
	\end{align*}
	and for all $w \in W$,
	\begin{align*}
		E_{V W} w &= 0
	\end{align*}
	\index{projection}
	\index{range}
	\index{null space}
\end{theorem}

\subsection{Orthogonal Projections}

\begin{definition}[Orthogonal projection]
	A projection $T$ is said to be an orthogonal projection if
	\begin{align*}
		T &= T^*
	\end{align*}
	Hence, as
	\begin{align*}
		\nullspace(T) &= \nullspace\left( T^* \right)\\
		&= \range(T)
	\end{align*}
	or equivalently
	\begin{align*}
		V &= W^{\perp}
	\end{align*}
	it is enough to specify only $V$ to describe a orthogonal projection.
	Hence,
	\begin{align*}
		T &= P_V
	\end{align*}
	\index{projection!orthogonal}
\end{definition}

\begin{theorem}
	For an orthogonal projection $P_V$,
	\begin{align*}
		\langle x_V , x_{V^{\perp}} \rangle &= 0
	\end{align*}
	where
	\begin{align*}
		x_V &= P_V x
	\end{align*}
	\index{projection!orthogonal}
\end{theorem}

\begin{theorem}
	For an orthogonal projection $P_V$,
	\begin{align*}
		\|x\|^2 &= \|x_V\|^2 + \|x_{V^{\perp}}\|^2\\
		\therefore \|x_V\|^2 &\le \|x\|^2
	\end{align*}
	where
	\begin{align*}
		x_V &= P_V x
	\end{align*}
	\index{projection!orthogonal}
\end{theorem}

\begin{proof}
	\begin{align*}
		x &= x_V + X_{V^{\perp}}\\
		\therefore \|x\|^2 &= \|x_V + X_{V^{\perp}}\|^2
	\end{align*}
	Hence, as $x_V$ and $x_{V^{\perp}}$ are orthogonal,
	\begin{align*}
		\|x\|^2 &= \|x_V\|^2 + \|X_{V^{\perp}}\|^2
	\end{align*}
	Therefore,
	\begin{align*}
		\|x_V\|^2 &\le \|x\|^2
	\end{align*}
\end{proof}

\begin{theorem}
	For an orthogonal projection $P_V$,
	\begin{align*}
		x_V &= P_V x\\
		&= \argmin\limits_{v \in V} \|v - x\|^2
	\end{align*}
	that is, $P_V x$ is the vector in $V$ which is closest to $x$.
	\index{projection!orthogonal}
\end{theorem}

\begin{theorem}
	Let $\{v_i\}$ be the Riesz basis of a Hilbert space $H_V$, and let $V : \l2 \to H_V$ be the corresponding transformation, i.e., any $v \in H_V$ can be expressed as
	\begin{align*}
		v &= V a\\
		&= \sum a_i v_i
	\end{align*}
	Then,
	\begin{align*}
		P_{H_V} &= V \tilde{V}^*\\
		&= V \left( V^* V \right)^{-1} V
	\end{align*}
	where
	\begin{align*}
		\tilde{V} &= V \left( V^* V \right)^{-1}
	\end{align*}
	is the bi-orthogonal basis of $V$, i.e., for any $v \in H_V$,
	\begin{align*}
		V a &= v
	\end{align*}
	and
	\begin{align*}
		\tilde{V}^* v &= a
	\end{align*}
	\index{projection!orthogonal}
\end{theorem}

\subsection{Oblique Projections}

\begin{definition}[Oblique projection]
	A projection $T$ is said to be an oblique projection if
	\begin{align*}
		T &\neq T^*
	\end{align*}
	Such a projection can be denoted as
	\begin{align*}
		T &= E_{V W}\\
		x &= x_V + x_W
	\end{align*}
	similar to orthogonal projections, but
	\begin{align*}
		\langle x_V , x_W \rangle &\neq 0
	\end{align*}
	\index{projection!oblique}
\end{definition}

\begin{definition}[Pseudoinverse of transformation]
	Let $T$ be a transformation with closed range.
	Then, the pseudoinverse of $T$ is defined to be $T^{\dagger}$, such that
	\begin{align*}
		T^{\dagger} T x &= x\\
		T^{\dagger} y &= 0
	\end{align*}
	for all $x \in {\nullspace(T)}^{\perp}$, and for all $y \in {\range(T)}^{\perp}$.
	\index{pseudoinverse!of transformations}
\end{definition}

\begin{theorem}
	As in \cref{thm:range_and_nullspace_of_conjugate_transpose_of_transformation},
	\begin{align*}
		\nullspace\left( T^{\dagger} \right) &= \range(T)^{\perp}\\
		\range\left( T^{\dagger} \right) &= \nullspace(T)^{\perp}
	\end{align*}
	\label{thm:range_and_nullspace_of_pseudoinverse_of_transformation}
\end{theorem}

\begin{theorem}
	\begin{align*}
		T T^{\dagger} &= P_{R(T)}
	\end{align*}
	\index{pseudoinverse!of transformations}
\end{theorem}

\begin{theorem}
	\begin{align*}
		T^{\dagger} T &= P_{{N(T)}^{\perp}}
	\end{align*}
	\index{pseudoinverse!of transformations}
\end{theorem}

\begin{theorem}
	If $T^* T$ is invertible,
	\begin{align*}
		T^{\dagger} &= \left( T^* T \right)^{-1} T^*
	\end{align*}
	and if $T T^*$ is invertible,
	\begin{align*}
		T^{\dagger} &= T^* \left( T T^* \right)^{-1}
	\end{align*}
	\index{pseudoinverse!of transformations}
\end{theorem}

\clearpage
\part{Sampling Theory}

\section{Shannon's Interpolation Formula}

\begin{theorem}[Shannon's Interpolation Formula]
	A bandlimited signal limited by $\pm\frac{1}{2 T}$ can be reconstructed from the samples $x(n T)$ from the formula
	\begin{align*}
		x(t) &= \sum\limits_{n = -\infty}^{\infty} x(n T) \sinc\left( \frac{t - n T}{T} \right)
	\end{align*}
	\label{thm:Shannons_Interpolation_Formula}
	\index{Shannon's Interpolation Formula}
\end{theorem}

\clearpage
\printindex

\end{document}
